{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP+iglqAMfqNRzpUEPe34T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kathy42xu/DL_TA/blob/main/LSTM_rf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGT4Z85izHxM",
        "outputId": "a71cf739-1949-41b8-afb7-c3bde9f60f56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: \n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=a2de3205221c8c405b52147f9888b7b78c702a80215eb09fd36f46cea1e13d8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "# 启用 GPU\n",
        "import tensorflow as tf\n",
        "device = tf.test.gpu_device_name()\n",
        "print(\"GPU:\", device)\n",
        "\n",
        "# 安装依赖（若需要）\n",
        "!pip install yfinance ta\n",
        "\n",
        "# 下载数据 (示例：S&P500)\n",
        "import yfinance as yf\n",
        "df = yf.download(\"^GSPC\", start=\"2002-08-01\", end=\"2018-06-28\")\n",
        "df.to_csv(\"SP500.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#preprocessing_indicator"
      ],
      "metadata": {
        "id": "Ziu6Gpq503gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1️⃣ 重新读取 CSV —— 明确告诉 pandas 用第一列作 index\n",
        "df = pd.read_csv(\"SP500.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "# 2️⃣ 确认列名正确，只保留 Open/High/Low/Close/Volume\n",
        "print(df.columns)\n",
        "df = df[['Open','High','Low','Close','Volume']]\n",
        "\n",
        "# 3️⃣ 强制转成浮点数（会把任何非数字变成 NaN）\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# 4️⃣ 删除因转换失败产生的 NaN 行\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWmMlDzy07aB",
        "outputId": "a9637039-4d6b-42e9-df86-3662ed26f3a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')\n",
            "                  Open        High         Low       Close        Volume\n",
            "Price                                                                   \n",
            "2002-08-01  911.619995  911.619995  882.479980  884.659973  1.672200e+09\n",
            "2002-08-02  884.400024  884.719971  853.950012  864.239990  1.538100e+09\n",
            "2002-08-05  864.239990  864.239990  833.440002  834.599976  1.425500e+09\n",
            "2002-08-06  834.599976  874.440002  834.599976  859.570007  1.514100e+09\n",
            "2002-08-07  859.570007  878.739990  854.150024  876.770020  1.490400e+09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-dcf6b7282b58>:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df = pd.read_csv(\"SP500.csv\", index_col=0, parse_dates=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import ta\n",
        "\n",
        "# —— 1️⃣ 读取 CSV ——\n",
        "df = pd.read_csv(\"SP500.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "# —— 2️⃣ 保留 OHLCV 并转换为 float ——\n",
        "df = df[['Open','High','Low','Close','Volume']]\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# —— 3️⃣ 计算全部 43 技术指标 ——\n",
        "# （示例仅列举部分；务必按论文 Table 1 补全其余指标）\n",
        "import ta\n",
        "\n",
        "# —— 全量技术指标列表 ——\n",
        "df['SMA_5'] = ta.trend.sma_indicator(df['Close'], window=5)\n",
        "df['SMA_10'] = ta.trend.sma_indicator(df['Close'], window=10)\n",
        "df['SMA_20'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
        "\n",
        "df['EMA_6'] = ta.trend.ema_indicator(df['Close'], window=6)\n",
        "df['EMA_10'] = ta.trend.ema_indicator(df['Close'], window=10)\n",
        "df['EMA_14'] = ta.trend.ema_indicator(df['Close'], window=14)\n",
        "\n",
        "df['MACD'] = ta.trend.macd_diff(df['Close'], window_slow=20, window_fast=6)\n",
        "df['RSI_10'] = ta.momentum.rsi(df['Close'], window=10)\n",
        "df['RSI_14'] = ta.momentum.rsi(df['Close'], window=14)\n",
        "df['CCI_20'] = ta.trend.cci(df['High'], df['Low'], df['Close'], window=20)\n",
        "\n",
        "df['BOLL_UB'] = ta.volatility.bollinger_hband(df['Close'], window=20)\n",
        "df['BOLL_LB'] = ta.volatility.bollinger_lband(df['Close'], window=20)\n",
        "\n",
        "df['ATR_14'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], window=14)\n",
        "df['H-L']   = df['High'] - df['Low']\n",
        "df['H-Cp']  = (df['High'] - df['Close'].shift(1)).abs()\n",
        "df['L-Cp']  = (df['Low']  - df['Close'].shift(1)).abs()\n",
        "df['TR']    = df[['H-L','H-Cp','L-Cp']].max(axis=1)\n",
        "\n",
        "\n",
        "df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])\n",
        "df['MFI'] = ta.volume.money_flow_index(df['High'], df['Low'], df['Close'], df['Volume'], window=14)\n",
        "df['ForceIndex'] = ta.volume.force_index(df['Close'], df['Volume'], window=1)\n",
        "df['FI_5'] = ta.volume.force_index(df['Close'], df['Volume'], window=5)\n",
        "\n",
        "df['ROC_12'] = ta.momentum.roc(df['Close'], window=12)\n",
        "df['Williams_%R'] = ta.momentum.williams_r(df['High'], df['Low'], df['Close'], lbp=14)\n",
        "df['DPO_20'] = ta.trend.dpo(df['Close'], window=20)\n",
        "\n",
        "df['ADX_7'] = ta.trend.adx(df['High'], df['Low'], df['Close'], window=7)\n",
        "df['ADX_14'] = ta.trend.adx(df['High'], df['Low'], df['Close'], window=14)\n",
        "df['DX'] = ta.trend.adx_pos(df['High'], df['Low'], df['Close'], window=14) - ta.trend.adx_neg(df['High'], df['Low'], df['Close'], window=14)\n",
        "\n",
        "\n",
        "df['KST'] = ta.trend.kst(df['Close'])\n",
        "# Signal line = 9‑day SMA of KST\n",
        "df['KST_9'] = df['KST'].rolling(window=9).mean()\n",
        "\n",
        "\n",
        "df['SEMV'] = ta.volume.ease_of_movement(df['High'], df['Low'], df['Volume'])\n",
        "df['NVI'] = ta.volume.negative_volume_index(df['Close'], df['Volume'])\n",
        "\n",
        "# True range components\n",
        "df['H-L'] = df['High'] - df['Low']\n",
        "df['H-Cp'] = (df['High'] - df['Close'].shift(1)).abs()\n",
        "df['L-Cp'] = (df['Low'] - df['Close'].shift(1)).abs()\n",
        "\n",
        "# Final drop NaN\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(\"Computed all 43 technical indicators; resulting shape:\", df.shape)\n",
        "\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# —— 4️⃣ 归一化 ——\n",
        "scaler = MinMaxScaler()\n",
        "features = df.columns.tolist()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# —— 5️⃣ 构造监督序列 ——\n",
        "window = 50\n",
        "returns = 100 * np.log(df['Close'] / df['Close'].shift(1))\n",
        "returns = returns.dropna().values\n",
        "\n",
        "X, y_reg, y_clf = [], [], []\n",
        "for i in range(len(returns) - window):\n",
        "    X.append(df.iloc[i:i+window].values)\n",
        "    y_reg.append(returns[i+window])\n",
        "    y_clf.append(int(returns[i+window] > 0))\n",
        "\n",
        "\n",
        "X = np.array(X)\n",
        "y_reg = np.array(y_reg)\n",
        "y_clf = np.array(y_clf)\n",
        "\n",
        "# —— 6️⃣ 保存结果 ——\n",
        "np.savez(\"SP500_preprocessed.npz\", X=X, y_reg=y_reg, y_clf=y_clf)\n",
        "\n",
        "print(\"Done! Shapes → X:\", X.shape, \"y_reg:\", y_reg.shape, \"y_clf:\", y_clf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFZ4vSjq2NtR",
        "outputId": "db148b78-8349-4420-8518-fe7ffb859ccd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d45b76edf271>:7: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df = pd.read_csv(\"SP500.csv\", index_col=0, parse_dates=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computed all 43 technical indicators; resulting shape: (3978, 36)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done! Shapes → X: (3927, 50, 36) y_reg: (3927,) y_clf: (3927,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train LSTM"
      ],
      "metadata": {
        "id": "hR35v7XbyhnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = np.load(\"SP500_preprocessed.npz\")\n",
        "X, y_reg, y_clf = data['X'], data['y_reg'], data['y_clf']\n",
        "\n",
        "# 按 70% train / 10% validation / 20% test 切分\n",
        "X_temp, X_test, y_reg_temp, y_reg_test, y_clf_temp, y_clf_test = train_test_split(\n",
        "    X, y_reg, y_clf, test_size=0.2, shuffle=False)\n",
        "\n",
        "X_train, X_val, y_reg_train, y_reg_val, y_clf_train, y_clf_val = train_test_split(\n",
        "    X_temp, y_reg_temp, y_clf_temp, test_size=0.125, shuffle=False)\n",
        "\n",
        "print(\"Shapes → Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WDNkV-gypcX",
        "outputId": "d66f1ef7-30d0-46bc-eb46-9256714e7c9b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes → Train: (2748, 50, 36) Val: (393, 50, 36) Test: (786, 50, 36)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = (~np.isnan(y_reg)) & (~np.isinf(y_reg))\n",
        "X, y_reg, y_clf = X[mask], y_reg[mask], y_clf[mask]\n"
      ],
      "metadata": {
        "id": "U1oZ5rpxzHra"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LFM"
      ],
      "metadata": {
        "id": "k8URpXWHyuSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Split data exactly as authors do\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_temp, X_test, y_reg_temp, y_reg_test, y_clf_temp, y_clf_test = train_test_split(X, y_reg, y_clf, test_size=0.2, shuffle=False)\n",
        "X_train, X_val, y_reg_train, y_reg_val, y_clf_train, y_clf_val = train_test_split(X_temp, y_reg_temp, y_clf_temp, test_size=0.125, shuffle=False)\n",
        "\n",
        "# 2️⃣ Build exactly as original script\n",
        "def build_model():\n",
        "    inp = Input(shape=(50, X.shape[2]))\n",
        "    x = LSTM(15, return_sequences=False)(inp)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    out_reg = Dense(1, name=\"regression\")(x)\n",
        "    out_clf = Dense(2, activation=\"softmax\", name=\"classification\")(x)\n",
        "    model = Model(inp, [out_reg, out_clf])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss={\"regression\":\"mse\", \"classification\":\"sparse_categorical_crossentropy\"},\n",
        "        loss_weights={\"regression\":0.1, \"classification\":1.0}\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# 3️⃣ Validate no NaNs in training batches\n",
        "print(\"Train X NaNs:\", np.isnan(X_train).sum(), \"y_reg NaNs:\", np.isnan(y_reg_train).sum())\n",
        "\n",
        "# 4️⃣ Train\n",
        "history = model.fit(\n",
        "    X_train, {\"regression\":y_reg_train, \"classification\":y_clf_train},\n",
        "    validation_data=(X_val, {\"regression\":y_reg_val, \"classification\":y_clf_val}),\n",
        "    batch_size=256, epochs=300, callbacks=[EarlyStopping(patience=20, restore_best_weights=True)]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKCjX-HEyvfb",
        "outputId": "cc778200-897a-482d-ccee-119336cadf97"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X NaNs: 0 y_reg NaNs: 0\n",
            "Epoch 1/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - classification_loss: 0.6887 - loss: 5.0865 - regression_loss: 43.8788 - val_classification_loss: 0.6907 - val_loss: 0.8137 - val_regression_loss: 1.2653\n",
            "Epoch 2/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - classification_loss: 0.6914 - loss: 5.1222 - regression_loss: 44.2889 - val_classification_loss: 0.6910 - val_loss: 0.8129 - val_regression_loss: 1.2580\n",
            "Epoch 3/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - classification_loss: 0.6890 - loss: 4.1000 - regression_loss: 34.0164 - val_classification_loss: 0.6914 - val_loss: 0.8134 - val_regression_loss: 1.2609\n",
            "Epoch 4/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6871 - loss: 3.7978 - regression_loss: 30.9953 - val_classification_loss: 0.6912 - val_loss: 0.8128 - val_regression_loss: 1.2579\n",
            "Epoch 5/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6905 - loss: 3.1821 - regression_loss: 25.5191 - val_classification_loss: 0.6908 - val_loss: 0.8125 - val_regression_loss: 1.2550\n",
            "Epoch 6/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6891 - loss: 4.6058 - regression_loss: 39.0919 - val_classification_loss: 0.6908 - val_loss: 0.8124 - val_regression_loss: 1.2557\n",
            "Epoch 7/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6898 - loss: 3.3434 - regression_loss: 26.4177 - val_classification_loss: 0.6910 - val_loss: 0.8129 - val_regression_loss: 1.2619\n",
            "Epoch 8/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6910 - loss: 4.1591 - regression_loss: 34.6381 - val_classification_loss: 0.6914 - val_loss: 0.8137 - val_regression_loss: 1.2672\n",
            "Epoch 9/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6890 - loss: 4.2982 - regression_loss: 35.9731 - val_classification_loss: 0.6912 - val_loss: 0.8140 - val_regression_loss: 1.2705\n",
            "Epoch 10/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6874 - loss: 4.0606 - regression_loss: 34.4244 - val_classification_loss: 0.6912 - val_loss: 0.8132 - val_regression_loss: 1.2627\n",
            "Epoch 11/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - classification_loss: 0.6881 - loss: 5.1969 - regression_loss: 45.0316 - val_classification_loss: 0.6912 - val_loss: 0.8127 - val_regression_loss: 1.2558\n",
            "Epoch 12/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6875 - loss: 3.3120 - regression_loss: 26.8937 - val_classification_loss: 0.6913 - val_loss: 0.8126 - val_regression_loss: 1.2557\n",
            "Epoch 13/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6915 - loss: 5.1771 - regression_loss: 44.8521 - val_classification_loss: 0.6911 - val_loss: 0.8130 - val_regression_loss: 1.2588\n",
            "Epoch 14/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - classification_loss: 0.6883 - loss: 4.1348 - regression_loss: 34.4424 - val_classification_loss: 0.6921 - val_loss: 0.8136 - val_regression_loss: 1.2604\n",
            "Epoch 15/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - classification_loss: 0.6874 - loss: 6.4119 - regression_loss: 57.1726 - val_classification_loss: 0.6914 - val_loss: 0.8136 - val_regression_loss: 1.2652\n",
            "Epoch 16/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6888 - loss: 3.6089 - regression_loss: 29.3186 - val_classification_loss: 0.6911 - val_loss: 0.8133 - val_regression_loss: 1.2654\n",
            "Epoch 17/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - classification_loss: 0.6891 - loss: 3.1247 - regression_loss: 25.0110 - val_classification_loss: 0.6914 - val_loss: 0.8132 - val_regression_loss: 1.2633\n",
            "Epoch 18/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - classification_loss: 0.6867 - loss: 6.9737 - regression_loss: 62.8267 - val_classification_loss: 0.6919 - val_loss: 0.8134 - val_regression_loss: 1.2601\n",
            "Epoch 19/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - classification_loss: 0.6879 - loss: 6.4605 - regression_loss: 57.6519 - val_classification_loss: 0.6915 - val_loss: 0.8131 - val_regression_loss: 1.2595\n",
            "Epoch 20/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - classification_loss: 0.6884 - loss: 3.5646 - regression_loss: 28.6579 - val_classification_loss: 0.6910 - val_loss: 0.8126 - val_regression_loss: 1.2591\n",
            "Epoch 21/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6886 - loss: 3.4917 - regression_loss: 28.0141 - val_classification_loss: 0.6910 - val_loss: 0.8129 - val_regression_loss: 1.2612\n",
            "Epoch 22/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - classification_loss: 0.6866 - loss: 4.3437 - regression_loss: 36.5807 - val_classification_loss: 0.6912 - val_loss: 0.8125 - val_regression_loss: 1.2591\n",
            "Epoch 23/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6888 - loss: 4.4119 - regression_loss: 37.2597 - val_classification_loss: 0.6910 - val_loss: 0.8119 - val_regression_loss: 1.2546\n",
            "Epoch 24/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6875 - loss: 6.0206 - regression_loss: 53.2372 - val_classification_loss: 0.6913 - val_loss: 0.8119 - val_regression_loss: 1.2518\n",
            "Epoch 25/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6891 - loss: 3.7243 - regression_loss: 30.2976 - val_classification_loss: 0.6912 - val_loss: 0.8119 - val_regression_loss: 1.2528\n",
            "Epoch 26/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6878 - loss: 4.2230 - regression_loss: 35.2664 - val_classification_loss: 0.6908 - val_loss: 0.8114 - val_regression_loss: 1.2512\n",
            "Epoch 27/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - classification_loss: 0.6870 - loss: 5.6234 - regression_loss: 49.2712 - val_classification_loss: 0.6908 - val_loss: 0.8113 - val_regression_loss: 1.2509\n",
            "Epoch 28/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - classification_loss: 0.6893 - loss: 3.9442 - regression_loss: 32.5259 - val_classification_loss: 0.6903 - val_loss: 0.8109 - val_regression_loss: 1.2487\n",
            "Epoch 29/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6881 - loss: 5.8301 - regression_loss: 51.3123 - val_classification_loss: 0.6909 - val_loss: 0.8116 - val_regression_loss: 1.2517\n",
            "Epoch 30/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - classification_loss: 0.6863 - loss: 3.8881 - regression_loss: 31.9848 - val_classification_loss: 0.6906 - val_loss: 0.8115 - val_regression_loss: 1.2514\n",
            "Epoch 31/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6891 - loss: 3.8088 - regression_loss: 31.1461 - val_classification_loss: 0.6909 - val_loss: 0.8114 - val_regression_loss: 1.2508\n",
            "Epoch 32/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6876 - loss: 5.0761 - regression_loss: 43.9911 - val_classification_loss: 0.6905 - val_loss: 0.8110 - val_regression_loss: 1.2494\n",
            "Epoch 33/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6883 - loss: 3.3070 - regression_loss: 26.0859 - val_classification_loss: 0.6904 - val_loss: 0.8109 - val_regression_loss: 1.2482\n",
            "Epoch 34/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6894 - loss: 3.5704 - regression_loss: 28.9140 - val_classification_loss: 0.6905 - val_loss: 0.8112 - val_regression_loss: 1.2489\n",
            "Epoch 35/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - classification_loss: 0.6864 - loss: 3.7921 - regression_loss: 31.1325 - val_classification_loss: 0.6910 - val_loss: 0.8115 - val_regression_loss: 1.2517\n",
            "Epoch 36/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - classification_loss: 0.6904 - loss: 3.8431 - regression_loss: 31.4688 - val_classification_loss: 0.6913 - val_loss: 0.8116 - val_regression_loss: 1.2489\n",
            "Epoch 37/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6876 - loss: 3.6556 - regression_loss: 30.2937 - val_classification_loss: 0.6914 - val_loss: 0.8116 - val_regression_loss: 1.2481\n",
            "Epoch 38/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - classification_loss: 0.6885 - loss: 3.6961 - regression_loss: 30.0330 - val_classification_loss: 0.6911 - val_loss: 0.8118 - val_regression_loss: 1.2505\n",
            "Epoch 39/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - classification_loss: 0.6864 - loss: 3.3786 - regression_loss: 26.9627 - val_classification_loss: 0.6915 - val_loss: 0.8122 - val_regression_loss: 1.2517\n",
            "Epoch 40/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - classification_loss: 0.6894 - loss: 3.5890 - regression_loss: 28.8805 - val_classification_loss: 0.6913 - val_loss: 0.8126 - val_regression_loss: 1.2563\n",
            "Epoch 41/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6870 - loss: 4.1408 - regression_loss: 34.5387 - val_classification_loss: 0.6905 - val_loss: 0.8114 - val_regression_loss: 1.2505\n",
            "Epoch 42/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6849 - loss: 3.6998 - regression_loss: 30.1083 - val_classification_loss: 0.6904 - val_loss: 0.8115 - val_regression_loss: 1.2491\n",
            "Epoch 43/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6884 - loss: 4.6622 - regression_loss: 39.6828 - val_classification_loss: 0.6906 - val_loss: 0.8118 - val_regression_loss: 1.2479\n",
            "Epoch 44/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6899 - loss: 6.2683 - regression_loss: 55.9194 - val_classification_loss: 0.6904 - val_loss: 0.8112 - val_regression_loss: 1.2449\n",
            "Epoch 45/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6894 - loss: 5.4581 - regression_loss: 47.5690 - val_classification_loss: 0.6905 - val_loss: 0.8111 - val_regression_loss: 1.2444\n",
            "Epoch 46/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - classification_loss: 0.6867 - loss: 3.4247 - regression_loss: 27.6107 - val_classification_loss: 0.6904 - val_loss: 0.8110 - val_regression_loss: 1.2460\n",
            "Epoch 47/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - classification_loss: 0.6860 - loss: 3.9758 - regression_loss: 32.7909 - val_classification_loss: 0.6904 - val_loss: 0.8112 - val_regression_loss: 1.2457\n",
            "Epoch 48/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6866 - loss: 4.8617 - regression_loss: 41.6355 - val_classification_loss: 0.6907 - val_loss: 0.8117 - val_regression_loss: 1.2488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## paper logic:100 LSTM base model + learning rate halved every 50 epochs + 300 epochs fixed training + 13 randomly drawn features + bootstrap samples + final average prediction\n"
      ],
      "metadata": {
        "id": "8ObU0YAi0zR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "def build_model(input_dim):\n",
        "    inp = Input(shape=(50, input_dim))\n",
        "    x = LSTM(15)(inp)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    out_reg = Dense(1, name=\"regression\")(x)\n",
        "    out_clf = Dense(2, activation=\"softmax\", name=\"classification\")(x)\n",
        "    model = Model(inp, [out_reg, out_clf])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss={\"regression\":\"mse\", \"classification\":\"sparse_categorical_crossentropy\"},\n",
        "        loss_weights={\"regression\":0.1, \"classification\":1.0}\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Learning rate schedule: halve every 50 epochs\n",
        "def lr_schedule(epoch):\n",
        "    return 1e-3 * (0.5 ** (epoch // 50))\n",
        "\n",
        "lr_cb = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "ensemble_preds_reg = []\n",
        "ensemble_preds_clf = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Randomly sample 13 features\n",
        "    features_idx = random.sample(range(X_train.shape[2]), 13)\n",
        "    X_tr_sub = X_train[:,:,features_idx]\n",
        "    X_val_sub = X_val[:,:,features_idx]\n",
        "    X_test_sub = X_test[:,:,features_idx]\n",
        "\n",
        "    # Bootstrap sample training data\n",
        "    idx = np.random.choice(len(X_tr_sub), size=len(X_tr_sub), replace=True)\n",
        "    X_boot, y_reg_boot, y_clf_boot = X_tr_sub[idx], y_reg_train[idx], y_clf_train[idx]\n",
        "\n",
        "    # Build & train\n",
        "    model_i = build_model(input_dim=X_train.shape[2] if False else X_boot.shape[2])\n",
        "    model_i.fit(\n",
        "        X_boot, {\"regression\": y_reg_boot, \"classification\": y_clf_boot},\n",
        "        validation_data=(X_val_sub, {\"regression\": y_reg_val, \"classification\": y_clf_val}),\n",
        "        epochs=300, batch_size=256, callbacks=[lr_cb], verbose=0\n",
        "    )\n",
        "\n",
        "    # Predict on test\n",
        "    reg_pred, clf_pred = model_i.predict(X_test_sub, verbose=0)\n",
        "    ensemble_preds_reg.append(reg_pred.flatten())\n",
        "    ensemble_preds_clf.append(clf_pred)\n",
        "\n",
        "# Average ensemble outputs\n",
        "final_reg = np.mean(np.vstack(ensemble_preds_reg), axis=0)\n",
        "final_clf = np.mean(np.stack(ensemble_preds_clf), axis=0).argmax(axis=1)\n",
        "\n",
        "print(\"Ensemble Test RMSE:\", np.sqrt(((final_reg - y_reg_test)**2).mean()))\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "print(\"Ensemble Test BACC:\", balanced_accuracy_score(y_clf_test, final_clf))\n"
      ],
      "metadata": {
        "id": "Eex-ZZcv0-tI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}