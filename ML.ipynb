{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ySDWSlDM87pX1-E3C-eWdrDXdwPa5Yxp",
      "authorship_tag": "ABX9TyNbmmV8a5J2UwmGo1Tz9sJA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kathy42xu/DL_TA/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data extraction\n"
      ],
      "metadata": {
        "id": "mY9-4lL03LUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "\n",
        "data_folder_path = './'\n",
        "\n",
        "num_files = 10\n",
        "file_prefix = 'factor_'\n",
        "file_suffix = '.csv'\n",
        "\n",
        "all_data_parts = []\n",
        "column_names = None\n",
        "\n",
        "print(f\"--- Starting Data Combination ---\")\n",
        "start_load_time = time.time()\n",
        "print(f\"Attempting to load files from {data_folder_path}...\")\n",
        "print(\"Expecting header ONLY in factor_1.csv\")\n",
        "\n",
        "if not os.path.isdir(data_folder_path):\n",
        "    print(f\"ERROR: Directory not found: {data_folder_path}\")\n",
        "    print(\"Please ensure you have uploaded the 'data' folder containing the CSV files.\")\n",
        "    # Handle error appropriately, maybe raise exception or exit\n",
        "    combined_data = None # Set combined_data to None or handle error\n",
        "else:\n",
        "    # --- Load the first file (factor_1.csv) WITH header ---\n",
        "    file_name_1 = f\"{file_prefix}1{file_suffix}\"\n",
        "    file_path_1 = os.path.join(data_folder_path, file_name_1)\n",
        "\n",
        "    if os.path.exists(file_path_1):\n",
        "        print(f\"  Loading {file_path_1} (with header)...\")\n",
        "        try:\n",
        "            df_first = pd.read_csv(file_path_1)\n",
        "            column_names = df_first.columns.tolist() # Store column names\n",
        "            all_data_parts.append(df_first)\n",
        "            print(f\"    Successfully loaded {file_name_1}. Shape: {df_first.shape}\")\n",
        "            if column_names:\n",
        "                 print(f\"    Stored {len(column_names)} column names.\")\n",
        "            else:\n",
        "                 print(\"    Warning: No column names found in header file.\")\n",
        "                 column_names = None # Treat as failure if no columns read\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR loading header file {file_name_1}: {e}\")\n",
        "            column_names = None # Ensure it's None if loading failed\n",
        "    else:\n",
        "        print(f\"  ERROR: Header file not found - {file_path_1}. Cannot proceed.\")\n",
        "        column_names = None # Ensure it's None if file not found\n",
        "\n",
        "    # --- Load subsequent files (factor_2.csv to factor_10.csv) WITHOUT header ---\n",
        "    if column_names is not None: # Proceed only if factor_1 was loaded successfully\n",
        "        for i in range(2, num_files + 1):\n",
        "            file_name = f\"{file_prefix}{i}{file_suffix}\"\n",
        "            file_path = os.path.join(data_folder_path, file_name)\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                print(f\"  Loading {file_path} (WITHOUT header)...\")\n",
        "                try:\n",
        "                    # Load WITHOUT header\n",
        "                    df_temp = pd.read_csv(file_path, header=None)\n",
        "\n",
        "                    # Important Check: Verify column count matches factor_1\n",
        "                    if len(df_temp.columns) != len(column_names):\n",
        "                         print(f\"    WARNING: {file_name} has {len(df_temp.columns)} columns, but header has {len(column_names)}. Skipping this file.\")\n",
        "                         continue # Skip this file\n",
        "\n",
        "                    # Assign correct column names HERE\n",
        "                    df_temp.columns = column_names\n",
        "\n",
        "                    # Add the DataFrame (now with correct names) to the list\n",
        "                    all_data_parts.append(df_temp)\n",
        "                    # print(f\"    Successfully loaded {file_name}. Shape: {df_temp.shape}\") # Reduced verbosity\n",
        "                except Exception as e:\n",
        "                    print(f\"    ERROR loading {file_name}: {e}\")\n",
        "                    # continue # Optional: skip failing files\n",
        "            else:\n",
        "                print(f\"  WARNING: File not found - {file_path}. Skipping.\")\n",
        "\n",
        "    # --- Concatenate ---\n",
        "    combined_data = None # Initialize in case of errors\n",
        "    if not all_data_parts:\n",
        "        print(\"\\nERROR: No data parts were loaded. Cannot concatenate.\")\n",
        "    elif column_names is None:\n",
        "         print(\"\\nERROR: Could not load header from factor_1.csv. Cannot proceed.\")\n",
        "    else:\n",
        "        print(\"\\nConcatenating all loaded data parts...\")\n",
        "        # Concatenate DataFrames. All parts should now have correct column names.\n",
        "        combined_data = pd.concat(all_data_parts, ignore_index=True)\n",
        "        print(\"Concatenation complete!\")\n",
        "\n",
        "        # Final check on columns (should pass now)\n",
        "        if len(combined_data.columns) != len(column_names):\n",
        "             print(\"ERROR: Final column count mismatch after assigning names. Check logic.\")\n",
        "             combined_data = None # Invalidate data if something went wrong\n",
        "        else:\n",
        "             # Display the shape and head/tail of the final combined DataFrame\n",
        "             print(\"\\nShape of the final combined DataFrame:\")\n",
        "             print(combined_data.shape)\n",
        "             print(\"\\nFirst 5 rows:\")\n",
        "             print(combined_data.head())\n",
        "             print(\"\\nLast 5 rows:\")\n",
        "             print(combined_data.tail())\n",
        "\n",
        "# --- End of Data Combination Cell ---\n",
        "load_end_time = time.time()\n",
        "print(f\"\\n--- Data Combination Finished ---\")\n",
        "if combined_data is not None:\n",
        "    print(f\"Variable 'combined_data' created. Shape: {combined_data.shape}\")\n",
        "    print(f\"Time taken: {load_end_time - start_load_time:.2f} seconds\")\n",
        "else:\n",
        "    print(\"Variable 'combined_data' could not be created due to errors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRxNrtgS7Ra-",
        "outputId": "69139ecf-2d44-419e-9941-8191f9ac6b82"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Data Combination ---\n",
            "Attempting to load files from ./...\n",
            "Expecting header ONLY in factor_1.csv\n",
            "  Loading ./factor_1.csv (with header)...\n",
            "    Successfully loaded factor_1.csv. Shape: (33700, 138)\n",
            "    Stored 138 column names.\n",
            "  Loading ./factor_2.csv (WITHOUT header)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8893eb8eabda>:64: DtypeWarning: Columns (13,51,61,64,65,66,81,84,85,98,103,109,132,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(file_path, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading ./factor_3.csv (WITHOUT header)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8893eb8eabda>:64: DtypeWarning: Columns (12,47,51,60,64,65,66,84,91,94,98,103,109,132,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(file_path, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading ./factor_4.csv (WITHOUT header)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8893eb8eabda>:64: DtypeWarning: Columns (12,46,63,64,65,84,90,109,132,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(file_path, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading ./factor_5.csv (WITHOUT header)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8893eb8eabda>:64: DtypeWarning: Columns (29,30,35,44,45,46,47,59,72,75,77,79,86,90,94,103,105,110) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(file_path, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading ./factor_6.csv (WITHOUT header)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8893eb8eabda>:64: DtypeWarning: Columns (8,12,44,45,46,47,63,64,65,66,70,71,72,73,74,75,76,77,81,84,86,90,94,103,132,133,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(file_path, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading ./factor_7.csv (WITHOUT header)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8893eb8eabda>:64: DtypeWarning: Columns (12,63,132,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(file_path, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading ./factor_8.csv (WITHOUT header)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8893eb8eabda>:64: DtypeWarning: Columns (12,30,63,84,132,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(file_path, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading ./factor_9.csv (WITHOUT header)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8893eb8eabda>:64: DtypeWarning: Columns (44,45,46,47,75,81,84,86,90,94,103,132,137) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(file_path, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Loading ./factor_10.csv (WITHOUT header)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-8893eb8eabda>:64: DtypeWarning: Columns (44,45,46,47,72,73,74,75,76,77,84,86,90,94,103,109) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df_temp = pd.read_csv(file_path, header=None)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Concatenating all loaded data parts...\n",
            "Concatenation complete!\n",
            "\n",
            "Shape of the final combined DataFrame:\n",
            "(336740, 138)\n",
            "\n",
            "First 5 rows:\n",
            "         date  ticker    target  market_return  excess_market_return  \\\n",
            "0  2002-05-01   47080 -0.277193      -0.000009             -0.004017   \n",
            "1  2002-05-01   17040 -0.384848      -0.000009             -0.004017   \n",
            "2  2002-05-01   31510 -0.039185      -0.000009             -0.004017   \n",
            "3  2002-05-01   65060 -0.106349      -0.000009             -0.004017   \n",
            "4  2002-05-01    2310  0.039370      -0.000009             -0.004017   \n",
            "\n",
            "   ff3_bin_return  smb  hml const  beta  ...  hml_bin_prec_9.0  \\\n",
            "0             NaN  NaN  NaN   NaN   NaN  ...               0.0   \n",
            "1             NaN  NaN  NaN   NaN   NaN  ...               0.0   \n",
            "2             NaN  NaN  NaN   NaN   NaN  ...               0.0   \n",
            "3             NaN  NaN  NaN   NaN   NaN  ...               0.0   \n",
            "4             NaN  NaN  NaN   NaN   NaN  ...               1.0   \n",
            "\n",
            "   hml_bin_prec_10.0 vix_cat_mid vix_cat_high  div_ret  tb3y  tb5y  tb10y  \\\n",
            "0                0.0         0.0          1.0      0.0   6.3   6.8   7.09   \n",
            "1                0.0         0.0          1.0      0.0   6.3   6.8   7.09   \n",
            "2                0.0         0.0          1.0      0.0   6.3   6.8   7.09   \n",
            "3                0.0         0.0          1.0      0.0   6.3   6.8   7.09   \n",
            "4                0.0         0.0          1.0     5.27   6.3   6.8   7.09   \n",
            "\n",
            "   cb3y    div  \n",
            "0  7.07    0.0  \n",
            "1  7.07    0.0  \n",
            "2  7.07    0.0  \n",
            "3  7.07    0.0  \n",
            "4  7.07  400.0  \n",
            "\n",
            "[5 rows x 138 columns]\n",
            "\n",
            "Last 5 rows:\n",
            "              date  ticker  target  market_return  excess_market_return  \\\n",
            "336735  2021-09-01   88130     NaN      -0.022537             -0.023354   \n",
            "336736  2021-09-01   32540     NaN      -0.022537             -0.023354   \n",
            "336737  2021-09-01   28260     NaN      -0.022537             -0.023354   \n",
            "336738  2021-09-01  170900     NaN      -0.022537             -0.023354   \n",
            "336739  2021-09-01   98120     NaN      -0.022537             -0.023354   \n",
            "\n",
            "        ff3_bin_return       smb       hml const  beta  ...  hml_bin_prec_9.0  \\\n",
            "336735       -0.015641 -0.045283  0.021204   NaN   NaN  ...               1.0   \n",
            "336736       -0.015641 -0.045283  0.021204   NaN   NaN  ...               0.0   \n",
            "336737        0.001817 -0.045283  0.021204   NaN   NaN  ...               1.0   \n",
            "336738        0.001817 -0.045283  0.021204   NaN   NaN  ...               0.0   \n",
            "336739       -0.038560 -0.045283  0.021204   NaN   NaN  ...               0.0   \n",
            "\n",
            "        hml_bin_prec_10.0 vix_cat_mid vix_cat_high  div_ret  tb3y  tb5y  \\\n",
            "336735                0.0         0.0          1.0     1.22  1.52  1.79   \n",
            "336736                0.0         0.0          1.0      0.0  1.52  1.79   \n",
            "336737                0.0         0.0          1.0     1.73  1.52  1.79   \n",
            "336738                0.0         0.0          1.0     1.29  1.52  1.79   \n",
            "336739                0.0         0.0          1.0     0.28  1.52  1.79   \n",
            "\n",
            "        tb10y  cb3y     div  \n",
            "336735   2.06  1.95   150.0  \n",
            "336736   2.06  1.95     0.0  \n",
            "336737   2.06  1.95  2300.0  \n",
            "336738   2.06  1.95  1000.0  \n",
            "336739   2.06  1.95    25.0  \n",
            "\n",
            "[5 rows x 138 columns]\n",
            "\n",
            "--- Data Combination Finished ---\n",
            "Variable 'combined_data' created. Shape: (336740, 138)\n",
            "Time taken: 12.30 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OLS-3+H Model (Huber Loss)"
      ],
      "metadata": {
        "id": "UFmI8o7y3efA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.linear_model import HuberRegressor # Using Huber for robust regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# (Keep the same configuration as before: data_folder_path, feature_cols, target_col, date ranges etc.)\n",
        "data_folder_path = './'\n",
        "num_files = 10\n",
        "file_prefix = 'factor_'\n",
        "file_suffix = '.csv'\n",
        "\n",
        "feature_cols = [\n",
        "    'size_rnk',\n",
        "    'BPR',\n",
        "    'mom12'\n",
        "]\n",
        "target_col = 'target'\n",
        "date_col = 'date'\n",
        "ticker_col = 'ticker'\n",
        "\n",
        "train_start_year = 2002\n",
        "# train_end_year = 2015 # Determined dynamically\n",
        "test_start_year = 2019\n",
        "test_end_year = 2020\n",
        "\n",
        "\n",
        "# --- Functions ---\n",
        "def preprocess_combined_data(df, date_col, ticker_col, target_col, feature_cols):\n",
        "    \"\"\"Preprocesses the combined dataframe with numeric conversion and median+0 imputation for features.\"\"\"\n",
        "    print(\"--- Starting Data Preprocessing ---\")\n",
        "    if df is None:\n",
        "        print(\"ERROR: Input DataFrame is None.\")\n",
        "        return None\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        print(\"ERROR: Input is not a pandas DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    df_processed = df.copy()\n",
        "    print(f\"Input shape: {df_processed.shape}\")\n",
        "\n",
        "    # Convert date column if it's not the index already\n",
        "    if date_col in df_processed.columns:\n",
        "        print(f\"Converting '{date_col}' to datetime...\")\n",
        "        df_processed[date_col] = pd.to_datetime(df_processed[date_col], errors='coerce')\n",
        "        df_processed = df_processed.dropna(subset=[date_col])\n",
        "        df_processed = df_processed.set_index(date_col)\n",
        "    elif not isinstance(df_processed.index, pd.DatetimeIndex):\n",
        "        print(f\"ERROR: DataFrame index is not a DatetimeIndex and '{date_col}' column not found.\")\n",
        "        return None\n",
        "    print(\"Date index set.\")\n",
        "\n",
        "    # Check for required columns AFTER potential date conversion drops\n",
        "    all_cols_needed = [ticker_col, target_col] + feature_cols\n",
        "    missing_cols = [col for col in all_cols_needed if col not in df_processed.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: Missing required columns: {missing_cols}\")\n",
        "        return None\n",
        "\n",
        "    # Select necessary columns AFTER setting index\n",
        "    df_processed = df_processed[[ticker_col, target_col] + feature_cols]\n",
        "\n",
        "    # --- Force Feature Columns to Numeric (Convert non-numeric to NaN) ---\n",
        "    print(\"Attempting to convert feature columns to numeric...\")\n",
        "    coerced_count = 0\n",
        "    for col in feature_cols:\n",
        "         if col in df_processed.columns:\n",
        "              initial_non_numeric = pd.to_numeric(df_processed[col], errors='coerce').isna().sum() - df_processed[col].isna().sum()\n",
        "              if initial_non_numeric > 0:\n",
        "                   print(f\"  Found {initial_non_numeric} non-numeric value(s) in '{col}'. Coercing to NaN.\")\n",
        "                   coerced_count += initial_non_numeric\n",
        "              df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "         else:\n",
        "              print(f\"Warning: Feature column {col} not found during numeric conversion.\")\n",
        "    if coerced_count > 0:\n",
        "         print(f\"Coerced a total of {coerced_count} non-numeric entries to NaN across features.\")\n",
        "\n",
        "    # Handle infinite values\n",
        "    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # --- Handle TARGET Variable Missing Values ---\n",
        "    initial_rows = len(df_processed)\n",
        "    n_target_na_initial = df_processed[target_col].isna().sum()\n",
        "    if n_target_na_initial > 0:\n",
        "         print(f\"Found {n_target_na_initial} missing values in '{target_col}'. Dropping rows...\")\n",
        "    df_processed.dropna(subset=[target_col], inplace=True)\n",
        "    rows_after_target_drop = len(df_processed)\n",
        "    rows_dropped = initial_rows - rows_after_target_drop\n",
        "    if rows_dropped > 0:\n",
        "         print(f\"Dropped {rows_dropped} rows due to missing '{target_col}'.\")\n",
        "    print(f\"Shape after handling target NaNs: {df_processed.shape}\")\n",
        "    if rows_after_target_drop == 0:\n",
        "         print(\"ERROR: All rows dropped due to missing target. Cannot proceed.\")\n",
        "         return None\n",
        "\n",
        "    # --- Handle FEATURE Missing Values (Median then 0 Imputation) ---\n",
        "    # Now includes NaNs created by coercing strings\n",
        "    print(\"Applying Median+0 imputation to features (including coerced NaNs)...\")\n",
        "    df_processed['month_period'] = df_processed.index.to_period('M')\n",
        "    imputed_count_zero = 0\n",
        "    for col in feature_cols:\n",
        "        if col in df_processed.columns:\n",
        "            n_initial_na = df_processed[col].isna().sum()\n",
        "            if n_initial_na > 0:\n",
        "                 # print(f\"  Processing feature '{col}': Found {n_initial_na} NaNs.\")\n",
        "                 # Use transform for median imputation\n",
        "                 df_processed[col] = df_processed.groupby('month_period')[col].transform(lambda x: x.fillna(x.median()))\n",
        "                 n_remaining_na = df_processed[col].isna().sum()\n",
        "                 if n_remaining_na > 0:\n",
        "                      df_processed[col].fillna(0, inplace=True)\n",
        "                      # print(f\"    Filled {n_remaining_na} remaining NAs with 0 in '{col}'.\")\n",
        "                      imputed_count_zero += n_remaining_na\n",
        "        else:\n",
        "             print(f\"Warning: Feature column {col} not found during imputation.\")\n",
        "    df_processed.drop(columns=['month_period'], inplace=True)\n",
        "    print(f\"Imputation complete. Filled {imputed_count_zero} values with 0 after median imputation.\")\n",
        "\n",
        "    # Final check for NaNs\n",
        "    nans_in_features = df_processed[feature_cols].isna().sum().sum()\n",
        "    if nans_in_features > 0:\n",
        "         print(f\"WARNING: {nans_in_features} NaNs still remain in feature columns after imputation!\")\n",
        "\n",
        "    print(f\"Preprocessing complete. Final shape: {df_processed.shape}\")\n",
        "    if not df_processed.empty:\n",
        "        print(f\"Final data date range: {df_processed.index.min().strftime('%Y-%m-%d')} to {df_processed.index.max().strftime('%Y-%m-%d')}\")\n",
        "    return df_processed\n",
        "\n",
        "# --- R-squared Calculation Functions ---\n",
        "def calculate_r2_oos(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates Out-of-Sample R-squared using the definition from the paper:\n",
        "    R2_OOS = 1 - sum( (y_true - y_pred)^2 ) / sum( y_true^2 )\n",
        "    This benchmarks against a naive forecast of zero.\n",
        "    \"\"\"\n",
        "    numerator = ((y_true - y_pred) ** 2).sum()\n",
        "    denominator = (y_true ** 2).sum()\n",
        "    # Handle cases where the sum of squared true values is zero or very close to zero\n",
        "    if np.isclose(denominator, 0):\n",
        "        # If numerator is also zero, perfect fit (or all zeros); otherwise, undefined or -inf\n",
        "        return 1.0 if np.isclose(numerator, 0) else -np.inf\n",
        "    return 1 - (numerator / denominator)\n",
        "\n",
        "def calculate_r2_is(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates In-Sample R-squared using the *same* formula structure as R2_OOS\n",
        "    for consistency in this specific replication context.\n",
        "    R2_IS = 1 - sum( (y_train_true - y_train_pred)^2 ) / sum( y_train_true^2 )\n",
        "    \"\"\"\n",
        "    # This uses the same logic as calculate_r2_oos but is applied to training data\n",
        "    return calculate_r2_oos(y_true, y_pred)\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(\"\\n--- Starting OLS-3+H Model Fitting (Huber Loss) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Ensure the preprocessed data DataFrame 'data' exists from the previous cell run\n",
        "# If 'combined_data' was the input to preprocessing in Cell 2, check for that first.\n",
        "if 'combined_data' not in locals() and 'combined_data' not in globals():\n",
        "     print(\"ERROR: 'combined_data' DataFrame not found. Please run the data combination cell first.\")\n",
        "     data = None\n",
        "elif 'data' not in locals() and 'data' not in globals():\n",
        "     # If 'data' doesn't exist, try to run preprocessing again\n",
        "     print(\"Preprocessed 'data' not found, attempting preprocessing...\")\n",
        "     if 'combined_data' in locals() or 'combined_data' in globals():\n",
        "         data = preprocess_combined_data(combined_data, date_col, ticker_col, target_col, feature_cols)\n",
        "     else:\n",
        "         data = None # Can't preprocess if combined_data is missing\n",
        "elif data is None or data.empty:\n",
        "    print(\"ERROR: Preprocessed DataFrame 'data' is None or empty.\")\n",
        "    # No need to set huber_results = None here, it's handled later\n",
        "\n",
        "\n",
        "# Proceed with model fitting only if preprocessing was successful\n",
        "if data is not None and not data.empty:\n",
        "    all_oos_predictions = []\n",
        "    all_oos_true_values = []\n",
        "    all_is_predictions = []  # To store in-sample predictions for overall IS R2\n",
        "    all_is_true_values = []   # To store in-sample true values for overall IS R2\n",
        "\n",
        "    test_year_r2_oos = {}     # R2_OOS per test year\n",
        "    train_year_r2_is = {}    # R2_IS per train period ending year\n",
        "\n",
        "    available_years = sorted(data.index.year.unique())\n",
        "    actual_test_years = [y for y in available_years if y >= test_start_year and y <= test_end_year]\n",
        "\n",
        "    if not actual_test_years:\n",
        "        print(f\"\\nERROR: No data available in the specified test period ({test_start_year}-{test_end_year}) after preprocessing. Cannot proceed.\")\n",
        "        huber_results = None\n",
        "    else:\n",
        "        print(f\"\\nStarting OLS-3+H annual refitting from {min(actual_test_years)} to {max(actual_test_years)}...\")\n",
        "        print(f\"Using Training Start Year: {train_start_year}\")\n",
        "\n",
        "        for current_test_year in actual_test_years:\n",
        "            loop_start_time = time.time()\n",
        "            print(f\"\\n  Processing test year: {current_test_year}\")\n",
        "\n",
        "            current_train_end_year = current_test_year - 1\n",
        "            if current_train_end_year < train_start_year:\n",
        "                 print(f\"    Skipping year {current_test_year}: Training end year ({current_train_end_year}) is before training start year ({train_start_year}).\")\n",
        "                 continue\n",
        "\n",
        "            print(f\"    Training period: {train_start_year}-{current_train_end_year}\")\n",
        "            train_mask = (data.index.year >= train_start_year) & (data.index.year <= current_train_end_year)\n",
        "            test_mask = (data.index.year == current_test_year)\n",
        "            train_df = data.loc[train_mask]\n",
        "            test_df = data.loc[test_mask]\n",
        "\n",
        "            if train_df.empty or test_df.empty:\n",
        "                print(f\"    Skipping year {current_test_year}: Not enough data for train ({len(train_df)}) / test ({len(test_df)}) split.\")\n",
        "                continue\n",
        "\n",
        "            # Final check for NaNs before scaling (should be handled by preprocessing)\n",
        "            if train_df[feature_cols].isna().any().any() or test_df[feature_cols].isna().any().any():\n",
        "                 print(f\"    ERROR: NaNs detected in features for year {current_test_year} before scaling, after preprocessing! Skipping.\")\n",
        "                 continue\n",
        "\n",
        "            # Prepare data for scikit-learn\n",
        "            X_train = train_df[feature_cols].values\n",
        "            y_train = train_df[target_col].values\n",
        "            X_test = test_df[feature_cols].values\n",
        "            y_test = test_df[target_col].values\n",
        "\n",
        "            # Scale features\n",
        "            scaler = StandardScaler()\n",
        "            try:\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                 # Check for NaNs again AFTER scaling (can happen with constant cols)\n",
        "                if np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any():\n",
        "                    print(f\"    Warning: NaNs generated during scaling for year {current_test_year}. Check for constant columns in training data. Skipping year.\")\n",
        "                    # Optional: Impute NaNs caused by scaling constant columns if necessary\n",
        "                    # X_train_scaled = np.nan_to_num(X_train_scaled)\n",
        "                    # X_test_scaled = np.nan_to_num(X_test_scaled)\n",
        "                    continue # Skip this year if NaNs appear after scaling\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"    ERROR during scaling for year {current_test_year}: {e}. Skipping year.\")\n",
        "                continue\n",
        "\n",
        "            # --- Huber Regressor Model Training and Prediction ---\n",
        "            # Using HuberRegressor for robustness to outliers, as in the original cell\n",
        "            huber_model = HuberRegressor(fit_intercept=True, max_iter=300, tol=1e-4)\n",
        "\n",
        "            try:\n",
        "                # Check for NaNs before fitting/predicting\n",
        "                if np.isnan(X_train_scaled).any() or np.isnan(y_train).any():\n",
        "                     print(f\"    ERROR: NaNs detected in training data before fitting model for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "                if np.isnan(X_test_scaled).any():\n",
        "                     print(f\"    ERROR: NaNs detected in test features before prediction for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                # Fit the Huber model\n",
        "                huber_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "                # --- In-Sample Prediction ---\n",
        "                is_predictions = huber_model.predict(X_train_scaled)\n",
        "                all_is_predictions.extend(is_predictions)\n",
        "                all_is_true_values.extend(y_train)\n",
        "                r2_is_this_year = calculate_r2_is(y_train, is_predictions)\n",
        "                train_year_r2_is[current_train_end_year] = r2_is_this_year # Store IS R2 by training end year\n",
        "\n",
        "                # --- Out-of-Sample Prediction ---\n",
        "                oos_predictions = huber_model.predict(X_test_scaled)\n",
        "                all_oos_predictions.extend(oos_predictions)\n",
        "                all_oos_true_values.extend(y_test)\n",
        "                r2_oos_this_year = calculate_r2_oos(y_test, oos_predictions)\n",
        "                test_year_r2_oos[current_test_year] = r2_oos_this_year # Store OOS R2 by test year\n",
        "\n",
        "                print(f\"    Train Year {current_train_end_year} R2_IS : {r2_is_this_year:+.4f}\")\n",
        "                print(f\"    Test Year  {current_test_year} R2_OOS: {r2_oos_this_year:+.4f}\")\n",
        "                print(f\"    Time for year: {time.time() - loop_start_time:.2f}s\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    ERROR during model fitting/prediction for year {current_test_year}: {e}\")\n",
        "\n",
        "\n",
        "        # --- Overall Results ---\n",
        "        print(\"\\n--- Overall OLS-3+H Results (Huber Loss) ---\")\n",
        "\n",
        "        # In-Sample Overall\n",
        "        overall_r2_is = None # Initialize\n",
        "        if all_is_true_values:\n",
        "            overall_r2_is = calculate_r2_is(np.array(all_is_true_values), np.array(all_is_predictions))\n",
        "            print(f\"\\nOverall In-Sample R-squared (R2_IS) across all training periods: {overall_r2_is:.4f}\")\n",
        "            print(\"\\nR2_IS per training period (end year):\")\n",
        "            for year, r2 in sorted(train_year_r2_is.items()):\n",
        "                print(f\"  {train_start_year}-{year}: {r2:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo valid in-sample predictions were generated.\")\n",
        "\n",
        "        # Out-of-Sample Overall\n",
        "        overall_r2_oos = None # Initialize\n",
        "        if all_oos_true_values:\n",
        "            overall_r2_oos = calculate_r2_oos(np.array(all_oos_true_values), np.array(all_oos_predictions))\n",
        "            print(f\"\\nOverall Out-of-Sample R-squared (R2_OOS) for period {min(actual_test_years)}-{max(actual_test_years)}: {overall_r2_oos:.4f}\")\n",
        "            # Compare to OLS-3 benchmark from paper if desired\n",
        "            print(f\"(Paper's OLS-3 benchmark R2_OOS: 0.16% - Note: Direct comparison depends on exact data/setup)\")\n",
        "            print(\"\\nR2_OOS per test year:\")\n",
        "            for year, r2 in sorted(test_year_r2_oos.items()):\n",
        "                print(f\"  {year}: {r2:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo valid out-of-sample predictions were generated for the test period.\")\n",
        "\n",
        "        # Store results in a dictionary\n",
        "        huber_results = {\n",
        "            \"overall_r2_is\": overall_r2_is,\n",
        "            \"overall_r2_oos\": overall_r2_oos,\n",
        "            \"yearly_r2_is\": train_year_r2_is,\n",
        "            \"yearly_r2_oos\": test_year_r2_oos\n",
        "        }\n",
        "\n",
        "else:\n",
        "    # This block executes if data is None or empty after preprocessing attempt\n",
        "    print(\"\\nData preprocessing failed or resulted in empty DataFrame. Cannot proceed with model fitting.\")\n",
        "    huber_results = None # Indicate failure\n",
        "\n",
        "# Final summary\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal execution time for OLS-3+H cell: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Check if results were successfully generated\n",
        "if 'huber_results' in locals() and huber_results is not None:\n",
        "    print(\"\\nOLS-3+H model fitting complete. Results stored in 'huber_results' dictionary.\")\n",
        "elif 'huber_results' not in locals() or huber_results is None:\n",
        "     print(\"\\nOLS-3+H model fitting did not complete successfully due to errors or lack of data.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCKMvLix7nd6",
        "outputId": "a0e7e047-10a0-455e-b61a-1ba6ed51de7d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting OLS-3+H Model Fitting (Huber Loss) ---\n",
            "\n",
            "Starting OLS-3+H annual refitting from 2019 to 2020...\n",
            "Using Training Start Year: 2002\n",
            "\n",
            "  Processing test year: 2019\n",
            "    Training period: 2002-2018\n",
            "    Train Year 2018 R2_IS : +0.0001\n",
            "    Test Year  2019 R2_OOS: -0.0010\n",
            "    Time for year: 2.93s\n",
            "\n",
            "  Processing test year: 2020\n",
            "    Training period: 2002-2019\n",
            "    Train Year 2019 R2_IS : +0.0000\n",
            "    Test Year  2020 R2_OOS: -0.0011\n",
            "    Time for year: 2.30s\n",
            "\n",
            "--- Overall OLS-3+H Results (Huber Loss) ---\n",
            "\n",
            "Overall In-Sample R-squared (R2_IS) across all training periods: 0.0000\n",
            "\n",
            "R2_IS per training period (end year):\n",
            "  2002-2018: 0.0001\n",
            "  2002-2019: 0.0000\n",
            "\n",
            "Overall Out-of-Sample R-squared (R2_OOS) for period 2019-2020: -0.0011\n",
            "(Paper's OLS-3 benchmark R2_OOS: 0.16% - Note: Direct comparison depends on exact data/setup)\n",
            "\n",
            "R2_OOS per test year:\n",
            "  2019: -0.0010\n",
            "  2020: -0.0011\n",
            "\n",
            "Total execution time for OLS-3+H cell: 5.47 seconds\n",
            "\n",
            "OLS-3+H model fitting complete. Results stored in 'huber_results' dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OLS-All+H Model (Huber Loss, All Features)"
      ],
      "metadata": {
        "id": "l98T5T3Mfgvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.linear_model import HuberRegressor # Using Huber for robust regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# (Keep the same configuration as before: data_folder_path, num_files etc.)\n",
        "data_folder_path = './'\n",
        "num_files = 10\n",
        "file_prefix = 'factor_'\n",
        "file_suffix = '.csv'\n",
        "\n",
        "# feature_cols will be determined dynamically later\n",
        "# feature_cols = [...] # No longer hardcoded\n",
        "target_col = 'target'\n",
        "date_col = 'date'\n",
        "ticker_col = 'ticker'\n",
        "\n",
        "train_start_year = 2002\n",
        "# train_end_year = 2015 # Determined dynamically\n",
        "test_start_year = 2019\n",
        "test_end_year = 2020\n",
        "\n",
        "\n",
        "# --- Functions ---\n",
        "def preprocess_combined_data(df, date_col, ticker_col, target_col):\n",
        "    \"\"\"\n",
        "    Preprocesses the combined dataframe:\n",
        "    - Sets date index.\n",
        "    - Converts potential feature columns to numeric (coercing errors to NaN).\n",
        "    - Handles infinite values.\n",
        "    - Drops rows with missing target values.\n",
        "    - Imputes missing feature values using monthly median, then 0.\n",
        "    - Returns the processed DataFrame and the list of identified feature columns.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Data Preprocessing ---\")\n",
        "    if df is None:\n",
        "        print(\"ERROR: Input DataFrame is None.\")\n",
        "        return None, None\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        print(\"ERROR: Input is not a pandas DataFrame.\")\n",
        "        return None, None\n",
        "\n",
        "    df_processed = df.copy()\n",
        "    print(f\"Input shape: {df_processed.shape}\")\n",
        "\n",
        "    # 1. Convert date column and set index\n",
        "    if date_col in df_processed.columns:\n",
        "        print(f\"Converting '{date_col}' to datetime...\")\n",
        "        df_processed[date_col] = pd.to_datetime(df_processed[date_col], errors='coerce')\n",
        "        df_processed = df_processed.dropna(subset=[date_col]) # Drop rows where date conversion failed\n",
        "        df_processed = df_processed.set_index(date_col)\n",
        "        print(f\"Shape after date processing: {df_processed.shape}\")\n",
        "    elif not isinstance(df_processed.index, pd.DatetimeIndex):\n",
        "        print(f\"ERROR: DataFrame index is not a DatetimeIndex and '{date_col}' column not found.\")\n",
        "        return None, None\n",
        "    print(\"Date index set.\")\n",
        "\n",
        "    # 2. Identify potential feature columns (all columns except target and ticker)\n",
        "    potential_feature_cols = [col for col in df_processed.columns if col not in [target_col, ticker_col]]\n",
        "    print(f\"Identified {len(potential_feature_cols)} potential feature columns.\")\n",
        "    if not potential_feature_cols:\n",
        "        print(\"ERROR: No potential feature columns found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Keep only necessary columns for now (target, ticker, potential features)\n",
        "    cols_to_keep = [ticker_col, target_col] + potential_feature_cols\n",
        "    missing_cols = [col for col in cols_to_keep if col not in df_processed.columns]\n",
        "    if missing_cols:\n",
        "       # This check might be redundant if potential_feature_cols logic is correct, but good practice\n",
        "        print(f\"ERROR: Missing required columns after date processing: {missing_cols}\")\n",
        "        return None, None\n",
        "    df_processed = df_processed[cols_to_keep]\n",
        "    print(f\"Shape after selecting columns: {df_processed.shape}\")\n",
        "\n",
        "\n",
        "    # 3. Force Feature Columns to Numeric (Convert non-numeric to NaN)\n",
        "    print(\"Attempting to convert identified feature columns to numeric...\")\n",
        "    coerced_count = 0\n",
        "    for col in potential_feature_cols:\n",
        "         # Check if column still exists (it should, based on cols_to_keep)\n",
        "         if col in df_processed.columns:\n",
        "              # Check if column is not already numeric\n",
        "              if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                  initial_non_numeric = pd.to_numeric(df_processed[col], errors='coerce').isna().sum() - df_processed[col].isna().sum()\n",
        "                  if initial_non_numeric > 0:\n",
        "                       print(f\"  Found {initial_non_numeric} non-numeric value(s) in '{col}'. Coercing to NaN.\")\n",
        "                       coerced_count += initial_non_numeric\n",
        "                  df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "              #else:\n",
        "              #     print(f\" Column '{col}' is already numeric.\") # Optional: for debugging\n",
        "         else:\n",
        "              # This case should ideally not happen if cols_to_keep logic is correct\n",
        "              print(f\"Warning: Potential feature column '{col}' not found during numeric conversion phase.\")\n",
        "\n",
        "    if coerced_count > 0:\n",
        "         print(f\"Coerced a total of {coerced_count} non-numeric entries to NaN across features.\")\n",
        "\n",
        "    # 4. Handle infinite values\n",
        "    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    print(\"Handled infinite values.\")\n",
        "\n",
        "    # 5. Handle TARGET Variable Missing Values\n",
        "    initial_rows = len(df_processed)\n",
        "    n_target_na_initial = df_processed[target_col].isna().sum()\n",
        "    if n_target_na_initial > 0:\n",
        "         print(f\"Found {n_target_na_initial} missing values in '{target_col}'. Dropping rows...\")\n",
        "         df_processed.dropna(subset=[target_col], inplace=True)\n",
        "         rows_after_target_drop = len(df_processed)\n",
        "         rows_dropped = initial_rows - rows_after_target_drop\n",
        "         if rows_dropped > 0:\n",
        "              print(f\"Dropped {rows_dropped} rows due to missing '{target_col}'.\")\n",
        "    else:\n",
        "        print(f\"No missing values found in '{target_col}'.\")\n",
        "\n",
        "    print(f\"Shape after handling target NaNs: {df_processed.shape}\")\n",
        "    if df_processed.empty:\n",
        "         print(\"ERROR: All rows dropped due to missing target or date issues. Cannot proceed.\")\n",
        "         return None, None\n",
        "\n",
        "    # --- Handle FEATURE Missing Values (Median then 0 Imputation) ---\n",
        "    # Now includes NaNs created by coercing strings or from original data\n",
        "    print(\"Applying Median+0 imputation to features...\")\n",
        "    # Calculate monthly periods for grouping\n",
        "    try:\n",
        "        df_processed['month_period'] = df_processed.index.to_period('M')\n",
        "    except AttributeError as e:\n",
        "        print(f\"Error creating 'month_period': {e}. Ensure index is DatetimeIndex.\")\n",
        "        return None, None\n",
        "\n",
        "    imputed_count_median = 0\n",
        "    imputed_count_zero = 0\n",
        "    final_feature_cols = [] # Store columns that are actually used after checks\n",
        "\n",
        "    for col in potential_feature_cols:\n",
        "        if col in df_processed.columns:\n",
        "            n_initial_na = df_processed[col].isna().sum()\n",
        "            if n_initial_na > 0:\n",
        "                # print(f\"  Processing feature '{col}': Found {n_initial_na} NaNs.\") # Verbose\n",
        "                # Group by month and impute median within each month\n",
        "                try:\n",
        "                    # Calculate medians per group, handling potential all-NaN groups\n",
        "                    medians = df_processed.groupby('month_period')[col].median()\n",
        "                    # Fill NaNs using transform\n",
        "                    df_processed[col] = df_processed.groupby('month_period')[col].transform(lambda x: x.fillna(medians.loc[x.name]))\n",
        "                    imputed_this_median = n_initial_na - df_processed[col].isna().sum()\n",
        "                    imputed_count_median += imputed_this_median\n",
        "                except Exception as e:\n",
        "                    print(f\"    WARNING: Could not impute median for '{col}' using groupby/transform. Error: {e}. Check data for this column.\")\n",
        "                    # Fallback or skip? For now, we'll let the fillna(0) handle remaining NaNs.\n",
        "\n",
        "                # Impute remaining NaNs (if median was NaN for a whole group) with 0\n",
        "                n_remaining_na = df_processed[col].isna().sum()\n",
        "                if n_remaining_na > 0:\n",
        "                    df_processed[col].fillna(0, inplace=True)\n",
        "                    # print(f\"    Filled {n_remaining_na} remaining NAs with 0 in '{col}'.\") # Verbose\n",
        "                    imputed_count_zero += n_remaining_na\n",
        "            # Check if column is numeric after potential imputation\n",
        "            if pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                 final_feature_cols.append(col)\n",
        "            else:\n",
        "                 print(f\"  WARNING: Column '{col}' is not numeric after imputation. Excluding from final features.\")\n",
        "        else:\n",
        "             # Should not happen\n",
        "             print(f\"Warning: Potential feature column '{col}' not found during imputation phase.\")\n",
        "\n",
        "    df_processed.drop(columns=['month_period'], inplace=True) # Remove temporary column\n",
        "    print(f\"Imputation complete. Imputed ~{imputed_count_median} values with median, {imputed_count_zero} values with 0.\")\n",
        "\n",
        "    # Final check for NaNs in the final feature set\n",
        "    if not final_feature_cols:\n",
        "        print(\"ERROR: No numeric feature columns remain after preprocessing.\")\n",
        "        return None, None\n",
        "\n",
        "    nans_in_features = df_processed[final_feature_cols].isna().sum().sum()\n",
        "    if nans_in_features > 0:\n",
        "         print(f\"WARNING: {nans_in_features} NaNs still remain in final feature columns after imputation! This should not happen.\")\n",
        "         # Consider dropping columns/rows with remaining NaNs or using a different imputer\n",
        "         # For now, we will proceed, but the model might fail.\n",
        "\n",
        "    print(f\"Preprocessing complete. Final shape: {df_processed.shape}\")\n",
        "    print(f\"Using {len(final_feature_cols)} features for modeling.\")\n",
        "    if not df_processed.empty:\n",
        "        print(f\"Final data date range: {df_processed.index.min().strftime('%Y-%m-%d')} to {df_processed.index.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    # Return only the columns needed for modeling (features + target) plus the ticker\n",
        "    # Also return the identified list of feature columns\n",
        "    return df_processed[[ticker_col, target_col] + final_feature_cols], final_feature_cols\n",
        "\n",
        "\n",
        "# --- R-squared Calculation Functions ---\n",
        "# (Keep R2 functions as they are)\n",
        "def calculate_r2_oos(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates Out-of-Sample R-squared using the definition from the paper:\n",
        "    R2_OOS = 1 - sum( (y_true - y_pred)^2 ) / sum( y_true^2 )\n",
        "    This benchmarks against a naive forecast of zero.\n",
        "    \"\"\"\n",
        "    numerator = ((y_true - y_pred) ** 2).sum()\n",
        "    denominator = (y_true ** 2).sum()\n",
        "    if np.isclose(denominator, 0):\n",
        "        return 1.0 if np.isclose(numerator, 0) else -np.inf\n",
        "    return 1 - (numerator / denominator)\n",
        "\n",
        "def calculate_r2_is(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates In-Sample R-squared using the *same* formula structure as R2_OOS\n",
        "    for consistency in this specific replication context.\n",
        "    R2_IS = 1 - sum( (y_train_true - y_train_pred)^2 ) / sum( y_train_true^2 )\n",
        "    \"\"\"\n",
        "    return calculate_r2_oos(y_true, y_pred)\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(\"\\n--- Starting OLS-All+H Model Fitting (Huber Loss, All Features) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. Load or ensure 'combined_data' exists\n",
        "if 'combined_data' not in locals() and 'combined_data' not in globals():\n",
        "     print(\"ERROR: 'combined_data' DataFrame not found. Please run the data combination cell first.\")\n",
        "     data_processed = None\n",
        "     feature_cols = None\n",
        "else:\n",
        "    # 2. Preprocess the data AND get the list of features used\n",
        "    # Pass only target_col and ticker_col, let the function find features\n",
        "    data_processed, feature_cols = preprocess_combined_data(combined_data, date_col, ticker_col, target_col)\n",
        "\n",
        "# 3. Proceed with model fitting if preprocessing was successful\n",
        "if data_processed is not None and not data_processed.empty and feature_cols:\n",
        "    print(f\"\\nProceeding with {len(feature_cols)} features.\")\n",
        "    all_oos_predictions = []\n",
        "    all_oos_true_values = []\n",
        "    all_is_predictions = []\n",
        "    all_is_true_values = []\n",
        "\n",
        "    test_year_r2_oos = {}\n",
        "    train_year_r2_is = {}\n",
        "\n",
        "    available_years = sorted(data_processed.index.year.unique())\n",
        "    actual_test_years = [y for y in available_years if y >= test_start_year and y <= test_end_year]\n",
        "\n",
        "    if not actual_test_years:\n",
        "        print(f\"\\nERROR: No data available in the specified test period ({test_start_year}-{test_end_year}) after preprocessing. Cannot proceed.\")\n",
        "        huber_results_all = None\n",
        "    else:\n",
        "        print(f\"\\nStarting OLS-All+H annual refitting from {min(actual_test_years)} to {max(actual_test_years)}...\")\n",
        "        print(f\"Using Training Start Year: {train_start_year}\")\n",
        "\n",
        "        for current_test_year in actual_test_years:\n",
        "            loop_start_time = time.time()\n",
        "            print(f\"\\n  Processing test year: {current_test_year}\")\n",
        "\n",
        "            current_train_end_year = current_test_year - 1\n",
        "            if current_train_end_year < train_start_year:\n",
        "                 print(f\"    Skipping year {current_test_year}: Training end year ({current_train_end_year}) is before training start year ({train_start_year}).\")\n",
        "                 continue\n",
        "\n",
        "            print(f\"    Training period: {train_start_year}-{current_train_end_year}\")\n",
        "            train_mask = (data_processed.index.year >= train_start_year) & (data_processed.index.year <= current_train_end_year)\n",
        "            test_mask = (data_processed.index.year == current_test_year)\n",
        "            train_df = data_processed.loc[train_mask]\n",
        "            test_df = data_processed.loc[test_mask]\n",
        "\n",
        "            if train_df.empty or test_df.empty:\n",
        "                print(f\"    Skipping year {current_test_year}: Not enough data for train ({len(train_df)}) / test ({len(test_df)}) split.\")\n",
        "                continue\n",
        "\n",
        "            # Check for NaNs in the dynamically determined feature columns before scaling\n",
        "            if train_df[feature_cols].isna().any().any() or test_df[feature_cols].isna().any().any():\n",
        "                 print(f\"    ERROR: NaNs detected in feature columns for year {current_test_year} before scaling! Skipping.\")\n",
        "                 # You might want to investigate which columns have NaNs here\n",
        "                 # print(train_df[feature_cols].isna().sum().sort_values(ascending=False).head())\n",
        "                 continue\n",
        "\n",
        "            # Prepare data for scikit-learn using the dynamic feature_cols\n",
        "            X_train = train_df[feature_cols].values\n",
        "            y_train = train_df[target_col].values\n",
        "            X_test = test_df[feature_cols].values\n",
        "            y_test = test_df[target_col].values\n",
        "\n",
        "            # Scale features\n",
        "            scaler = StandardScaler()\n",
        "            try:\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                # Check for NaNs again AFTER scaling\n",
        "                if np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any():\n",
        "                    print(f\"    Warning: NaNs generated during scaling for year {current_test_year}. Check for constant columns. Imputing with 0.\")\n",
        "                    # Impute NaNs caused by scaling constant columns with 0\n",
        "                    X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                    X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                    # continue # Decide whether to skip or impute\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"    ERROR during scaling for year {current_test_year}: {e}. Skipping year.\")\n",
        "                continue\n",
        "\n",
        "            # --- Huber Regressor Model Training and Prediction ---\n",
        "            huber_model = HuberRegressor(fit_intercept=True, max_iter=300, tol=1e-4) # Keep max_iter reasonable\n",
        "\n",
        "            try:\n",
        "                # Final check for NaNs before fitting/predicting\n",
        "                if np.isnan(X_train_scaled).any() or np.isnan(y_train).any():\n",
        "                     print(f\"    ERROR: NaNs detected in training data before fitting model for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "                if np.isnan(X_test_scaled).any():\n",
        "                     print(f\"    ERROR: NaNs detected in test features before prediction for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                # Fit the Huber model\n",
        "                huber_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "                # --- In-Sample Prediction ---\n",
        "                is_predictions = huber_model.predict(X_train_scaled)\n",
        "                all_is_predictions.extend(is_predictions)\n",
        "                all_is_true_values.extend(y_train)\n",
        "                r2_is_this_year = calculate_r2_is(y_train, is_predictions)\n",
        "                train_year_r2_is[current_train_end_year] = r2_is_this_year\n",
        "\n",
        "                # --- Out-of-Sample Prediction ---\n",
        "                oos_predictions = huber_model.predict(X_test_scaled)\n",
        "                all_oos_predictions.extend(oos_predictions)\n",
        "                all_oos_true_values.extend(y_test)\n",
        "                r2_oos_this_year = calculate_r2_oos(y_test, oos_predictions)\n",
        "                test_year_r2_oos[current_test_year] = r2_oos_this_year\n",
        "\n",
        "                print(f\"    Train Year {current_train_end_year} R2_IS : {r2_is_this_year:+.4f}\")\n",
        "                print(f\"    Test Year  {current_test_year} R2_OOS: {r2_oos_this_year:+.4f}\")\n",
        "                print(f\"    Time for year: {time.time() - loop_start_time:.2f}s\")\n",
        "\n",
        "            except Exception as e:\n",
        "                # Catch potential memory errors with many features\n",
        "                print(f\"    ERROR during model fitting/prediction for year {current_test_year}: {e}\")\n",
        "                if \"MemoryError\" in str(e):\n",
        "                    print(\"      >>> Potential MemoryError likely due to large number of features. Consider reducing features or using a different model.\")\n",
        "                    # Optionally break the loop if memory errors persist\n",
        "                    # break\n",
        "\n",
        "\n",
        "        # --- Overall Results ---\n",
        "        print(\"\\n--- Overall OLS-All+H Results (Huber Loss, All Features) ---\")\n",
        "\n",
        "        # In-Sample Overall\n",
        "        overall_r2_is = None\n",
        "        if all_is_true_values:\n",
        "            overall_r2_is = calculate_r2_is(np.array(all_is_true_values), np.array(all_is_predictions))\n",
        "            print(f\"\\nOverall In-Sample R-squared (R2_IS) across all training periods: {overall_r2_is:.4f}\")\n",
        "            print(\"\\nR2_IS per training period (end year):\")\n",
        "            for year, r2 in sorted(train_year_r2_is.items()):\n",
        "                print(f\"  {train_start_year}-{year}: {r2:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo valid in-sample predictions were generated.\")\n",
        "\n",
        "        # Out-of-Sample Overall\n",
        "        overall_r2_oos = None\n",
        "        if all_oos_true_values:\n",
        "            overall_r2_oos = calculate_r2_oos(np.array(all_oos_true_values), np.array(all_oos_predictions))\n",
        "            print(f\"\\nOverall Out-of-Sample R-squared (R2_OOS) for period {min(actual_test_years)}-{max(actual_test_years)}: {overall_r2_oos:.4f}\")\n",
        "            print(f\"(Paper's OLS-3 benchmark R2_OOS: 0.16%)\") # Keep for reference\n",
        "            print(f\"(Paper's ENet+H (all features) R2_OOS: 0.11%)\") # Add ENet comparison [source: 91]\n",
        "            print(\"\\nR2_OOS per test year:\")\n",
        "            for year, r2 in sorted(test_year_r2_oos.items()):\n",
        "                print(f\"  {year}: {r2:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo valid out-of-sample predictions were generated for the test period.\")\n",
        "\n",
        "        # Store results\n",
        "        huber_results_all = {\n",
        "            \"overall_r2_is\": overall_r2_is,\n",
        "            \"overall_r2_oos\": overall_r2_oos,\n",
        "            \"yearly_r2_is\": train_year_r2_is,\n",
        "            \"yearly_r2_oos\": test_year_r2_oos,\n",
        "            \"features_used\": feature_cols # Store the list of features used\n",
        "        }\n",
        "\n",
        "else:\n",
        "    # This block executes if preprocessing failed or returned empty data/features\n",
        "    print(\"\\nData preprocessing failed or resulted in empty DataFrame/features. Cannot proceed with model fitting.\")\n",
        "    huber_results_all = None # Indicate failure\n",
        "\n",
        "# Final summary\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal execution time for OLS-All+H cell: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "if 'huber_results_all' in locals() and huber_results_all is not None:\n",
        "    print(\"\\nOLS-All+H model fitting complete. Results stored in 'huber_results_all' dictionary.\")\n",
        "    print(f\"Number of features used: {len(huber_results_all.get('features_used', []))}\")\n",
        "elif 'huber_results_all' not in locals() or huber_results_all is None:\n",
        "     print(\"\\nOLS-All+H model fitting did not complete successfully due to errors or lack of data/features.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6eOQJbdmL49",
        "outputId": "3a52b6e2-c970-459a-8c3f-329043e3d16b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting OLS-All+H Model Fitting (Huber Loss, All Features) ---\n",
            "--- Starting Data Preprocessing ---\n",
            "Input shape: (336740, 138)\n",
            "Converting 'date' to datetime...\n",
            "Shape after date processing: (336740, 137)\n",
            "Date index set.\n",
            "Identified 135 potential feature columns.\n",
            "Shape after selecting columns: (336740, 137)\n",
            "Attempting to convert identified feature columns to numeric...\n",
            "  Found 1 non-numeric value(s) in 'const'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'EPR'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'BPR'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'support_low'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'support_high'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'co_usd_krw_monthly'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'change_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'lo_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'ho_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'co_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'close_bond_1y'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'lo_sp500'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'ho_sp500'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'co_sp500'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'time_rnk'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_mom1'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_mom2'. Coercing to NaN.\n",
            "  Found 3 non-numeric value(s) in 'log_mom3'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_mom6'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_mom5'. Coercing to NaN.\n",
            "  Found 3 non-numeric value(s) in 'log_mom12'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'log_mom11'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'log_mom10'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_mom8'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'log_mom9'. Coercing to NaN.\n",
            "  Found 3 non-numeric value(s) in 'log_mom7'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_BPR'. Coercing to NaN.\n",
            "  Found 3 non-numeric value(s) in 'log_ff3_bin_return'. Coercing to NaN.\n",
            "  Found 7 non-numeric value(s) in 'log_smb'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_co_sp500'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_change_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'log_ho_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_ho_sp500'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'log_co_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'log_close_bond_1y'. Coercing to NaN.\n",
            "  Found 6 non-numeric value(s) in 'log_lo_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_lo_sp500'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_hml'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_co_usd_krw_monthly'. Coercing to NaN.\n",
            "  Found 7 non-numeric value(s) in 'div_ret'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'tb3y'. Coercing to NaN.\n",
            "  Found 7 non-numeric value(s) in 'div'. Coercing to NaN.\n",
            "Coerced a total of 123 non-numeric entries to NaN across features.\n",
            "Handled infinite values.\n",
            "Found 2196 missing values in 'target'. Dropping rows...\n",
            "Dropped 2196 rows due to missing 'target'.\n",
            "Shape after handling target NaNs: (334544, 137)\n",
            "Applying Median+0 imputation to features...\n",
            "Imputation complete. Imputed ~299696 values with median, 711300 values with 0.\n",
            "Preprocessing complete. Final shape: (334544, 137)\n",
            "Using 135 features for modeling.\n",
            "Final data date range: 2002-05-01 to 2021-08-01\n",
            "\n",
            "Proceeding with 135 features.\n",
            "\n",
            "Starting OLS-All+H annual refitting from 2019 to 2020...\n",
            "Using Training Start Year: 2002\n",
            "\n",
            "  Processing test year: 2019\n",
            "    Training period: 2002-2018\n",
            "    Train Year 2018 R2_IS : +0.0025\n",
            "    Test Year  2019 R2_OOS: -0.0046\n",
            "    Time for year: 138.30s\n",
            "\n",
            "  Processing test year: 2020\n",
            "    Training period: 2002-2019\n",
            "    Train Year 2019 R2_IS : +0.0024\n",
            "    Test Year  2020 R2_OOS: -0.0542\n",
            "    Time for year: 158.59s\n",
            "\n",
            "--- Overall OLS-All+H Results (Huber Loss, All Features) ---\n",
            "\n",
            "Overall In-Sample R-squared (R2_IS) across all training periods: 0.0025\n",
            "\n",
            "R2_IS per training period (end year):\n",
            "  2002-2018: 0.0025\n",
            "  2002-2019: 0.0024\n",
            "\n",
            "Overall Out-of-Sample R-squared (R2_OOS) for period 2019-2020: -0.0404\n",
            "(Paper's OLS-3 benchmark R2_OOS: 0.16%)\n",
            "(Paper's ENet+H (all features) R2_OOS: 0.11%)\n",
            "\n",
            "R2_OOS per test year:\n",
            "  2019: -0.0046\n",
            "  2020: -0.0542\n",
            "\n",
            "Total execution time for OLS-All+H cell: 318.14 seconds\n",
            "\n",
            "OLS-All+H model fitting complete. Results stored in 'huber_results_all' dictionary.\n",
            "Number of features used: 135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Overall PCR Results (All Features, 95% Var)"
      ],
      "metadata": {
        "id": "uqxpvYaxf966"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.linear_model import LinearRegression # PCR uses OLS in the second step\n",
        "from sklearn.decomposition import PCA # For Principal Component Analysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "data_folder_path = './'\n",
        "num_files = 10\n",
        "file_prefix = 'factor_'\n",
        "file_suffix = '.csv'\n",
        "\n",
        "# feature_cols will be determined dynamically by the preprocessing function\n",
        "target_col = 'target'\n",
        "date_col = 'date'\n",
        "ticker_col = 'ticker'\n",
        "\n",
        "train_start_year = 2002\n",
        "test_start_year = 2019\n",
        "test_end_year = 2020\n",
        "pca_explained_variance_threshold = 0.95 # Use components explaining 95% of variance\n",
        "\n",
        "# --- Functions ---\n",
        "\n",
        "# Use the same preprocessing function from the previous step (OLS-All+H)\n",
        "# It identifies features, handles numeric conversion, imputation, etc.\n",
        "# and returns the processed data and the list of feature columns.\n",
        "def preprocess_combined_data(df, date_col, ticker_col, target_col):\n",
        "    \"\"\"\n",
        "    Preprocesses the combined dataframe:\n",
        "    - Sets date index.\n",
        "    - Converts potential feature columns to numeric (coercing errors to NaN).\n",
        "    - Handles infinite values.\n",
        "    - Drops rows with missing target values.\n",
        "    - Imputes missing feature values using monthly median, then 0.\n",
        "    - Returns the processed DataFrame and the list of identified feature columns.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Data Preprocessing ---\")\n",
        "    if df is None:\n",
        "        print(\"ERROR: Input DataFrame is None.\")\n",
        "        return None, None\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        print(\"ERROR: Input is not a pandas DataFrame.\")\n",
        "        return None, None\n",
        "\n",
        "    df_processed = df.copy()\n",
        "    print(f\"Input shape: {df_processed.shape}\")\n",
        "\n",
        "    # 1. Convert date column and set index\n",
        "    if date_col in df_processed.columns:\n",
        "        print(f\"Converting '{date_col}' to datetime...\")\n",
        "        df_processed[date_col] = pd.to_datetime(df_processed[date_col], errors='coerce')\n",
        "        df_processed = df_processed.dropna(subset=[date_col]) # Drop rows where date conversion failed\n",
        "        df_processed = df_processed.set_index(date_col)\n",
        "        print(f\"Shape after date processing: {df_processed.shape}\")\n",
        "    elif not isinstance(df_processed.index, pd.DatetimeIndex):\n",
        "        print(f\"ERROR: DataFrame index is not a DatetimeIndex and '{date_col}' column not found.\")\n",
        "        return None, None\n",
        "    print(\"Date index set.\")\n",
        "\n",
        "    # 2. Identify potential feature columns (all columns except target and ticker)\n",
        "    potential_feature_cols = [col for col in df_processed.columns if col not in [target_col, ticker_col]]\n",
        "    print(f\"Identified {len(potential_feature_cols)} potential feature columns.\")\n",
        "    if not potential_feature_cols:\n",
        "        print(\"ERROR: No potential feature columns found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Keep only necessary columns for now (target, ticker, potential features)\n",
        "    cols_to_keep = [ticker_col, target_col] + potential_feature_cols\n",
        "    missing_cols = [col for col in cols_to_keep if col not in df_processed.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: Missing required columns after date processing: {missing_cols}\")\n",
        "        return None, None\n",
        "    df_processed = df_processed[cols_to_keep]\n",
        "    print(f\"Shape after selecting columns: {df_processed.shape}\")\n",
        "\n",
        "\n",
        "    # 3. Force Feature Columns to Numeric (Convert non-numeric to NaN)\n",
        "    print(\"Attempting to convert identified feature columns to numeric...\")\n",
        "    coerced_count = 0\n",
        "    numeric_cols_found = [] # Keep track of columns confirmed numeric\n",
        "    for col in potential_feature_cols:\n",
        "         if col in df_processed.columns:\n",
        "              if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                  initial_non_numeric = pd.to_numeric(df_processed[col], errors='coerce').isna().sum() - df_processed[col].isna().sum()\n",
        "                  if initial_non_numeric > 0:\n",
        "                       print(f\"  Found {initial_non_numeric} non-numeric value(s) in '{col}'. Coercing to NaN.\")\n",
        "                       coerced_count += initial_non_numeric\n",
        "                  df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "              # Check again if it's numeric now (or was already)\n",
        "              if pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                  numeric_cols_found.append(col)\n",
        "              else:\n",
        "                   print(f\"  WARNING: Column '{col}' could not be converted to numeric. Excluding.\")\n",
        "         else:\n",
        "              print(f\"Warning: Potential feature column '{col}' not found during numeric conversion phase.\")\n",
        "\n",
        "    potential_feature_cols = numeric_cols_found # Update the list to only include numeric ones\n",
        "    if not potential_feature_cols:\n",
        "        print(\"ERROR: No numeric feature columns identified after conversion attempt.\")\n",
        "        return None, None\n",
        "    print(f\"Confirmed {len(potential_feature_cols)} numeric feature columns.\")\n",
        "    if coerced_count > 0:\n",
        "         print(f\"Coerced a total of {coerced_count} non-numeric entries to NaN across features.\")\n",
        "\n",
        "    # 4. Handle infinite values\n",
        "    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    print(\"Handled infinite values.\")\n",
        "\n",
        "    # 5. Handle TARGET Variable Missing Values\n",
        "    initial_rows = len(df_processed)\n",
        "    n_target_na_initial = df_processed[target_col].isna().sum()\n",
        "    if n_target_na_initial > 0:\n",
        "         print(f\"Found {n_target_na_initial} missing values in '{target_col}'. Dropping rows...\")\n",
        "         df_processed.dropna(subset=[target_col], inplace=True)\n",
        "         rows_after_target_drop = len(df_processed)\n",
        "         rows_dropped = initial_rows - rows_after_target_drop\n",
        "         if rows_dropped > 0:\n",
        "              print(f\"Dropped {rows_dropped} rows due to missing '{target_col}'.\")\n",
        "    else:\n",
        "        print(f\"No missing values found in '{target_col}'.\")\n",
        "\n",
        "    print(f\"Shape after handling target NaNs: {df_processed.shape}\")\n",
        "    if df_processed.empty:\n",
        "         print(\"ERROR: All rows dropped due to missing target or date issues. Cannot proceed.\")\n",
        "         return None, None\n",
        "\n",
        "    # --- Handle FEATURE Missing Values (Median then 0 Imputation) ---\n",
        "    print(\"Applying Median+0 imputation to numeric features...\")\n",
        "    try:\n",
        "        df_processed['month_period'] = df_processed.index.to_period('M')\n",
        "    except AttributeError as e:\n",
        "        print(f\"Error creating 'month_period': {e}. Ensure index is DatetimeIndex.\")\n",
        "        return None, None\n",
        "\n",
        "    imputed_count_median = 0\n",
        "    imputed_count_zero = 0\n",
        "    final_feature_cols = []\n",
        "\n",
        "    for col in potential_feature_cols: # Iterate through confirmed numeric columns\n",
        "        if col in df_processed.columns:\n",
        "            n_initial_na = df_processed[col].isna().sum()\n",
        "            if n_initial_na > 0:\n",
        "                try:\n",
        "                    medians = df_processed.groupby('month_period')[col].median()\n",
        "                    df_processed[col] = df_processed.groupby('month_period')[col].transform(lambda x: x.fillna(medians.loc[x.name]))\n",
        "                    imputed_this_median = n_initial_na - df_processed[col].isna().sum()\n",
        "                    imputed_count_median += imputed_this_median\n",
        "                except Exception as e:\n",
        "                    print(f\"    WARNING: Could not impute median for '{col}'. Error: {e}.\")\n",
        "\n",
        "                n_remaining_na = df_processed[col].isna().sum()\n",
        "                if n_remaining_na > 0:\n",
        "                    df_processed[col].fillna(0, inplace=True)\n",
        "                    imputed_count_zero += n_remaining_na\n",
        "            # Add to final list if it's still present and numeric (should be)\n",
        "            if col in df_processed.columns and pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                final_feature_cols.append(col)\n",
        "            else:\n",
        "                 print(f\"  WARNING: Column '{col}' is missing or non-numeric after imputation. Excluding.\")\n",
        "        else:\n",
        "             print(f\"Warning: Numeric feature column '{col}' not found during imputation phase.\")\n",
        "\n",
        "    df_processed.drop(columns=['month_period'], inplace=True)\n",
        "    print(f\"Imputation complete. Imputed ~{imputed_count_median} values with median, {imputed_count_zero} values with 0.\")\n",
        "\n",
        "    if not final_feature_cols:\n",
        "        print(\"ERROR: No numeric feature columns remain after imputation.\")\n",
        "        return None, None\n",
        "\n",
        "    nans_in_features = df_processed[final_feature_cols].isna().sum().sum()\n",
        "    if nans_in_features > 0:\n",
        "         print(f\"WARNING: {nans_in_features} NaNs still remain in final feature columns after imputation!\")\n",
        "\n",
        "    print(f\"Preprocessing complete. Final shape: {df_processed.shape}\")\n",
        "    print(f\"Using {len(final_feature_cols)} features for modeling.\")\n",
        "    if not df_processed.empty:\n",
        "        print(f\"Final data date range: {df_processed.index.min().strftime('%Y-%m-%d')} to {df_processed.index.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    # Return only the columns needed for modeling\n",
        "    return df_processed[[ticker_col, target_col] + final_feature_cols], final_feature_cols\n",
        "\n",
        "\n",
        "# --- R-squared Calculation Functions ---\n",
        "def calculate_r2_oos(y_true, y_pred):\n",
        "    \"\"\"Calculates Out-of-Sample R-squared (vs. zero forecast).\"\"\"\n",
        "    numerator = ((y_true - y_pred) ** 2).sum()\n",
        "    denominator = (y_true ** 2).sum()\n",
        "    if np.isclose(denominator, 0):\n",
        "        return 1.0 if np.isclose(numerator, 0) else -np.inf\n",
        "    return 1 - (numerator / denominator)\n",
        "\n",
        "def calculate_r2_is(y_true, y_pred):\n",
        "    \"\"\"Calculates In-Sample R-squared (vs. zero forecast).\"\"\"\n",
        "    return calculate_r2_oos(y_true, y_pred)\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(f\"\\n--- Starting PCR Model Fitting (All Features, {pca_explained_variance_threshold*100:.0f}% Variance) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. Load or ensure 'combined_data' exists\n",
        "if 'combined_data' not in locals() and 'combined_data' not in globals():\n",
        "     print(\"ERROR: 'combined_data' DataFrame not found. Please run the data combination cell first.\")\n",
        "     data_processed = None\n",
        "     feature_cols = None\n",
        "else:\n",
        "    # 2. Preprocess the data AND get the list of features used\n",
        "    data_processed, feature_cols = preprocess_combined_data(combined_data, date_col, ticker_col, target_col)\n",
        "\n",
        "# 3. Proceed with model fitting if preprocessing was successful\n",
        "if data_processed is not None and not data_processed.empty and feature_cols:\n",
        "    print(f\"\\nProceeding with {len(feature_cols)} features.\")\n",
        "    all_oos_predictions = []\n",
        "    all_oos_true_values = []\n",
        "    all_is_predictions = []\n",
        "    all_is_true_values = []\n",
        "\n",
        "    test_year_r2_oos = {}\n",
        "    train_year_r2_is = {}\n",
        "    components_used = {} # To store number of components used each year\n",
        "\n",
        "    available_years = sorted(data_processed.index.year.unique())\n",
        "    actual_test_years = [y for y in available_years if y >= test_start_year and y <= test_end_year]\n",
        "\n",
        "    if not actual_test_years:\n",
        "        print(f\"\\nERROR: No data available in the specified test period ({test_start_year}-{test_end_year}) after preprocessing.\")\n",
        "        pcr_results_all = None\n",
        "    else:\n",
        "        print(f\"\\nStarting PCR annual refitting from {min(actual_test_years)} to {max(actual_test_years)}...\")\n",
        "        print(f\"Using Training Start Year: {train_start_year}\")\n",
        "\n",
        "        for current_test_year in actual_test_years:\n",
        "            loop_start_time = time.time()\n",
        "            print(f\"\\n  Processing test year: {current_test_year}\")\n",
        "\n",
        "            current_train_end_year = current_test_year - 1\n",
        "            if current_train_end_year < train_start_year:\n",
        "                 print(f\"    Skipping year {current_test_year}: Training end year ({current_train_end_year}) is before training start year ({train_start_year}).\")\n",
        "                 continue\n",
        "\n",
        "            print(f\"    Training period: {train_start_year}-{current_train_end_year}\")\n",
        "            train_mask = (data_processed.index.year >= train_start_year) & (data_processed.index.year <= current_train_end_year)\n",
        "            test_mask = (data_processed.index.year == current_test_year)\n",
        "            train_df = data_processed.loc[train_mask]\n",
        "            test_df = data_processed.loc[test_mask]\n",
        "\n",
        "            if train_df.empty or test_df.empty:\n",
        "                print(f\"    Skipping year {current_test_year}: Not enough data for train ({len(train_df)}) / test ({len(test_df)}) split.\")\n",
        "                continue\n",
        "\n",
        "            # Check for NaNs before scaling\n",
        "            if train_df[feature_cols].isna().any().any() or test_df[feature_cols].isna().any().any():\n",
        "                 print(f\"    ERROR: NaNs detected in feature columns for year {current_test_year} before scaling! Skipping.\")\n",
        "                 continue\n",
        "\n",
        "            # Prepare data\n",
        "            X_train = train_df[feature_cols].values\n",
        "            y_train = train_df[target_col].values\n",
        "            X_test = test_df[feature_cols].values\n",
        "            y_test = test_df[target_col].values\n",
        "\n",
        "            # --- Scaling ---\n",
        "            scaler = StandardScaler()\n",
        "            try:\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                # Check for NaNs again AFTER scaling and impute if necessary\n",
        "                if np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any():\n",
        "                    print(f\"    Warning: NaNs generated during scaling for year {current_test_year}. Imputing with 0.\")\n",
        "                    X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                    X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"    ERROR during scaling for year {current_test_year}: {e}. Skipping year.\")\n",
        "                continue\n",
        "\n",
        "            # --- PCA Transformation ---\n",
        "            try:\n",
        "                # Fit PCA on the scaled training data to find components\n",
        "                pca = PCA(n_components=pca_explained_variance_threshold)\n",
        "                pca.fit(X_train_scaled)\n",
        "                n_comps = pca.n_components_\n",
        "                components_used[current_train_end_year] = n_comps\n",
        "                print(f\"    PCA: Selected {n_comps} components to explain {pca_explained_variance_threshold*100:.0f}% variance.\")\n",
        "\n",
        "                # Transform both training and test sets using the fitted PCA\n",
        "                X_train_pca = pca.transform(X_train_scaled)\n",
        "                X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    ERROR during PCA for year {current_test_year}: {e}. Skipping year.\")\n",
        "                continue\n",
        "\n",
        "            # --- OLS Regression on Principal Components ---\n",
        "            ols_model = LinearRegression(fit_intercept=True)\n",
        "            try:\n",
        "                # Check for NaNs before fitting (should be handled by imputation/PCA)\n",
        "                if np.isnan(X_train_pca).any() or np.isnan(y_train).any():\n",
        "                     print(f\"    ERROR: NaNs detected in PCA-transformed training data before fitting model for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "                if np.isnan(X_test_pca).any():\n",
        "                     print(f\"    ERROR: NaNs detected in PCA-transformed test features before prediction for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                # Fit OLS on the transformed training data\n",
        "                ols_model.fit(X_train_pca, y_train)\n",
        "\n",
        "                # --- In-Sample Prediction (on transformed data) ---\n",
        "                is_predictions = ols_model.predict(X_train_pca)\n",
        "                all_is_predictions.extend(is_predictions)\n",
        "                all_is_true_values.extend(y_train) # Use original y_train\n",
        "                r2_is_this_year = calculate_r2_is(y_train, is_predictions)\n",
        "                train_year_r2_is[current_train_end_year] = r2_is_this_year\n",
        "\n",
        "                # --- Out-of-Sample Prediction (on transformed data) ---\n",
        "                oos_predictions = ols_model.predict(X_test_pca)\n",
        "                all_oos_predictions.extend(oos_predictions)\n",
        "                all_oos_true_values.extend(y_test) # Use original y_test\n",
        "                r2_oos_this_year = calculate_r2_oos(y_test, oos_predictions)\n",
        "                test_year_r2_oos[current_test_year] = r2_oos_this_year\n",
        "\n",
        "                print(f\"    Train Year {current_train_end_year} R2_IS : {r2_is_this_year:+.4f}\")\n",
        "                print(f\"    Test Year  {current_test_year} R2_OOS: {r2_oos_this_year:+.4f}\")\n",
        "                print(f\"    Time for year: {time.time() - loop_start_time:.2f}s\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    ERROR during OLS fitting/prediction on PCA components for year {current_test_year}: {e}\")\n",
        "\n",
        "        # --- Overall Results ---\n",
        "        print(f\"\\n--- Overall PCR Results (All Features, {pca_explained_variance_threshold*100:.0f}% Var) ---\")\n",
        "\n",
        "        # In-Sample Overall\n",
        "        overall_r2_is = None\n",
        "        if all_is_true_values:\n",
        "            overall_r2_is = calculate_r2_is(np.array(all_is_true_values), np.array(all_is_predictions))\n",
        "            print(f\"\\nOverall In-Sample R-squared (R2_IS) across all training periods: {overall_r2_is:.4f}\")\n",
        "            print(\"\\nR2_IS per training period (end year):\")\n",
        "            for year, r2 in sorted(train_year_r2_is.items()):\n",
        "                print(f\"  {train_start_year}-{year}: {r2:.4f} (Components: {components_used.get(year, 'N/A')})\")\n",
        "        else:\n",
        "            print(\"\\nNo valid in-sample predictions were generated.\")\n",
        "\n",
        "        # Out-of-Sample Overall\n",
        "        overall_r2_oos = None\n",
        "        if all_oos_true_values:\n",
        "            overall_r2_oos = calculate_r2_oos(np.array(all_oos_true_values), np.array(all_oos_predictions))\n",
        "            print(f\"\\nOverall Out-of-Sample R-squared (R2_OOS) for period {min(actual_test_years)}-{max(actual_test_years)}: {overall_r2_oos:.4f}\")\n",
        "            print(f\"(Paper's PCR (all features) R2_OOS: 0.26%)\") # Add PCR comparison [source: 91]\n",
        "            print(\"\\nR2_OOS per test year:\")\n",
        "            for year, r2 in sorted(test_year_r2_oos.items()):\n",
        "                 # Find the corresponding training end year to get components used\n",
        "                 train_end_y = year - 1\n",
        "                 comps = components_used.get(train_end_y, 'N/A')\n",
        "                 print(f\"  {year}: {r2:.4f} (Used {comps} components from {train_start_year}-{train_end_y} training)\")\n",
        "        else:\n",
        "            print(\"\\nNo valid out-of-sample predictions were generated for the test period.\")\n",
        "\n",
        "        # Store results\n",
        "        pcr_results_all = {\n",
        "            \"overall_r2_is\": overall_r2_is,\n",
        "            \"overall_r2_oos\": overall_r2_oos,\n",
        "            \"yearly_r2_is\": train_year_r2_is,\n",
        "            \"yearly_r2_oos\": test_year_r2_oos,\n",
        "            \"components_used_per_training_year\": components_used,\n",
        "            \"features_used_before_pca\": feature_cols\n",
        "        }\n",
        "\n",
        "else:\n",
        "    # This block executes if preprocessing failed\n",
        "    print(\"\\nData preprocessing failed or resulted in empty DataFrame/features. Cannot proceed with PCR model fitting.\")\n",
        "    pcr_results_all = None # Indicate failure\n",
        "\n",
        "# Final summary\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal execution time for PCR (All Features) cell: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "if 'pcr_results_all' in locals() and pcr_results_all is not None:\n",
        "    print(\"\\nPCR (All Features) model fitting complete. Results stored in 'pcr_results_all' dictionary.\")\n",
        "    avg_comps = np.mean(list(pcr_results_all['components_used_per_training_year'].values())) if pcr_results_all['components_used_per_training_year'] else 'N/A'\n",
        "    print(f\"Number of features before PCA: {len(pcr_results_all.get('features_used_before_pca', []))}\")\n",
        "    print(f\"Average number of PCA components used (explaining {pca_explained_variance_threshold*100:.0f}% variance): {avg_comps:.1f}\")\n",
        "elif 'pcr_results_all' not in locals() or pcr_results_all is None:\n",
        "     print(\"\\nPCR (All Features) model fitting did not complete successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1A57DCPoYwD",
        "outputId": "b73135ea-52dd-47e0-deb0-8d6e7f33533c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting PCR Model Fitting (All Features, 95% Variance) ---\n",
            "--- Starting Data Preprocessing ---\n",
            "Input shape: (336740, 138)\n",
            "Converting 'date' to datetime...\n",
            "Shape after date processing: (336740, 137)\n",
            "Date index set.\n",
            "Identified 135 potential feature columns.\n",
            "Shape after selecting columns: (336740, 137)\n",
            "Attempting to convert identified feature columns to numeric...\n",
            "  Found 1 non-numeric value(s) in 'const'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'EPR'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'BPR'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'support_low'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'support_high'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'co_usd_krw_monthly'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'change_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'lo_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'ho_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'co_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'close_bond_1y'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'lo_sp500'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'ho_sp500'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'co_sp500'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'time_rnk'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_mom1'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_mom2'. Coercing to NaN.\n",
            "  Found 3 non-numeric value(s) in 'log_mom3'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_mom6'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_mom5'. Coercing to NaN.\n",
            "  Found 3 non-numeric value(s) in 'log_mom12'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'log_mom11'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'log_mom10'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_mom8'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'log_mom9'. Coercing to NaN.\n",
            "  Found 3 non-numeric value(s) in 'log_mom7'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_BPR'. Coercing to NaN.\n",
            "  Found 3 non-numeric value(s) in 'log_ff3_bin_return'. Coercing to NaN.\n",
            "  Found 7 non-numeric value(s) in 'log_smb'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_co_sp500'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_change_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'log_ho_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_ho_sp500'. Coercing to NaN.\n",
            "  Found 5 non-numeric value(s) in 'log_co_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 2 non-numeric value(s) in 'log_close_bond_1y'. Coercing to NaN.\n",
            "  Found 6 non-numeric value(s) in 'log_lo_usd_krw_daily'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_lo_sp500'. Coercing to NaN.\n",
            "  Found 4 non-numeric value(s) in 'log_hml'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'log_co_usd_krw_monthly'. Coercing to NaN.\n",
            "  Found 7 non-numeric value(s) in 'div_ret'. Coercing to NaN.\n",
            "  Found 1 non-numeric value(s) in 'tb3y'. Coercing to NaN.\n",
            "  Found 7 non-numeric value(s) in 'div'. Coercing to NaN.\n",
            "Confirmed 135 numeric feature columns.\n",
            "Coerced a total of 123 non-numeric entries to NaN across features.\n",
            "Handled infinite values.\n",
            "Found 2196 missing values in 'target'. Dropping rows...\n",
            "Dropped 2196 rows due to missing 'target'.\n",
            "Shape after handling target NaNs: (334544, 137)\n",
            "Applying Median+0 imputation to numeric features...\n",
            "Imputation complete. Imputed ~299696 values with median, 711300 values with 0.\n",
            "Preprocessing complete. Final shape: (334544, 137)\n",
            "Using 135 features for modeling.\n",
            "Final data date range: 2002-05-01 to 2021-08-01\n",
            "\n",
            "Proceeding with 135 features.\n",
            "\n",
            "Starting PCR annual refitting from 2019 to 2020...\n",
            "Using Training Start Year: 2002\n",
            "\n",
            "  Processing test year: 2019\n",
            "    Training period: 2002-2018\n",
            "    PCA: Selected 70 components to explain 95% variance.\n",
            "    Train Year 2018 R2_IS : +0.0034\n",
            "    Test Year  2019 R2_OOS: -0.0132\n",
            "    Time for year: 5.50s\n",
            "\n",
            "  Processing test year: 2020\n",
            "    Training period: 2002-2019\n",
            "    PCA: Selected 72 components to explain 95% variance.\n",
            "    Train Year 2019 R2_IS : +0.0034\n",
            "    Test Year  2020 R2_OOS: +0.0194\n",
            "    Time for year: 4.57s\n",
            "\n",
            "--- Overall PCR Results (All Features, 95% Var) ---\n",
            "\n",
            "Overall In-Sample R-squared (R2_IS) across all training periods: 0.0034\n",
            "\n",
            "R2_IS per training period (end year):\n",
            "  2002-2018: 0.0034 (Components: 70)\n",
            "  2002-2019: 0.0034 (Components: 72)\n",
            "\n",
            "Overall Out-of-Sample R-squared (R2_OOS) for period 2019-2020: 0.0103\n",
            "(Paper's PCR (all features) R2_OOS: 0.26%)\n",
            "\n",
            "R2_OOS per test year:\n",
            "  2019: -0.0132 (Used 70 components from 2002-2018 training)\n",
            "  2020: 0.0194 (Used 72 components from 2002-2019 training)\n",
            "\n",
            "Total execution time for PCR (All Features) cell: 33.89 seconds\n",
            "\n",
            "PCR (All Features) model fitting complete. Results stored in 'pcr_results_all' dictionary.\n",
            "Number of features before PCA: 135\n",
            "Average number of PCA components used (explaining 95% variance): 71.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Overall OLS-3 Results (MSE Loss)"
      ],
      "metadata": {
        "id": "JEgWt6nPgD4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE Loss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.linear_model import LinearRegression # Changed from HuberRegressor\n",
        "from sklearn.metrics import mean_squared_error # Can still be useful for other checks\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# (Keep the same configuration as Cell 2)\n",
        "data_folder_path = './'\n",
        "num_files = 10\n",
        "file_prefix = 'factor_'\n",
        "file_suffix = '.csv'\n",
        "\n",
        "feature_cols = [\n",
        "    'size_rnk',\n",
        "    'BPR',\n",
        "    'mom12'\n",
        "]\n",
        "target_col = 'target'\n",
        "date_col = 'date'\n",
        "ticker_col = 'ticker'\n",
        "\n",
        "train_start_year = 2002\n",
        "# train_end_year = 2015 # This is determined dynamically in the loop\n",
        "test_start_year = 2019\n",
        "test_end_year = 2020\n",
        "\n",
        "\n",
        "# --- Functions ---\n",
        "# Re-use the preprocessing function from Cell 2 if needed, or assume 'data' is preprocessed\n",
        "# If 'data' isn't available from Cell 2, you might need to rerun preprocessing or load it\n",
        "# For simplicity, assume 'data' is available from Cell 2's execution\n",
        "\n",
        "# --- R-squared Calculation Functions ---\n",
        "def calculate_r2_oos(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates Out-of-Sample R-squared using the definition from the paper:\n",
        "    R2_OOS = 1 - sum( (y_true - y_pred)^2 ) / sum( y_true^2 )\n",
        "    This benchmarks against a naive forecast of zero.\n",
        "    \"\"\"\n",
        "    numerator = ((y_true - y_pred) ** 2).sum()\n",
        "    denominator = (y_true ** 2).sum()\n",
        "    # Handle cases where the sum of squared true values is zero or very close to zero\n",
        "    if np.isclose(denominator, 0):\n",
        "        # If numerator is also zero, perfect fit (or all zeros); otherwise, undefined or -inf\n",
        "        return 1.0 if np.isclose(numerator, 0) else -np.inf\n",
        "    return 1 - (numerator / denominator)\n",
        "\n",
        "def calculate_r2_is(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculates In-Sample R-squared using the *same* formula structure as R2_OOS\n",
        "    for consistency in this specific replication context.\n",
        "    R2_IS = 1 - sum( (y_train_true - y_train_pred)^2 ) / sum( y_train_true^2 )\n",
        "    \"\"\"\n",
        "    # This uses the same logic as calculate_r2_oos but is applied to training data\n",
        "    return calculate_r2_oos(y_true, y_pred)\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(\"\\n--- Starting OLS-3 Model Fitting (MSE Loss) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Ensure the preprocessed data DataFrame 'data' exists from the previous cell\n",
        "if 'data' not in locals() and 'data' not in globals():\n",
        "     print(\"ERROR: Preprocessed DataFrame 'data' not found.\")\n",
        "     print(\"Please ensure the preprocessing step in the previous cell ran successfully.\")\n",
        "     ols_results = None # Indicate failure\n",
        "elif data is None or data.empty:\n",
        "    print(\"ERROR: Preprocessed DataFrame 'data' is None or empty.\")\n",
        "    ols_results = None # Indicate failure\n",
        "else:\n",
        "    # --- Proceed with model fitting ---\n",
        "    all_oos_predictions = []\n",
        "    all_oos_true_values = []\n",
        "    all_is_predictions = []  # To store in-sample predictions for overall IS R2\n",
        "    all_is_true_values = []   # To store in-sample true values for overall IS R2\n",
        "\n",
        "    test_year_r2_oos = {}     # R2_OOS per test year\n",
        "    train_year_r2_is = {}    # R2_IS per train period ending year\n",
        "\n",
        "    available_years = sorted(data.index.year.unique())\n",
        "    actual_test_years = [y for y in available_years if y >= test_start_year and y <= test_end_year]\n",
        "\n",
        "    if not actual_test_years:\n",
        "        print(f\"\\nERROR: No data available in the specified test period ({test_start_year}-{test_end_year}) after preprocessing. Cannot proceed.\")\n",
        "        ols_results = None\n",
        "    else:\n",
        "        print(f\"\\nStarting OLS-3 annual refitting from {min(actual_test_years)} to {max(actual_test_years)}...\")\n",
        "        print(f\"Using Training Start Year: {train_start_year}\")\n",
        "\n",
        "        for current_test_year in actual_test_years:\n",
        "            loop_start_time = time.time()\n",
        "            print(f\"\\n  Processing test year: {current_test_year}\")\n",
        "\n",
        "            current_train_end_year = current_test_year - 1\n",
        "            if current_train_end_year < train_start_year:\n",
        "                 print(f\"    Skipping year {current_test_year}: Training end year ({current_train_end_year}) is before training start year ({train_start_year}).\")\n",
        "                 continue\n",
        "\n",
        "            print(f\"    Training period: {train_start_year}-{current_train_end_year}\")\n",
        "            train_mask = (data.index.year >= train_start_year) & (data.index.year <= current_train_end_year)\n",
        "            test_mask = (data.index.year == current_test_year)\n",
        "            train_df = data.loc[train_mask]\n",
        "            test_df = data.loc[test_mask]\n",
        "\n",
        "            if train_df.empty or test_df.empty:\n",
        "                print(f\"    Skipping year {current_test_year}: Not enough data for train ({len(train_df)}) / test ({len(test_df)}) split.\")\n",
        "                continue\n",
        "\n",
        "            # Final check for NaNs before scaling (should be handled by preprocessing)\n",
        "            if train_df[feature_cols].isna().any().any() or test_df[feature_cols].isna().any().any():\n",
        "                 print(f\"    ERROR: NaNs detected in features for year {current_test_year} before scaling, after preprocessing! Skipping.\")\n",
        "                 continue\n",
        "\n",
        "            # Prepare data for scikit-learn\n",
        "            X_train = train_df[feature_cols].values\n",
        "            y_train = train_df[target_col].values\n",
        "            X_test = test_df[feature_cols].values\n",
        "            y_test = test_df[target_col].values\n",
        "\n",
        "            # Scale features\n",
        "            scaler = StandardScaler()\n",
        "            try:\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                # Check for NaNs again AFTER scaling (can happen with constant cols)\n",
        "                if np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any():\n",
        "                    print(f\"    Warning: NaNs generated during scaling for year {current_test_year}. Check for constant columns in training data. Skipping year.\")\n",
        "                    # Impute NaNs caused by scaling constant columns if necessary, or skip\n",
        "                    # X_train_scaled = np.nan_to_num(X_train_scaled) # Example imputation\n",
        "                    # X_test_scaled = np.nan_to_num(X_test_scaled)\n",
        "                    continue # Skip this year if NaNs appear after scaling\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"    ERROR during scaling for year {current_test_year}: {e}. Skipping year.\")\n",
        "                continue\n",
        "\n",
        "            # --- OLS Model Training and Prediction ---\n",
        "            ols_model = LinearRegression(fit_intercept=True) # Use standard OLS\n",
        "\n",
        "            try:\n",
        "                # Check for NaNs before fitting/predicting (belt-and-suspenders check)\n",
        "                if np.isnan(X_train_scaled).any() or np.isnan(y_train).any():\n",
        "                     print(f\"    ERROR: NaNs detected in training data before fitting model for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "                if np.isnan(X_test_scaled).any():\n",
        "                     print(f\"    ERROR: NaNs detected in test features before prediction for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                # Fit the OLS model\n",
        "                ols_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "                # --- In-Sample Prediction ---\n",
        "                is_predictions = ols_model.predict(X_train_scaled)\n",
        "                all_is_predictions.extend(is_predictions)\n",
        "                all_is_true_values.extend(y_train)\n",
        "                r2_is_this_year = calculate_r2_is(y_train, is_predictions)\n",
        "                train_year_r2_is[current_train_end_year] = r2_is_this_year # Store IS R2 by training end year\n",
        "\n",
        "                # --- Out-of-Sample Prediction ---\n",
        "                oos_predictions = ols_model.predict(X_test_scaled)\n",
        "                all_oos_predictions.extend(oos_predictions)\n",
        "                all_oos_true_values.extend(y_test)\n",
        "                r2_oos_this_year = calculate_r2_oos(y_test, oos_predictions)\n",
        "                test_year_r2_oos[current_test_year] = r2_oos_this_year # Store OOS R2 by test year\n",
        "\n",
        "                print(f\"    Train Year {current_train_end_year} R2_IS : {r2_is_this_year:+.4f}\")\n",
        "                print(f\"    Test Year  {current_test_year} R2_OOS: {r2_oos_this_year:+.4f}\")\n",
        "                print(f\"    Time for year: {time.time() - loop_start_time:.2f}s\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    ERROR during model fitting/prediction for year {current_test_year}: {e}\")\n",
        "\n",
        "\n",
        "        # --- Overall Results ---\n",
        "        print(\"\\n--- Overall OLS-3 Results (MSE Loss) ---\")\n",
        "\n",
        "        # In-Sample Overall\n",
        "        if all_is_true_values:\n",
        "            overall_r2_is = calculate_r2_is(np.array(all_is_true_values), np.array(all_is_predictions))\n",
        "            print(f\"\\nOverall In-Sample R-squared (R2_IS) across all training periods: {overall_r2_is:.4f}\")\n",
        "            print(\"\\nR2_IS per training period (end year):\")\n",
        "            for year, r2 in sorted(train_year_r2_is.items()):\n",
        "                print(f\"  {train_start_year}-{year}: {r2:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo valid in-sample predictions were generated.\")\n",
        "\n",
        "        # Out-of-Sample Overall\n",
        "        if all_oos_true_values:\n",
        "            overall_r2_oos = calculate_r2_oos(np.array(all_oos_true_values), np.array(all_oos_predictions))\n",
        "            print(f\"\\nOverall Out-of-Sample R-squared (R2_OOS) for period {min(actual_test_years)}-{max(actual_test_years)}: {overall_r2_oos:.4f}\")\n",
        "            # Compare to OLS-3 benchmark from paper if desired\n",
        "            print(f\"(Paper's OLS-3 benchmark R2_OOS: 0.16% - Note: Direct comparison depends on exact data/setup)\") #\n",
        "            print(\"\\nR2_OOS per test year:\")\n",
        "            for year, r2 in sorted(test_year_r2_oos.items()):\n",
        "                print(f\"  {year}: {r2:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo valid out-of-sample predictions were generated for the test period.\")\n",
        "\n",
        "        ols_results = {\n",
        "            \"overall_r2_is\": overall_r2_is if all_is_true_values else None,\n",
        "            \"overall_r2_oos\": overall_r2_oos if all_oos_true_values else None,\n",
        "            \"yearly_r2_is\": train_year_r2_is,\n",
        "            \"yearly_r2_oos\": test_year_r2_oos\n",
        "        }\n",
        "\n",
        "\n",
        "# Final summary\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal execution time for OLS-3 cell: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "if 'ols_results' in locals() and ols_results is not None:\n",
        "    print(\"\\nOLS-3 model fitting complete. Results stored in 'ols_results' dictionary.\")\n",
        "elif 'ols_results' not in locals() or ols_results is None:\n",
        "     print(\"\\nOLS-3 model fitting did not complete successfully due to errors.\")"
      ],
      "metadata": {
        "id": "Ipb9XNuZdzlW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37d05fb9-8e7f-4ae0-c8a3-c06e321f5f52"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting OLS-3 Model Fitting (MSE Loss) ---\n",
            "\n",
            "Starting OLS-3 annual refitting from 2019 to 2020...\n",
            "Using Training Start Year: 2002\n",
            "\n",
            "  Processing test year: 2019\n",
            "    Training period: 2002-2018\n",
            "    Train Year 2018 R2_IS : +0.0011\n",
            "    Test Year  2019 R2_OOS: -0.0103\n",
            "    Time for year: 0.16s\n",
            "\n",
            "  Processing test year: 2020\n",
            "    Training period: 2002-2019\n",
            "    Train Year 2019 R2_IS : +0.0011\n",
            "    Test Year  2020 R2_OOS: +0.0103\n",
            "    Time for year: 0.19s\n",
            "\n",
            "--- Overall OLS-3 Results (MSE Loss) ---\n",
            "\n",
            "Overall In-Sample R-squared (R2_IS) across all training periods: 0.0011\n",
            "\n",
            "R2_IS per training period (end year):\n",
            "  2002-2018: 0.0011\n",
            "  2002-2019: 0.0011\n",
            "\n",
            "Overall Out-of-Sample R-squared (R2_OOS) for period 2019-2020: 0.0045\n",
            "(Paper's OLS-3 benchmark R2_OOS: 0.16% - Note: Direct comparison depends on exact data/setup)\n",
            "\n",
            "R2_OOS per test year:\n",
            "  2019: -0.0103\n",
            "  2020: 0.0103\n",
            "\n",
            "Total execution time for OLS-3 cell: 0.46 seconds\n",
            "\n",
            "OLS-3 model fitting complete. Results stored in 'ols_results' dictionary.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Overall NN3 Results (All Features, Ensemble=5)"
      ],
      "metadata": {
        "id": "5-O_j4XOgLq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NN 3\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split # Needed for validation split if not using model.fit's split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "import time\n",
        "import random # For setting random seeds\n",
        "\n",
        "# Suppress TensorFlow warnings and general warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress TF info/warning messages\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "data_folder_path = './'\n",
        "num_files = 10\n",
        "file_prefix = 'factor_'\n",
        "file_suffix = '.csv'\n",
        "\n",
        "# feature_cols will be determined dynamically\n",
        "target_col = 'target'\n",
        "date_col = 'date'\n",
        "ticker_col = 'ticker'\n",
        "\n",
        "train_start_year = 2002\n",
        "test_start_year = 2019\n",
        "test_end_year = 2020\n",
        "\n",
        "# --- NN Configuration ---\n",
        "N_ENSEMBLE = 5 # Number of models in the ensemble\n",
        "EPOCHS = 100 # Max epochs (early stopping will likely stop sooner)\n",
        "BATCH_SIZE = 512\n",
        "VALIDATION_SPLIT_RATIO = 0.15 # Use 15% of training data for validation in Early Stopping\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "L1_REG = 1e-5 # L1 regularization factor (tune if necessary)\n",
        "\n",
        "# --- Functions ---\n",
        "\n",
        "# Use the same robust preprocessing function\n",
        "def preprocess_combined_data(df, date_col, ticker_col, target_col):\n",
        "    \"\"\"\n",
        "    Preprocesses the combined dataframe:\n",
        "    - Sets date index.\n",
        "    - Converts potential feature columns to numeric (coercing errors to NaN).\n",
        "    - Handles infinite values.\n",
        "    - Drops rows with missing target values.\n",
        "    - Imputes missing feature values using monthly median, then 0.\n",
        "    - Returns the processed DataFrame and the list of identified feature columns.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Data Preprocessing ---\")\n",
        "    if df is None:\n",
        "        print(\"ERROR: Input DataFrame is None.\")\n",
        "        return None, None\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        print(\"ERROR: Input is not a pandas DataFrame.\")\n",
        "        return None, None\n",
        "\n",
        "    df_processed = df.copy()\n",
        "    print(f\"Input shape: {df_processed.shape}\")\n",
        "\n",
        "    # 1. Convert date column and set index\n",
        "    if date_col in df_processed.columns:\n",
        "        print(f\"Converting '{date_col}' to datetime...\")\n",
        "        df_processed[date_col] = pd.to_datetime(df_processed[date_col], errors='coerce')\n",
        "        df_processed = df_processed.dropna(subset=[date_col]) # Drop rows where date conversion failed\n",
        "        df_processed = df_processed.set_index(date_col)\n",
        "        print(f\"Shape after date processing: {df_processed.shape}\")\n",
        "    elif not isinstance(df_processed.index, pd.DatetimeIndex):\n",
        "        print(f\"ERROR: DataFrame index is not a DatetimeIndex and '{date_col}' column not found.\")\n",
        "        return None, None\n",
        "    print(\"Date index set.\")\n",
        "\n",
        "    # 2. Identify potential feature columns (all columns except target and ticker)\n",
        "    potential_feature_cols = [col for col in df_processed.columns if col not in [target_col, ticker_col]]\n",
        "    print(f\"Identified {len(potential_feature_cols)} potential feature columns.\")\n",
        "    if not potential_feature_cols:\n",
        "        print(\"ERROR: No potential feature columns found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Keep only necessary columns for now (target, ticker, potential features)\n",
        "    cols_to_keep = [ticker_col, target_col] + potential_feature_cols\n",
        "    missing_cols = [col for col in cols_to_keep if col not in df_processed.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: Missing required columns after date processing: {missing_cols}\")\n",
        "        return None, None\n",
        "    df_processed = df_processed[cols_to_keep]\n",
        "    print(f\"Shape after selecting columns: {df_processed.shape}\")\n",
        "\n",
        "\n",
        "    # 3. Force Feature Columns to Numeric (Convert non-numeric to NaN)\n",
        "    print(\"Attempting to convert identified feature columns to numeric...\")\n",
        "    coerced_count = 0\n",
        "    numeric_cols_found = [] # Keep track of columns confirmed numeric\n",
        "    for col in potential_feature_cols:\n",
        "         if col in df_processed.columns:\n",
        "              if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                  initial_non_numeric = pd.to_numeric(df_processed[col], errors='coerce').isna().sum() - df_processed[col].isna().sum()\n",
        "                  if initial_non_numeric > 0:\n",
        "                       # print(f\"  Found {initial_non_numeric} non-numeric value(s) in '{col}'. Coercing to NaN.\") # Verbose\n",
        "                       coerced_count += initial_non_numeric\n",
        "                  df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "              # Check again if it's numeric now (or was already)\n",
        "              if pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                  numeric_cols_found.append(col)\n",
        "              # else: # Don't warn for columns that couldn't be converted (e.g., pure text)\n",
        "                   # print(f\"  WARNING: Column '{col}' could not be converted to numeric. Excluding.\")\n",
        "         else:\n",
        "              print(f\"Warning: Potential feature column '{col}' not found during numeric conversion phase.\")\n",
        "\n",
        "    potential_feature_cols = numeric_cols_found # Update the list to only include numeric ones\n",
        "    if not potential_feature_cols:\n",
        "        print(\"ERROR: No numeric feature columns identified after conversion attempt.\")\n",
        "        return None, None\n",
        "    print(f\"Confirmed {len(potential_feature_cols)} numeric feature columns.\")\n",
        "    if coerced_count > 0:\n",
        "         print(f\"Coerced a total of {coerced_count} non-numeric entries to NaN across features.\")\n",
        "\n",
        "    # 4. Handle infinite values\n",
        "    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    print(\"Handled infinite values.\")\n",
        "\n",
        "    # 5. Handle TARGET Variable Missing Values\n",
        "    initial_rows = len(df_processed)\n",
        "    n_target_na_initial = df_processed[target_col].isna().sum()\n",
        "    if n_target_na_initial > 0:\n",
        "         print(f\"Found {n_target_na_initial} missing values in '{target_col}'. Dropping rows...\")\n",
        "         df_processed.dropna(subset=[target_col], inplace=True)\n",
        "         rows_after_target_drop = len(df_processed)\n",
        "         rows_dropped = initial_rows - rows_after_target_drop\n",
        "         if rows_dropped > 0:\n",
        "              print(f\"Dropped {rows_dropped} rows due to missing '{target_col}'.\")\n",
        "    else:\n",
        "        print(f\"No missing values found in '{target_col}'.\")\n",
        "\n",
        "    print(f\"Shape after handling target NaNs: {df_processed.shape}\")\n",
        "    if df_processed.empty:\n",
        "         print(\"ERROR: All rows dropped due to missing target or date issues. Cannot proceed.\")\n",
        "         return None, None\n",
        "\n",
        "    # --- Handle FEATURE Missing Values (Median then 0 Imputation) ---\n",
        "    print(\"Applying Median+0 imputation to numeric features...\")\n",
        "    try:\n",
        "        df_processed['month_period'] = df_processed.index.to_period('M')\n",
        "    except AttributeError as e:\n",
        "        print(f\"Error creating 'month_period': {e}. Ensure index is DatetimeIndex.\")\n",
        "        return None, None\n",
        "\n",
        "    imputed_count_median = 0\n",
        "    imputed_count_zero = 0\n",
        "    final_feature_cols = []\n",
        "\n",
        "    for col in potential_feature_cols: # Iterate through confirmed numeric columns\n",
        "        if col in df_processed.columns:\n",
        "            n_initial_na = df_processed[col].isna().sum()\n",
        "            if n_initial_na > 0:\n",
        "                try:\n",
        "                    # Calculate medians per group, handling potential all-NaN groups by filling NaN medians with 0\n",
        "                    medians = df_processed.groupby('month_period')[col].median().fillna(0)\n",
        "                    # Fill NaNs using transform\n",
        "                    df_processed[col] = df_processed.groupby('month_period')[col].transform(lambda x: x.fillna(medians.loc[x.name]))\n",
        "                    imputed_this_median = n_initial_na - df_processed[col].isna().sum() # Count how many were filled by median\n",
        "                    imputed_count_median += imputed_this_median\n",
        "                except Exception as e:\n",
        "                    print(f\"    WARNING: Could not impute median for '{col}'. Error: {e}. Filling remaining NaNs with 0.\")\n",
        "                    # If transform fails, ensure fillna(0) still runs\n",
        "                    df_processed[col].fillna(0, inplace=True)\n",
        "                    imputed_count_zero += df_processed[col].isna().sum() # Count NAs *before* fillna\n",
        "\n",
        "                # Impute any remaining NaNs (if median was NaN or transform failed) with 0\n",
        "                n_remaining_na = df_processed[col].isna().sum()\n",
        "                if n_remaining_na > 0:\n",
        "                    df_processed[col].fillna(0, inplace=True)\n",
        "                    imputed_count_zero += n_remaining_na\n",
        "\n",
        "            # Add to final list if it's still present and numeric (should be)\n",
        "            if col in df_processed.columns and pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                final_feature_cols.append(col)\n",
        "            # else: # Don't warn if excluded due to non-numeric type initially\n",
        "                 # print(f\"  WARNING: Column '{col}' is missing or non-numeric after imputation. Excluding.\")\n",
        "        else:\n",
        "             print(f\"Warning: Numeric feature column '{col}' not found during imputation phase.\")\n",
        "\n",
        "    df_processed.drop(columns=['month_period'], inplace=True)\n",
        "    print(f\"Imputation complete. Imputed ~{imputed_count_median} values with median, {imputed_count_zero} values with 0.\")\n",
        "\n",
        "    if not final_feature_cols:\n",
        "        print(\"ERROR: No numeric feature columns remain after imputation.\")\n",
        "        return None, None\n",
        "\n",
        "    nans_in_features = df_processed[final_feature_cols].isna().sum().sum()\n",
        "    if nans_in_features > 0:\n",
        "         print(f\"WARNING: {nans_in_features} NaNs still remain in final feature columns after imputation! Check imputation logic.\")\n",
        "         # Consider dropping remaining NaNs or using a more robust imputer like IterativeImputer\n",
        "\n",
        "    print(f\"Preprocessing complete. Final shape: {df_processed.shape}\")\n",
        "    print(f\"Using {len(final_feature_cols)} features for modeling.\")\n",
        "    if not df_processed.empty:\n",
        "        print(f\"Final data date range: {df_processed.index.min().strftime('%Y-%m-%d')} to {df_processed.index.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    # Return only the columns needed for modeling\n",
        "    return df_processed[[ticker_col, target_col] + final_feature_cols], final_feature_cols\n",
        "\n",
        "\n",
        "# --- R-squared Calculation Functions ---\n",
        "def calculate_r2_oos(y_true, y_pred):\n",
        "    \"\"\"Calculates Out-of-Sample R-squared (vs. zero forecast).\"\"\"\n",
        "    numerator = ((y_true - y_pred) ** 2).sum()\n",
        "    denominator = (y_true ** 2).sum()\n",
        "    if np.isclose(denominator, 0):\n",
        "        return 1.0 if np.isclose(numerator, 0) else -np.inf\n",
        "    return 1 - (numerator / denominator)\n",
        "\n",
        "def calculate_r2_is(y_true, y_pred):\n",
        "    \"\"\"Calculates In-Sample R-squared (vs. zero forecast).\"\"\"\n",
        "    return calculate_r2_oos(y_true, y_pred)\n",
        "\n",
        "# --- Build NN3 Model Function ---\n",
        "def build_nn3_model(input_shape, l1_reg=1e-5):\n",
        "    \"\"\"Builds the NN3 Keras model with specified architecture.\"\"\"\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=(input_shape,)),\n",
        "            # Layer 1\n",
        "            layers.Dense(32, kernel_regularizer=regularizers.l1(l1_reg)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.ReLU(),\n",
        "            # Layer 2\n",
        "            layers.Dense(16, kernel_regularizer=regularizers.l1(l1_reg)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.ReLU(),\n",
        "            # Layer 3\n",
        "            layers.Dense(8, kernel_regularizer=regularizers.l1(l1_reg)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.ReLU(),\n",
        "            # Output Layer\n",
        "            layers.Dense(1, activation='linear') # Linear activation for regression\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# --- Set Random Seeds for Reproducibility ---\n",
        "def set_seeds(seed_value=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "    # Optional: Configure TensorFlow to be deterministic (might impact performance)\n",
        "    # tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(f\"\\n--- Starting NN3 Model Fitting (All Features, Ensemble={N_ENSEMBLE}) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. Load or ensure 'combined_data' exists\n",
        "if 'combined_data' not in locals() and 'combined_data' not in globals():\n",
        "     print(\"ERROR: 'combined_data' DataFrame not found. Please run the data combination cell first.\")\n",
        "     data_processed = None\n",
        "     feature_cols = None\n",
        "else:\n",
        "    # 2. Preprocess the data AND get the list of features used\n",
        "    data_processed, feature_cols = preprocess_combined_data(combined_data, date_col, ticker_col, target_col)\n",
        "\n",
        "# 3. Proceed with model fitting if preprocessing was successful\n",
        "if data_processed is not None and not data_processed.empty and feature_cols:\n",
        "    print(f\"\\nProceeding with {len(feature_cols)} features.\")\n",
        "    num_features = len(feature_cols)\n",
        "\n",
        "    all_oos_predictions_ensemble_avg = []\n",
        "    all_oos_true_values = [] # True values are the same for all ensemble members\n",
        "    all_is_predictions_ensemble_avg = []\n",
        "    all_is_true_values = [] # True values are the same for all ensemble members\n",
        "\n",
        "    test_year_r2_oos = {}\n",
        "    train_year_r2_is = {}\n",
        "    epochs_stopped_at = {} # Track when early stopping occurred\n",
        "\n",
        "    available_years = sorted(data_processed.index.year.unique())\n",
        "    actual_test_years = [y for y in available_years if y >= test_start_year and y <= test_end_year]\n",
        "\n",
        "    if not actual_test_years:\n",
        "        print(f\"\\nERROR: No data available in the specified test period ({test_start_year}-{test_end_year}) after preprocessing.\")\n",
        "        nn3_results_all = None\n",
        "    else:\n",
        "        print(f\"\\nStarting NN3 annual refitting from {min(actual_test_years)} to {max(actual_test_years)}...\")\n",
        "        print(f\"Using Training Start Year: {train_start_year}\")\n",
        "\n",
        "        for current_test_year in actual_test_years:\n",
        "            loop_start_time = time.time()\n",
        "            print(f\"\\n  Processing test year: {current_test_year}\")\n",
        "\n",
        "            current_train_end_year = current_test_year - 1\n",
        "            if current_train_end_year < train_start_year:\n",
        "                 print(f\"    Skipping year {current_test_year}: Training end year ({current_train_end_year}) is before training start year ({train_start_year}).\")\n",
        "                 continue\n",
        "\n",
        "            print(f\"    Training period: {train_start_year}-{current_train_end_year}\")\n",
        "            train_mask = (data_processed.index.year >= train_start_year) & (data_processed.index.year <= current_train_end_year)\n",
        "            test_mask = (data_processed.index.year == current_test_year)\n",
        "            train_df = data_processed.loc[train_mask]\n",
        "            test_df = data_processed.loc[test_mask]\n",
        "\n",
        "            if train_df.empty or test_df.empty:\n",
        "                print(f\"    Skipping year {current_test_year}: Not enough data for train ({len(train_df)}) / test ({len(test_df)}) split.\")\n",
        "                continue\n",
        "\n",
        "            # Check for NaNs before scaling\n",
        "            if train_df[feature_cols].isna().any().any() or test_df[feature_cols].isna().any().any():\n",
        "                 print(f\"    ERROR: NaNs detected in feature columns for year {current_test_year} before scaling! Skipping.\")\n",
        "                 continue\n",
        "\n",
        "            # Prepare data\n",
        "            X_train_full = train_df[feature_cols].values\n",
        "            y_train_full = train_df[target_col].values\n",
        "            X_test = test_df[feature_cols].values\n",
        "            y_test = test_df[target_col].values\n",
        "\n",
        "            # --- Scaling ---\n",
        "            # Scale based on the FULL training data for this period\n",
        "            scaler = StandardScaler()\n",
        "            try:\n",
        "                X_train_full_scaled = scaler.fit_transform(X_train_full)\n",
        "                X_test_scaled = scaler.transform(X_test) # Use the same scaler\n",
        "\n",
        "                # Check for NaNs again AFTER scaling and impute if necessary\n",
        "                if np.isnan(X_train_full_scaled).any() or np.isnan(X_test_scaled).any():\n",
        "                    print(f\"    Warning: NaNs generated during scaling for year {current_test_year}. Imputing with 0.\")\n",
        "                    X_train_full_scaled = np.nan_to_num(X_train_full_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                    X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"    ERROR during scaling for year {current_test_year}: {e}. Skipping year.\")\n",
        "                continue\n",
        "\n",
        "            # --- Ensemble Training and Prediction ---\n",
        "            ensemble_is_predictions = []\n",
        "            ensemble_oos_predictions = []\n",
        "            ensemble_epochs_stopped = []\n",
        "\n",
        "            print(f\"    Starting ensemble training (N={N_ENSEMBLE})...\")\n",
        "            for i in range(N_ENSEMBLE):\n",
        "                print(f\"      Training model {i+1}/{N_ENSEMBLE}...\")\n",
        "                # Set different seeds for each model instance\n",
        "                set_seeds(seed_value=42 + i)\n",
        "\n",
        "                # Build and compile a new model instance\n",
        "                model = build_nn3_model(input_shape=num_features, l1_reg=L1_REG)\n",
        "                model.compile(optimizer=keras.optimizers.Adam(), loss='mean_squared_error')\n",
        "\n",
        "                # Define Early Stopping callback\n",
        "                early_stopping = EarlyStopping(\n",
        "                    monitor='val_loss',       # Monitor loss on validation data\n",
        "                    patience=EARLY_STOPPING_PATIENCE,         # Number of epochs with no improvement to wait\n",
        "                    restore_best_weights=True # Restore model weights from the epoch with the best val_loss\n",
        "                )\n",
        "\n",
        "                # Train the model using validation_split within fit\n",
        "                try:\n",
        "                     history = model.fit(\n",
        "                         X_train_full_scaled,\n",
        "                         y_train_full,\n",
        "                         epochs=EPOCHS,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         validation_split=VALIDATION_SPLIT_RATIO, # Use Keras' built-in split\n",
        "                         callbacks=[early_stopping],\n",
        "                         verbose=0 # Set to 1 or 2 for progress, 0 for silent\n",
        "                     )\n",
        "                     stopped_epoch = early_stopping.stopped_epoch\n",
        "                     if stopped_epoch > 0: # stopped_epoch is 0 if it didn't stop early\n",
        "                          print(f\"        Model {i+1} stopped early at epoch {stopped_epoch + 1}\")\n",
        "                          ensemble_epochs_stopped.append(stopped_epoch + 1)\n",
        "                     else:\n",
        "                          print(f\"        Model {i+1} completed all {EPOCHS} epochs.\")\n",
        "                          ensemble_epochs_stopped.append(EPOCHS)\n",
        "\n",
        "\n",
        "                     # Predict (In-Sample and Out-of-Sample)\n",
        "                     is_preds = model.predict(X_train_full_scaled, batch_size=BATCH_SIZE).flatten()\n",
        "                     oos_preds = model.predict(X_test_scaled, batch_size=BATCH_SIZE).flatten()\n",
        "\n",
        "                     ensemble_is_predictions.append(is_preds)\n",
        "                     ensemble_oos_predictions.append(oos_preds)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"      ERROR training/predicting model {i+1} for year {current_test_year}: {e}\")\n",
        "                    # If one model fails, maybe skip it or handle appropriately\n",
        "                    # For now, we'll just print the error and potentially have fewer models in the average\n",
        "                    ensemble_epochs_stopped.append(np.nan) # Mark failure\n",
        "                    continue # Skip to the next ensemble member\n",
        "\n",
        "                # Clear session to free memory (optional, but can help with many models/features)\n",
        "                tf.keras.backend.clear_session()\n",
        "\n",
        "\n",
        "            # --- Aggregate Ensemble Predictions ---\n",
        "            if not ensemble_oos_predictions: # Check if any models trained successfully\n",
        "                 print(f\"    ERROR: No models in the ensemble trained successfully for year {current_test_year}. Skipping.\")\n",
        "                 continue\n",
        "\n",
        "            # Average the predictions from all successful models in the ensemble\n",
        "            avg_is_predictions = np.mean(np.array(ensemble_is_predictions), axis=0)\n",
        "            avg_oos_predictions = np.mean(np.array(ensemble_oos_predictions), axis=0)\n",
        "\n",
        "            # Store the averaged predictions and the true values\n",
        "            all_is_predictions_ensemble_avg.extend(avg_is_predictions)\n",
        "            all_is_true_values.extend(y_train_full) # True values only need to be stored once per year\n",
        "            all_oos_predictions_ensemble_avg.extend(avg_oos_predictions)\n",
        "            all_oos_true_values.extend(y_test) # True values only need to be stored once per year\n",
        "\n",
        "            # Calculate R-squared based on averaged predictions\n",
        "            r2_is_this_year = calculate_r2_is(y_train_full, avg_is_predictions)\n",
        "            r2_oos_this_year = calculate_r2_oos(y_test, avg_oos_predictions)\n",
        "\n",
        "            train_year_r2_is[current_train_end_year] = r2_is_this_year\n",
        "            test_year_r2_oos[current_test_year] = r2_oos_this_year\n",
        "            epochs_stopped_at[current_train_end_year] = np.nanmean(ensemble_epochs_stopped) # Average epochs stopped\n",
        "\n",
        "            print(f\"    Train Year {current_train_end_year} Avg R2_IS : {r2_is_this_year:+.4f}\")\n",
        "            print(f\"    Test Year  {current_test_year} Avg R2_OOS: {r2_oos_this_year:+.4f}\")\n",
        "            print(f\"    Avg Epochs Stopped: {epochs_stopped_at[current_train_end_year]:.1f}\")\n",
        "            print(f\"    Time for year: {time.time() - loop_start_time:.2f}s\")\n",
        "\n",
        "\n",
        "        # --- Overall Results ---\n",
        "        print(f\"\\n--- Overall NN3 Results (All Features, Ensemble={N_ENSEMBLE}) ---\")\n",
        "\n",
        "        # In-Sample Overall\n",
        "        overall_r2_is = None\n",
        "        if all_is_true_values:\n",
        "            overall_r2_is = calculate_r2_is(np.array(all_is_true_values), np.array(all_is_predictions_ensemble_avg))\n",
        "            print(f\"\\nOverall In-Sample R-squared (R2_IS) across all training periods: {overall_r2_is:.4f}\")\n",
        "            print(\"\\nR2_IS per training period (end year):\")\n",
        "            for year, r2 in sorted(train_year_r2_is.items()):\n",
        "                print(f\"  {train_start_year}-{year}: {r2:.4f} (Avg Epochs: {epochs_stopped_at.get(year, 'N/A'):.0f})\")\n",
        "        else:\n",
        "            print(\"\\nNo valid in-sample predictions were generated.\")\n",
        "\n",
        "        # Out-of-Sample Overall\n",
        "        overall_r2_oos = None\n",
        "        if all_oos_true_values:\n",
        "            overall_r2_oos = calculate_r2_oos(np.array(all_oos_true_values), np.array(all_oos_predictions_ensemble_avg))\n",
        "            print(f\"\\nOverall Out-of-Sample R-squared (R2_OOS) for period {min(actual_test_years)}-{max(actual_test_years)}: {overall_r2_oos:.4f}\")\n",
        "            print(f\"(Paper's NN3 (all features) R2_OOS: 0.40%)\") # Add NN3 comparison [source: 91]\n",
        "            print(\"\\nR2_OOS per test year:\")\n",
        "            for year, r2 in sorted(test_year_r2_oos.items()):\n",
        "                 print(f\"  {year}: {r2:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo valid out-of-sample predictions were generated for the test period.\")\n",
        "\n",
        "        # Store results\n",
        "        nn3_results_all = {\n",
        "            \"overall_r2_is\": overall_r2_is,\n",
        "            \"overall_r2_oos\": overall_r2_oos,\n",
        "            \"yearly_r2_is\": train_year_r2_is,\n",
        "            \"yearly_r2_oos\": test_year_r2_oos,\n",
        "            \"avg_epochs_stopped_per_training_year\": epochs_stopped_at,\n",
        "            \"features_used\": feature_cols,\n",
        "            \"ensemble_size\": N_ENSEMBLE\n",
        "        }\n",
        "\n",
        "else:\n",
        "    # This block executes if preprocessing failed\n",
        "    print(\"\\nData preprocessing failed or resulted in empty DataFrame/features. Cannot proceed with NN3 model fitting.\")\n",
        "    nn3_results_all = None # Indicate failure\n",
        "\n",
        "# Final summary\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal execution time for NN3 (All Features, Ensemble) cell: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "if 'nn3_results_all' in locals() and nn3_results_all is not None:\n",
        "    print(\"\\nNN3 (All Features, Ensemble) model fitting complete. Results stored in 'nn3_results_all' dictionary.\")\n",
        "    print(f\"Number of features used: {len(nn3_results_all.get('features_used', []))}\")\n",
        "    print(f\"Ensemble size: {nn3_results_all.get('ensemble_size', 'N/A')}\")\n",
        "elif 'nn3_results_all' not in locals() or nn3_results_all is None:\n",
        "     print(\"\\nNN3 (All Features, Ensemble) model fitting did not complete successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0ZvuikhpYzG",
        "outputId": "493072d9-d0b5-4f8f-bfc8-e3caa7992cb3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting NN3 Model Fitting (All Features, Ensemble=5) ---\n",
            "--- Starting Data Preprocessing ---\n",
            "Input shape: (336740, 138)\n",
            "Converting 'date' to datetime...\n",
            "Shape after date processing: (336740, 137)\n",
            "Date index set.\n",
            "Identified 135 potential feature columns.\n",
            "Shape after selecting columns: (336740, 137)\n",
            "Attempting to convert identified feature columns to numeric...\n",
            "Confirmed 135 numeric feature columns.\n",
            "Coerced a total of 123 non-numeric entries to NaN across features.\n",
            "Handled infinite values.\n",
            "Found 2196 missing values in 'target'. Dropping rows...\n",
            "Dropped 2196 rows due to missing 'target'.\n",
            "Shape after handling target NaNs: (334544, 137)\n",
            "Applying Median+0 imputation to numeric features...\n",
            "Imputation complete. Imputed ~1010996 values with median, 0 values with 0.\n",
            "Preprocessing complete. Final shape: (334544, 137)\n",
            "Using 135 features for modeling.\n",
            "Final data date range: 2002-05-01 to 2021-08-01\n",
            "\n",
            "Proceeding with 135 features.\n",
            "\n",
            "Starting NN3 annual refitting from 2019 to 2020...\n",
            "Using Training Start Year: 2002\n",
            "\n",
            "  Processing test year: 2019\n",
            "    Training period: 2002-2018\n",
            "    Starting ensemble training (N=5)...\n",
            "      Training model 1/5...\n",
            "        Model 1 stopped early at epoch 13\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 2/5...\n",
            "        Model 2 stopped early at epoch 11\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 3/5...\n",
            "        Model 3 stopped early at epoch 11\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 4/5...\n",
            "        Model 4 stopped early at epoch 12\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 5/5...\n",
            "        Model 5 stopped early at epoch 13\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "    Train Year 2018 Avg R2_IS : +0.0075\n",
            "    Test Year  2019 Avg R2_OOS: -0.0002\n",
            "    Avg Epochs Stopped: 12.0\n",
            "    Time for year: 195.62s\n",
            "\n",
            "  Processing test year: 2020\n",
            "    Training period: 2002-2019\n",
            "    Starting ensemble training (N=5)...\n",
            "      Training model 1/5...\n",
            "        Model 1 stopped early at epoch 12\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 2/5...\n",
            "        Model 2 stopped early at epoch 11\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 3/5...\n",
            "        Model 3 stopped early at epoch 12\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "      Training model 4/5...\n",
            "        Model 4 stopped early at epoch 14\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 5/5...\n",
            "        Model 5 stopped early at epoch 15\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "    Train Year 2019 Avg R2_IS : +0.0094\n",
            "    Test Year  2020 Avg R2_OOS: -0.0963\n",
            "    Avg Epochs Stopped: 12.8\n",
            "    Time for year: 206.73s\n",
            "\n",
            "--- Overall NN3 Results (All Features, Ensemble=5) ---\n",
            "\n",
            "Overall In-Sample R-squared (R2_IS) across all training periods: 0.0085\n",
            "\n",
            "R2_IS per training period (end year):\n",
            "  2002-2018: 0.0075 (Avg Epochs: 12)\n",
            "  2002-2019: 0.0094 (Avg Epochs: 13)\n",
            "\n",
            "Overall Out-of-Sample R-squared (R2_OOS) for period 2019-2020: -0.0696\n",
            "(Paper's NN3 (all features) R2_OOS: 0.40%)\n",
            "\n",
            "R2_OOS per test year:\n",
            "  2019: -0.0002\n",
            "  2020: -0.0963\n",
            "\n",
            "Total execution time for NN3 (All Features, Ensemble) cell: 420.80 seconds\n",
            "\n",
            "NN3 (All Features, Ensemble) model fitting complete. Results stored in 'nn3_results_all' dictionary.\n",
            "Number of features used: 135\n",
            "Ensemble size: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NN3 Results (3 Features, Ensemble=5)"
      ],
      "metadata": {
        "id": "dl7sTYFkgc-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 feature NN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import regularizers\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split # Needed for validation split if not using model.fit's split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "import time\n",
        "import random # For setting random seeds\n",
        "\n",
        "# Suppress TensorFlow warnings and general warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Suppress TF info/warning messages\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "data_folder_path = './'\n",
        "num_files = 10\n",
        "file_prefix = 'factor_'\n",
        "file_suffix = '.csv'\n",
        "\n",
        "# Define the specific 3 features to use\n",
        "specific_feature_cols = ['size_rnk', 'BPR', 'mom12']\n",
        "target_col = 'target'\n",
        "date_col = 'date'\n",
        "ticker_col = 'ticker'\n",
        "\n",
        "train_start_year = 2002\n",
        "test_start_year = 2019\n",
        "test_end_year = 2020\n",
        "\n",
        "# --- NN Configuration ---\n",
        "N_ENSEMBLE = 5 # Number of models in the ensemble\n",
        "EPOCHS = 100 # Max epochs (early stopping will likely stop sooner)\n",
        "BATCH_SIZE = 512\n",
        "VALIDATION_SPLIT_RATIO = 0.15 # Use 15% of training data for validation in Early Stopping\n",
        "EARLY_STOPPING_PATIENCE = 10\n",
        "L1_REG = 1e-5 # L1 regularization factor (tune if necessary)\n",
        "\n",
        "# --- Functions ---\n",
        "\n",
        "# Use the same robust preprocessing function, but we will select columns *after*\n",
        "def preprocess_combined_data(df, date_col, ticker_col, target_col):\n",
        "    \"\"\"\n",
        "    Preprocesses the combined dataframe:\n",
        "    - Sets date index.\n",
        "    - Converts potential feature columns to numeric (coercing errors to NaN).\n",
        "    - Handles infinite values.\n",
        "    - Drops rows with missing target values.\n",
        "    - Imputes missing feature values using monthly median, then 0.\n",
        "    - Returns the processed DataFrame and the list of ALL identified numeric feature columns.\n",
        "      (Column selection happens *after* this function call in this script)\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Data Preprocessing ---\")\n",
        "    if df is None:\n",
        "        print(\"ERROR: Input DataFrame is None.\")\n",
        "        return None, None\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        print(\"ERROR: Input is not a pandas DataFrame.\")\n",
        "        return None, None\n",
        "\n",
        "    df_processed = df.copy()\n",
        "    print(f\"Input shape: {df_processed.shape}\")\n",
        "\n",
        "    # 1. Convert date column and set index\n",
        "    if date_col in df_processed.columns:\n",
        "        print(f\"Converting '{date_col}' to datetime...\")\n",
        "        df_processed[date_col] = pd.to_datetime(df_processed[date_col], errors='coerce')\n",
        "        df_processed = df_processed.dropna(subset=[date_col]) # Drop rows where date conversion failed\n",
        "        df_processed = df_processed.set_index(date_col)\n",
        "        print(f\"Shape after date processing: {df_processed.shape}\")\n",
        "    elif not isinstance(df_processed.index, pd.DatetimeIndex):\n",
        "        print(f\"ERROR: DataFrame index is not a DatetimeIndex and '{date_col}' column not found.\")\n",
        "        return None, None\n",
        "    print(\"Date index set.\")\n",
        "\n",
        "    # 2. Identify potential feature columns (all columns except target and ticker)\n",
        "    potential_feature_cols = [col for col in df_processed.columns if col not in [target_col, ticker_col]]\n",
        "    print(f\"Identified {len(potential_feature_cols)} potential feature columns.\")\n",
        "    if not potential_feature_cols:\n",
        "        print(\"ERROR: No potential feature columns found.\")\n",
        "        return None, None\n",
        "\n",
        "    # Keep only necessary columns for now (target, ticker, potential features)\n",
        "    # Ensure target and ticker are present before selecting\n",
        "    required_base_cols = [ticker_col, target_col]\n",
        "    if not all(col in df_processed.columns for col in required_base_cols):\n",
        "         print(f\"ERROR: Missing base columns ({ticker_col}, {target_col}) in DataFrame.\")\n",
        "         return None, None\n",
        "\n",
        "    cols_to_keep = required_base_cols + potential_feature_cols\n",
        "    # Ensure no duplicates if ticker/target were somehow in potential_feature_cols\n",
        "    cols_to_keep = list(dict.fromkeys(cols_to_keep))\n",
        "    # Check if all columns actually exist\n",
        "    missing_cols = [col for col in cols_to_keep if col not in df_processed.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: Missing required/potential columns after date processing: {missing_cols}\")\n",
        "        # Remove missing potential features instead of failing?\n",
        "        potential_feature_cols = [col for col in potential_feature_cols if col in df_processed.columns]\n",
        "        cols_to_keep = required_base_cols + potential_feature_cols\n",
        "        print(f\"Proceeding with available columns: {cols_to_keep}\")\n",
        "        # return None, None # Or adjust cols_to_keep\n",
        "\n",
        "    df_processed = df_processed[cols_to_keep]\n",
        "    print(f\"Shape after selecting columns: {df_processed.shape}\")\n",
        "\n",
        "\n",
        "    # 3. Force Feature Columns to Numeric (Convert non-numeric to NaN)\n",
        "    print(\"Attempting to convert identified feature columns to numeric...\")\n",
        "    coerced_count = 0\n",
        "    numeric_cols_found = [] # Keep track of columns confirmed numeric\n",
        "    for col in potential_feature_cols:\n",
        "         if col in df_processed.columns:\n",
        "              if not pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                  initial_non_numeric = pd.to_numeric(df_processed[col], errors='coerce').isna().sum() - df_processed[col].isna().sum()\n",
        "                  if initial_non_numeric > 0:\n",
        "                       # print(f\"  Found {initial_non_numeric} non-numeric value(s) in '{col}'. Coercing to NaN.\") # Verbose\n",
        "                       coerced_count += initial_non_numeric\n",
        "                  df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "              # Check again if it's numeric now (or was already)\n",
        "              if pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                  numeric_cols_found.append(col)\n",
        "         # No warning needed if column not found here, handled above\n",
        "    potential_feature_cols = numeric_cols_found # Update the list to only include numeric ones\n",
        "\n",
        "    if not potential_feature_cols:\n",
        "        print(\"ERROR: No numeric feature columns identified after conversion attempt.\")\n",
        "        return None, None\n",
        "    print(f\"Confirmed {len(potential_feature_cols)} numeric feature columns.\")\n",
        "    if coerced_count > 0:\n",
        "         print(f\"Coerced a total of {coerced_count} non-numeric entries to NaN across features.\")\n",
        "\n",
        "    # 4. Handle infinite values\n",
        "    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    print(\"Handled infinite values.\")\n",
        "\n",
        "    # 5. Handle TARGET Variable Missing Values\n",
        "    initial_rows = len(df_processed)\n",
        "    n_target_na_initial = df_processed[target_col].isna().sum()\n",
        "    if n_target_na_initial > 0:\n",
        "         print(f\"Found {n_target_na_initial} missing values in '{target_col}'. Dropping rows...\")\n",
        "         df_processed.dropna(subset=[target_col], inplace=True)\n",
        "         rows_after_target_drop = len(df_processed)\n",
        "         rows_dropped = initial_rows - rows_after_target_drop\n",
        "         if rows_dropped > 0:\n",
        "              print(f\"Dropped {rows_dropped} rows due to missing '{target_col}'.\")\n",
        "    else:\n",
        "        print(f\"No missing values found in '{target_col}'.\")\n",
        "\n",
        "    print(f\"Shape after handling target NaNs: {df_processed.shape}\")\n",
        "    if df_processed.empty:\n",
        "         print(\"ERROR: All rows dropped due to missing target or date issues. Cannot proceed.\")\n",
        "         return None, None\n",
        "\n",
        "    # --- Handle FEATURE Missing Values (Median then 0 Imputation) ---\n",
        "    print(\"Applying Median+0 imputation to numeric features...\")\n",
        "    try:\n",
        "        df_processed['month_period'] = df_processed.index.to_period('M')\n",
        "    except AttributeError as e:\n",
        "        print(f\"Error creating 'month_period': {e}. Ensure index is DatetimeIndex.\")\n",
        "        return None, None\n",
        "\n",
        "    imputed_count_median = 0\n",
        "    imputed_count_zero = 0\n",
        "    final_numeric_cols = [] # Renamed for clarity\n",
        "\n",
        "    for col in potential_feature_cols: # Iterate through confirmed numeric columns\n",
        "        if col in df_processed.columns:\n",
        "            n_initial_na = df_processed[col].isna().sum()\n",
        "            if n_initial_na > 0:\n",
        "                try:\n",
        "                    medians = df_processed.groupby('month_period')[col].median().fillna(0)\n",
        "                    df_processed[col] = df_processed.groupby('month_period')[col].transform(lambda x: x.fillna(medians.loc[x.name]))\n",
        "                    imputed_this_median = n_initial_na - df_processed[col].isna().sum()\n",
        "                    imputed_count_median += imputed_this_median\n",
        "                except Exception as e:\n",
        "                    print(f\"    WARNING: Could not impute median for '{col}'. Error: {e}. Filling remaining NaNs with 0.\")\n",
        "                    df_processed[col].fillna(0, inplace=True)\n",
        "                    imputed_count_zero += df_processed[col].isna().sum() # Count NAs *before* fillna\n",
        "\n",
        "                n_remaining_na = df_processed[col].isna().sum()\n",
        "                if n_remaining_na > 0:\n",
        "                    df_processed[col].fillna(0, inplace=True)\n",
        "                    imputed_count_zero += n_remaining_na\n",
        "\n",
        "            if col in df_processed.columns and pd.api.types.is_numeric_dtype(df_processed[col]):\n",
        "                final_numeric_cols.append(col) # Keep track of all numeric cols processed\n",
        "\n",
        "    df_processed.drop(columns=['month_period'], inplace=True)\n",
        "    print(f\"Imputation complete. Imputed ~{imputed_count_median} values with median, {imputed_count_zero} values with 0.\")\n",
        "\n",
        "    if not final_numeric_cols:\n",
        "        print(\"ERROR: No numeric feature columns remain after imputation.\")\n",
        "        return None, None\n",
        "\n",
        "    nans_in_features = df_processed[final_numeric_cols].isna().sum().sum()\n",
        "    if nans_in_features > 0:\n",
        "         print(f\"WARNING: {nans_in_features} NaNs still remain in final numeric columns after imputation!\")\n",
        "\n",
        "    print(f\"Preprocessing complete. Final shape: {df_processed.shape}\")\n",
        "    print(f\"Total {len(final_numeric_cols)} numeric features processed.\")\n",
        "    if not df_processed.empty:\n",
        "        print(f\"Final data date range: {df_processed.index.min().strftime('%Y-%m-%d')} to {df_processed.index.max().strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    # Return the dataframe containing ALL processed numeric columns and the list of their names\n",
        "    return df_processed, final_numeric_cols\n",
        "\n",
        "\n",
        "# --- R-squared Calculation Functions ---\n",
        "def calculate_r2_oos(y_true, y_pred):\n",
        "    \"\"\"Calculates Out-of-Sample R-squared (vs. zero forecast).\"\"\"\n",
        "    numerator = ((y_true - y_pred) ** 2).sum()\n",
        "    denominator = (y_true ** 2).sum()\n",
        "    if np.isclose(denominator, 0):\n",
        "        return 1.0 if np.isclose(numerator, 0) else -np.inf\n",
        "    return 1 - (numerator / denominator)\n",
        "\n",
        "def calculate_r2_is(y_true, y_pred):\n",
        "    \"\"\"Calculates In-Sample R-squared (vs. zero forecast).\"\"\"\n",
        "    return calculate_r2_oos(y_true, y_pred)\n",
        "\n",
        "# --- Build NN3 Model Function ---\n",
        "# Model structure remains the same, input shape will change\n",
        "def build_nn3_model(input_shape, l1_reg=1e-5):\n",
        "    \"\"\"Builds the NN3 Keras model with specified architecture.\"\"\"\n",
        "    model = keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=(input_shape,)), # input_shape will be 3 here\n",
        "            # Layer 1\n",
        "            layers.Dense(32, kernel_regularizer=regularizers.l1(l1_reg)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.ReLU(),\n",
        "            # Layer 2\n",
        "            layers.Dense(16, kernel_regularizer=regularizers.l1(l1_reg)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.ReLU(),\n",
        "            # Layer 3\n",
        "            layers.Dense(8, kernel_regularizer=regularizers.l1(l1_reg)),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.ReLU(),\n",
        "            # Output Layer\n",
        "            layers.Dense(1, activation='linear') # Linear activation for regression\n",
        "        ]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# --- Set Random Seeds for Reproducibility ---\n",
        "def set_seeds(seed_value=42):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    tf.random.set_seed(seed_value)\n",
        "    # tf.config.experimental.enable_op_determinism() # Optional for determinism\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(f\"\\n--- Starting NN3 Model Fitting (3 Features: {specific_feature_cols}, Ensemble={N_ENSEMBLE}) ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# 1. Load or ensure 'combined_data' exists\n",
        "if 'combined_data' not in locals() and 'combined_data' not in globals():\n",
        "     print(\"ERROR: 'combined_data' DataFrame not found. Please run the data combination cell first.\")\n",
        "     data_processed = None\n",
        "     all_numeric_features = None\n",
        "else:\n",
        "    # 2. Preprocess the data (handles imputation etc. for ALL numeric columns)\n",
        "    data_processed, all_numeric_features = preprocess_combined_data(combined_data, date_col, ticker_col, target_col)\n",
        "\n",
        "# 3. Proceed with model fitting ONLY IF preprocessing was successful AND the 3 specific columns exist\n",
        "feature_cols = specific_feature_cols # Explicitly set the features to use\n",
        "num_features = len(feature_cols)\n",
        "\n",
        "if data_processed is not None and not data_processed.empty and all(col in data_processed.columns for col in feature_cols):\n",
        "    print(f\"\\nProceeding with {num_features} specific features: {feature_cols}\")\n",
        "\n",
        "    all_oos_predictions_ensemble_avg = []\n",
        "    all_oos_true_values = []\n",
        "    all_is_predictions_ensemble_avg = []\n",
        "    all_is_true_values = []\n",
        "\n",
        "    test_year_r2_oos = {}\n",
        "    train_year_r2_is = {}\n",
        "    epochs_stopped_at = {}\n",
        "\n",
        "    available_years = sorted(data_processed.index.year.unique())\n",
        "    actual_test_years = [y for y in available_years if y >= test_start_year and y <= test_end_year]\n",
        "\n",
        "    if not actual_test_years:\n",
        "        print(f\"\\nERROR: No data available in the specified test period ({test_start_year}-{test_end_year}) after preprocessing.\")\n",
        "        nn3_results_3f = None\n",
        "    else:\n",
        "        print(f\"\\nStarting NN3 (3 Features) annual refitting from {min(actual_test_years)} to {max(actual_test_years)}...\")\n",
        "        print(f\"Using Training Start Year: {train_start_year}\")\n",
        "\n",
        "        for current_test_year in actual_test_years:\n",
        "            loop_start_time = time.time()\n",
        "            print(f\"\\n  Processing test year: {current_test_year}\")\n",
        "\n",
        "            current_train_end_year = current_test_year - 1\n",
        "            if current_train_end_year < train_start_year:\n",
        "                 print(f\"    Skipping year {current_test_year}: Training end year ({current_train_end_year}) is before training start year ({train_start_year}).\")\n",
        "                 continue\n",
        "\n",
        "            print(f\"    Training period: {train_start_year}-{current_train_end_year}\")\n",
        "            train_mask = (data_processed.index.year >= train_start_year) & (data_processed.index.year <= current_train_end_year)\n",
        "            test_mask = (data_processed.index.year == current_test_year)\n",
        "            # Select only the required columns (target + 3 features) for this period\n",
        "            train_df = data_processed.loc[train_mask, [target_col] + feature_cols]\n",
        "            test_df = data_processed.loc[test_mask, [target_col] + feature_cols]\n",
        "\n",
        "            if train_df.empty or test_df.empty:\n",
        "                print(f\"    Skipping year {current_test_year}: Not enough data for train ({len(train_df)}) / test ({len(test_df)}) split.\")\n",
        "                continue\n",
        "\n",
        "            # Check for NaNs in the 3 specific features before scaling\n",
        "            if train_df[feature_cols].isna().any().any() or test_df[feature_cols].isna().any().any():\n",
        "                 print(f\"    ERROR: NaNs detected in the 3 specific feature columns for year {current_test_year} before scaling! Skipping.\")\n",
        "                 continue\n",
        "\n",
        "            # Prepare data using the 3 features\n",
        "            X_train_full = train_df[feature_cols].values\n",
        "            y_train_full = train_df[target_col].values\n",
        "            X_test = test_df[feature_cols].values\n",
        "            y_test = test_df[target_col].values\n",
        "\n",
        "            # --- Scaling ---\n",
        "            scaler = StandardScaler()\n",
        "            try:\n",
        "                X_train_full_scaled = scaler.fit_transform(X_train_full)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                if np.isnan(X_train_full_scaled).any() or np.isnan(X_test_scaled).any():\n",
        "                    print(f\"    Warning: NaNs generated during scaling for year {current_test_year}. Imputing with 0.\")\n",
        "                    X_train_full_scaled = np.nan_to_num(X_train_full_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "                    X_test_scaled = np.nan_to_num(X_test_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"    ERROR during scaling for year {current_test_year}: {e}. Skipping year.\")\n",
        "                continue\n",
        "\n",
        "            # --- Ensemble Training and Prediction ---\n",
        "            ensemble_is_predictions = []\n",
        "            ensemble_oos_predictions = []\n",
        "            ensemble_epochs_stopped = []\n",
        "\n",
        "            print(f\"    Starting ensemble training (N={N_ENSEMBLE})...\")\n",
        "            for i in range(N_ENSEMBLE):\n",
        "                print(f\"      Training model {i+1}/{N_ENSEMBLE}...\")\n",
        "                set_seeds(seed_value=42 + i)\n",
        "\n",
        "                # Build model with input_shape=3\n",
        "                model = build_nn3_model(input_shape=num_features, l1_reg=L1_REG)\n",
        "                model.compile(optimizer=keras.optimizers.Adam(), loss='mean_squared_error')\n",
        "\n",
        "                early_stopping = EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)\n",
        "\n",
        "                try:\n",
        "                     history = model.fit(\n",
        "                         X_train_full_scaled, y_train_full,\n",
        "                         epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
        "                         validation_split=VALIDATION_SPLIT_RATIO,\n",
        "                         callbacks=[early_stopping], verbose=0\n",
        "                     )\n",
        "                     stopped_epoch = early_stopping.stopped_epoch\n",
        "                     if stopped_epoch > 0:\n",
        "                          print(f\"        Model {i+1} stopped early at epoch {stopped_epoch + 1}\")\n",
        "                          ensemble_epochs_stopped.append(stopped_epoch + 1)\n",
        "                     else:\n",
        "                          print(f\"        Model {i+1} completed all {EPOCHS} epochs.\")\n",
        "                          ensemble_epochs_stopped.append(EPOCHS)\n",
        "\n",
        "                     is_preds = model.predict(X_train_full_scaled, batch_size=BATCH_SIZE).flatten()\n",
        "                     oos_preds = model.predict(X_test_scaled, batch_size=BATCH_SIZE).flatten()\n",
        "                     ensemble_is_predictions.append(is_preds)\n",
        "                     ensemble_oos_predictions.append(oos_preds)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"      ERROR training/predicting model {i+1} for year {current_test_year}: {e}\")\n",
        "                    ensemble_epochs_stopped.append(np.nan)\n",
        "                    continue\n",
        "\n",
        "                tf.keras.backend.clear_session()\n",
        "\n",
        "            # --- Aggregate Ensemble Predictions ---\n",
        "            if not ensemble_oos_predictions:\n",
        "                 print(f\"    ERROR: No models trained successfully for year {current_test_year}. Skipping.\")\n",
        "                 continue\n",
        "\n",
        "            avg_is_predictions = np.mean(np.array(ensemble_is_predictions), axis=0)\n",
        "            avg_oos_predictions = np.mean(np.array(ensemble_oos_predictions), axis=0)\n",
        "\n",
        "            all_is_predictions_ensemble_avg.extend(avg_is_predictions)\n",
        "            all_is_true_values.extend(y_train_full)\n",
        "            all_oos_predictions_ensemble_avg.extend(avg_oos_predictions)\n",
        "            all_oos_true_values.extend(y_test)\n",
        "\n",
        "            r2_is_this_year = calculate_r2_is(y_train_full, avg_is_predictions)\n",
        "            r2_oos_this_year = calculate_r2_oos(y_test, avg_oos_predictions)\n",
        "\n",
        "            train_year_r2_is[current_train_end_year] = r2_is_this_year\n",
        "            test_year_r2_oos[current_test_year] = r2_oos_this_year\n",
        "            epochs_stopped_at[current_train_end_year] = np.nanmean(ensemble_epochs_stopped)\n",
        "\n",
        "            print(f\"    Train Year {current_train_end_year} Avg R2_IS : {r2_is_this_year:+.4f}\")\n",
        "            print(f\"    Test Year  {current_test_year} Avg R2_OOS: {r2_oos_this_year:+.4f}\")\n",
        "            print(f\"    Avg Epochs Stopped: {epochs_stopped_at[current_train_end_year]:.1f}\")\n",
        "            print(f\"    Time for year: {time.time() - loop_start_time:.2f}s\")\n",
        "\n",
        "        # --- Overall Results ---\n",
        "        print(f\"\\n--- Overall NN3 Results (3 Features, Ensemble={N_ENSEMBLE}) ---\")\n",
        "\n",
        "        # In-Sample Overall\n",
        "        overall_r2_is = None\n",
        "        if all_is_true_values:\n",
        "            overall_r2_is = calculate_r2_is(np.array(all_is_true_values), np.array(all_is_predictions_ensemble_avg))\n",
        "            print(f\"\\nOverall In-Sample R-squared (R2_IS) across all training periods: {overall_r2_is:.4f}\")\n",
        "            print(\"\\nR2_IS per training period (end year):\")\n",
        "            for year, r2 in sorted(train_year_r2_is.items()):\n",
        "                print(f\"  {train_start_year}-{year}: {r2:.4f} (Avg Epochs: {epochs_stopped_at.get(year, 'N/A'):.0f})\")\n",
        "        else:\n",
        "            print(\"\\nNo valid in-sample predictions were generated.\")\n",
        "\n",
        "        # Out-of-Sample Overall\n",
        "        overall_r2_oos = None\n",
        "        if all_oos_true_values:\n",
        "            overall_r2_oos = calculate_r2_oos(np.array(all_oos_true_values), np.array(all_oos_predictions_ensemble_avg))\n",
        "            print(f\"\\nOverall Out-of-Sample R-squared (R2_OOS) for period {min(actual_test_years)}-{max(actual_test_years)}: {overall_r2_oos:.4f}\")\n",
        "            print(f\"(Paper's OLS-3 benchmark R2_OOS: 0.16%)\") # Compare to OLS-3\n",
        "            print(\"\\nR2_OOS per test year:\")\n",
        "            for year, r2 in sorted(test_year_r2_oos.items()):\n",
        "                 print(f\"  {year}: {r2:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo valid out-of-sample predictions were generated for the test period.\")\n",
        "\n",
        "        # Store results\n",
        "        nn3_results_3f = {\n",
        "            \"overall_r2_is\": overall_r2_is,\n",
        "            \"overall_r2_oos\": overall_r2_oos,\n",
        "            \"yearly_r2_is\": train_year_r2_is,\n",
        "            \"yearly_r2_oos\": test_year_r2_oos,\n",
        "            \"avg_epochs_stopped_per_training_year\": epochs_stopped_at,\n",
        "            \"features_used\": feature_cols,\n",
        "            \"ensemble_size\": N_ENSEMBLE\n",
        "        }\n",
        "\n",
        "elif data_processed is None or data_processed.empty:\n",
        "    print(\"\\nData preprocessing failed or resulted in empty DataFrame. Cannot proceed.\")\n",
        "    nn3_results_3f = None\n",
        "else: # Preprocessing succeeded, but the required 3 columns were missing\n",
        "    missing_req_cols = [col for col in specific_feature_cols if col not in data_processed.columns]\n",
        "    print(f\"\\nERROR: The required feature columns {missing_req_cols} were not found in the preprocessed data. Cannot proceed.\")\n",
        "    nn3_results_3f = None\n",
        "\n",
        "\n",
        "# Final summary\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal execution time for NN3 (3 Features, Ensemble) cell: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "if 'nn3_results_3f' in locals() and nn3_results_3f is not None:\n",
        "    print(\"\\nNN3 (3 Features, Ensemble) model fitting complete. Results stored in 'nn3_results_3f' dictionary.\")\n",
        "    print(f\"Features used: {nn3_results_3f.get('features_used', [])}\")\n",
        "    print(f\"Ensemble size: {nn3_results_3f.get('ensemble_size', 'N/A')}\")\n",
        "elif 'nn3_results_3f' not in locals() or nn3_results_3f is None:\n",
        "     print(\"\\nNN3 (3 Features, Ensemble) model fitting did not complete successfully.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYvPlCEar0p2",
        "outputId": "06c6b4cf-9af2-45e0-a112-aad9587f4b10"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting NN3 Model Fitting (3 Features: ['size_rnk', 'BPR', 'mom12'], Ensemble=5) ---\n",
            "--- Starting Data Preprocessing ---\n",
            "Input shape: (336740, 138)\n",
            "Converting 'date' to datetime...\n",
            "Shape after date processing: (336740, 137)\n",
            "Date index set.\n",
            "Identified 135 potential feature columns.\n",
            "Shape after selecting columns: (336740, 137)\n",
            "Attempting to convert identified feature columns to numeric...\n",
            "Confirmed 135 numeric feature columns.\n",
            "Coerced a total of 123 non-numeric entries to NaN across features.\n",
            "Handled infinite values.\n",
            "Found 2196 missing values in 'target'. Dropping rows...\n",
            "Dropped 2196 rows due to missing 'target'.\n",
            "Shape after handling target NaNs: (334544, 137)\n",
            "Applying Median+0 imputation to numeric features...\n",
            "Imputation complete. Imputed ~1010996 values with median, 0 values with 0.\n",
            "Preprocessing complete. Final shape: (334544, 137)\n",
            "Total 135 numeric features processed.\n",
            "Final data date range: 2002-05-01 to 2021-08-01\n",
            "\n",
            "Proceeding with 3 specific features: ['size_rnk', 'BPR', 'mom12']\n",
            "\n",
            "Starting NN3 (3 Features) annual refitting from 2019 to 2020...\n",
            "Using Training Start Year: 2002\n",
            "\n",
            "  Processing test year: 2019\n",
            "    Training period: 2002-2018\n",
            "    Starting ensemble training (N=5)...\n",
            "      Training model 1/5...\n",
            "        Model 1 stopped early at epoch 30\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 2/5...\n",
            "        Model 2 stopped early at epoch 45\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 3/5...\n",
            "        Model 3 stopped early at epoch 12\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "      Training model 4/5...\n",
            "        Model 4 stopped early at epoch 22\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 5/5...\n",
            "        Model 5 stopped early at epoch 34\n",
            "\u001b[1m529/529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "    Train Year 2018 Avg R2_IS : +0.0059\n",
            "    Test Year  2019 Avg R2_OOS: -0.0051\n",
            "    Avg Epochs Stopped: 28.6\n",
            "    Time for year: 352.88s\n",
            "\n",
            "  Processing test year: 2020\n",
            "    Training period: 2002-2019\n",
            "    Starting ensemble training (N=5)...\n",
            "      Training model 1/5...\n",
            "        Model 1 stopped early at epoch 19\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 2/5...\n",
            "        Model 2 stopped early at epoch 20\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 3/5...\n",
            "        Model 3 stopped early at epoch 17\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 4/5...\n",
            "        Model 4 stopped early at epoch 26\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "      Training model 5/5...\n",
            "        Model 5 stopped early at epoch 19\n",
            "\u001b[1m574/574\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "    Train Year 2019 Avg R2_IS : +0.0051\n",
            "    Test Year  2020 Avg R2_OOS: +0.0091\n",
            "    Avg Epochs Stopped: 20.2\n",
            "    Time for year: 288.96s\n",
            "\n",
            "--- Overall NN3 Results (3 Features, Ensemble=5) ---\n",
            "\n",
            "Overall In-Sample R-squared (R2_IS) across all training periods: 0.0055\n",
            "\n",
            "R2_IS per training period (end year):\n",
            "  2002-2018: 0.0059 (Avg Epochs: 29)\n",
            "  2002-2019: 0.0051 (Avg Epochs: 20)\n",
            "\n",
            "Overall Out-of-Sample R-squared (R2_OOS) for period 2019-2020: 0.0052\n",
            "(Paper's OLS-3 benchmark R2_OOS: 0.16%)\n",
            "\n",
            "R2_OOS per test year:\n",
            "  2019: -0.0051\n",
            "  2020: 0.0091\n",
            "\n",
            "Total execution time for NN3 (3 Features, Ensemble) cell: 663.47 seconds\n",
            "\n",
            "NN3 (3 Features, Ensemble) model fitting complete. Results stored in 'nn3_results_3f' dictionary.\n",
            "Features used: ['size_rnk', 'BPR', 'mom12']\n",
            "Ensemble size: 5\n"
          ]
        }
      ]
    }
  ]
}