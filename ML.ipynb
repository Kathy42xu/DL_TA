{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5ZlcpaPQP+bz1WmTrmcYO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kathy42xu/DL_TA/blob/main/ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os # Needed to work with file paths and check existence\n",
        "import time # Optional: to time execution\n",
        "\n",
        "# --- Configuration ---\n",
        "# Path to the folder you uploaded containing the factor_x.csv files\n",
        "data_folder_path = './' # Assumes 'data' folder is in the root\n",
        "\n",
        "# Number of files\n",
        "num_files = 10\n",
        "file_prefix = 'factor_'\n",
        "file_suffix = '.csv'\n",
        "\n",
        "# List to hold individual DataFrames\n",
        "all_data_parts = []\n",
        "column_names = None # To store column names from factor_1.csv\n",
        "\n",
        "print(f\"--- Starting Data Combination ---\")\n",
        "start_load_time = time.time()\n",
        "print(f\"Attempting to load files from {data_folder_path}...\")\n",
        "print(\"Expecting header ONLY in factor_1.csv\")\n",
        "\n",
        "# Check if the data folder exists\n",
        "if not os.path.isdir(data_folder_path):\n",
        "    print(f\"ERROR: Directory not found: {data_folder_path}\")\n",
        "    print(\"Please ensure you have uploaded the 'data' folder containing the CSV files.\")\n",
        "    # Handle error appropriately, maybe raise exception or exit\n",
        "    combined_data = None # Set combined_data to None or handle error\n",
        "else:\n",
        "    # --- Load the first file (factor_1.csv) WITH header ---\n",
        "    file_name_1 = f\"{file_prefix}1{file_suffix}\"\n",
        "    file_path_1 = os.path.join(data_folder_path, file_name_1)\n",
        "\n",
        "    if os.path.exists(file_path_1):\n",
        "        print(f\"  Loading {file_path_1} (with header)...\")\n",
        "        try:\n",
        "            df_first = pd.read_csv(file_path_1)\n",
        "            column_names = df_first.columns.tolist() # Store column names\n",
        "            all_data_parts.append(df_first)\n",
        "            print(f\"    Successfully loaded {file_name_1}. Shape: {df_first.shape}\")\n",
        "            if column_names:\n",
        "                 print(f\"    Stored {len(column_names)} column names.\")\n",
        "            else:\n",
        "                 print(\"    Warning: No column names found in header file.\")\n",
        "                 column_names = None # Treat as failure if no columns read\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR loading header file {file_name_1}: {e}\")\n",
        "            column_names = None # Ensure it's None if loading failed\n",
        "    else:\n",
        "        print(f\"  ERROR: Header file not found - {file_path_1}. Cannot proceed.\")\n",
        "        column_names = None # Ensure it's None if file not found\n",
        "\n",
        "    # --- Load subsequent files (factor_2.csv to factor_10.csv) WITHOUT header ---\n",
        "    if column_names is not None: # Proceed only if factor_1 was loaded successfully\n",
        "        for i in range(2, num_files + 1):\n",
        "            file_name = f\"{file_prefix}{i}{file_suffix}\"\n",
        "            file_path = os.path.join(data_folder_path, file_name)\n",
        "\n",
        "            if os.path.exists(file_path):\n",
        "                print(f\"  Loading {file_path} (WITHOUT header)...\")\n",
        "                try:\n",
        "                    # Load WITHOUT header\n",
        "                    df_temp = pd.read_csv(file_path, header=None)\n",
        "\n",
        "                    # Important Check: Verify column count matches factor_1\n",
        "                    if len(df_temp.columns) != len(column_names):\n",
        "                         print(f\"    WARNING: {file_name} has {len(df_temp.columns)} columns, but header has {len(column_names)}. Skipping this file.\")\n",
        "                         continue # Skip this file\n",
        "\n",
        "                    # Assign correct column names HERE\n",
        "                    df_temp.columns = column_names\n",
        "\n",
        "                    # Add the DataFrame (now with correct names) to the list\n",
        "                    all_data_parts.append(df_temp)\n",
        "                    # print(f\"    Successfully loaded {file_name}. Shape: {df_temp.shape}\") # Reduced verbosity\n",
        "                except Exception as e:\n",
        "                    print(f\"    ERROR loading {file_name}: {e}\")\n",
        "                    # continue # Optional: skip failing files\n",
        "            else:\n",
        "                print(f\"  WARNING: File not found - {file_path}. Skipping.\")\n",
        "\n",
        "    # --- Concatenate ---\n",
        "    combined_data = None # Initialize in case of errors\n",
        "    if not all_data_parts:\n",
        "        print(\"\\nERROR: No data parts were loaded. Cannot concatenate.\")\n",
        "    elif column_names is None:\n",
        "         print(\"\\nERROR: Could not load header from factor_1.csv. Cannot proceed.\")\n",
        "    else:\n",
        "        print(\"\\nConcatenating all loaded data parts...\")\n",
        "        # Concatenate DataFrames. All parts should now have correct column names.\n",
        "        combined_data = pd.concat(all_data_parts, ignore_index=True)\n",
        "        print(\"Concatenation complete!\")\n",
        "\n",
        "        # Final check on columns (should pass now)\n",
        "        if len(combined_data.columns) != len(column_names):\n",
        "             print(\"ERROR: Final column count mismatch after assigning names. Check logic.\")\n",
        "             combined_data = None # Invalidate data if something went wrong\n",
        "        else:\n",
        "             # Display the shape and head/tail of the final combined DataFrame\n",
        "             print(\"\\nShape of the final combined DataFrame:\")\n",
        "             print(combined_data.shape)\n",
        "             print(\"\\nFirst 5 rows:\")\n",
        "             print(combined_data.head())\n",
        "             print(\"\\nLast 5 rows:\")\n",
        "             print(combined_data.tail())\n",
        "\n",
        "# --- End of Data Combination Cell ---\n",
        "load_end_time = time.time()\n",
        "print(f\"\\n--- Data Combination Finished ---\")\n",
        "if combined_data is not None:\n",
        "    print(f\"Variable 'combined_data' created. Shape: {combined_data.shape}\")\n",
        "    print(f\"Time taken: {load_end_time - start_load_time:.2f} seconds\")\n",
        "else:\n",
        "    print(\"Variable 'combined_data' could not be created due to errors.\")\n",
        "\n",
        "# --- Next Steps ---\n",
        "# If 'combined_data' was created successfully, you can now use it\n",
        "# in the next cell for preprocessing and model fitting.\n",
        "# Example for next cell:\n",
        "# if combined_data is not None:\n",
        "#     data = preprocess_combined_data(combined_data.copy(), ...) # Use .copy() if preprocess modifies inplace\n",
        "#     # ... rest of the OLS code ...\n",
        "# else:\n",
        "#     print(\"Cannot proceed without combined_data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRxNrtgS7Ra-",
        "outputId": "da606af8-38dc-4fd1-c6b8-750080559ee0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Data Combination ---\n",
            "Attempting to load files from ./...\n",
            "Expecting header ONLY in factor_1.csv\n",
            "  Loading ./factor_1.csv (with header)...\n",
            "    Successfully loaded factor_1.csv. Shape: (33700, 138)\n",
            "    Stored 138 column names.\n",
            "  Loading ./factor_2.csv (WITHOUT header)...\n",
            "  Loading ./factor_3.csv (WITHOUT header)...\n",
            "  Loading ./factor_4.csv (WITHOUT header)...\n",
            "  Loading ./factor_5.csv (WITHOUT header)...\n",
            "  Loading ./factor_6.csv (WITHOUT header)...\n",
            "  Loading ./factor_7.csv (WITHOUT header)...\n",
            "  Loading ./factor_8.csv (WITHOUT header)...\n",
            "  Loading ./factor_9.csv (WITHOUT header)...\n",
            "  Loading ./factor_10.csv (WITHOUT header)...\n",
            "\n",
            "Concatenating all loaded data parts...\n",
            "Concatenation complete!\n",
            "\n",
            "Shape of the final combined DataFrame:\n",
            "(336740, 138)\n",
            "\n",
            "First 5 rows:\n",
            "         date  ticker    target  market_return  excess_market_return  \\\n",
            "0  2002-05-01   47080 -0.277193      -0.000009             -0.004017   \n",
            "1  2002-05-01   17040 -0.384848      -0.000009             -0.004017   \n",
            "2  2002-05-01   31510 -0.039185      -0.000009             -0.004017   \n",
            "3  2002-05-01   65060 -0.106349      -0.000009             -0.004017   \n",
            "4  2002-05-01    2310  0.039370      -0.000009             -0.004017   \n",
            "\n",
            "   ff3_bin_return  smb  hml const  beta  ...  hml_bin_prec_9.0  \\\n",
            "0             NaN  NaN  NaN   NaN   NaN  ...               0.0   \n",
            "1             NaN  NaN  NaN   NaN   NaN  ...               0.0   \n",
            "2             NaN  NaN  NaN   NaN   NaN  ...               0.0   \n",
            "3             NaN  NaN  NaN   NaN   NaN  ...               0.0   \n",
            "4             NaN  NaN  NaN   NaN   NaN  ...               1.0   \n",
            "\n",
            "   hml_bin_prec_10.0 vix_cat_mid vix_cat_high  div_ret  tb3y  tb5y  tb10y  \\\n",
            "0                0.0         0.0          1.0      0.0   6.3   6.8   7.09   \n",
            "1                0.0         0.0          1.0      0.0   6.3   6.8   7.09   \n",
            "2                0.0         0.0          1.0      0.0   6.3   6.8   7.09   \n",
            "3                0.0         0.0          1.0      0.0   6.3   6.8   7.09   \n",
            "4                0.0         0.0          1.0     5.27   6.3   6.8   7.09   \n",
            "\n",
            "   cb3y    div  \n",
            "0  7.07    0.0  \n",
            "1  7.07    0.0  \n",
            "2  7.07    0.0  \n",
            "3  7.07    0.0  \n",
            "4  7.07  400.0  \n",
            "\n",
            "[5 rows x 138 columns]\n",
            "\n",
            "Last 5 rows:\n",
            "              date  ticker  target  market_return  excess_market_return  \\\n",
            "336735  2021-09-01   88130     NaN      -0.022537             -0.023354   \n",
            "336736  2021-09-01   32540     NaN      -0.022537             -0.023354   \n",
            "336737  2021-09-01   28260     NaN      -0.022537             -0.023354   \n",
            "336738  2021-09-01  170900     NaN      -0.022537             -0.023354   \n",
            "336739  2021-09-01   98120     NaN      -0.022537             -0.023354   \n",
            "\n",
            "        ff3_bin_return       smb       hml const  beta  ...  hml_bin_prec_9.0  \\\n",
            "336735       -0.015641 -0.045283  0.021204   NaN   NaN  ...               1.0   \n",
            "336736       -0.015641 -0.045283  0.021204   NaN   NaN  ...               0.0   \n",
            "336737        0.001817 -0.045283  0.021204   NaN   NaN  ...               1.0   \n",
            "336738        0.001817 -0.045283  0.021204   NaN   NaN  ...               0.0   \n",
            "336739       -0.038560 -0.045283  0.021204   NaN   NaN  ...               0.0   \n",
            "\n",
            "        hml_bin_prec_10.0 vix_cat_mid vix_cat_high  div_ret  tb3y  tb5y  \\\n",
            "336735                0.0         0.0          1.0     1.22  1.52  1.79   \n",
            "336736                0.0         0.0          1.0      0.0  1.52  1.79   \n",
            "336737                0.0         0.0          1.0     1.73  1.52  1.79   \n",
            "336738                0.0         0.0          1.0     1.29  1.52  1.79   \n",
            "336739                0.0         0.0          1.0     0.28  1.52  1.79   \n",
            "\n",
            "        tb10y  cb3y     div  \n",
            "336735   2.06  1.95   150.0  \n",
            "336736   2.06  1.95     0.0  \n",
            "336737   2.06  1.95  2300.0  \n",
            "336738   2.06  1.95  1000.0  \n",
            "336739   2.06  1.95    25.0  \n",
            "\n",
            "[5 rows x 138 columns]\n",
            "\n",
            "--- Data Combination Finished ---\n",
            "Variable 'combined_data' created. Shape: (336740, 138)\n",
            "Time taken: 11.35 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries (needed again in a new cell)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "import time\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Configuration ---\n",
        "# (Keep the same configuration as before: data_folder_path, feature_cols, target_col, date ranges etc.)\n",
        "data_folder_path = './' # Example: Adjust if your data is in a subfolder like './data/'\n",
        "num_files = 10\n",
        "file_prefix = 'factor_'\n",
        "file_suffix = '.csv'\n",
        "\n",
        "feature_cols = [\n",
        "    'size_rnk', # Assuming this is the size feature\n",
        "    'BPR',      # Assuming this is Book-to-Price\n",
        "    'mom12'     # Assuming this is 12-month momentum\n",
        "]\n",
        "target_col = 'target'\n",
        "date_col = 'date'\n",
        "ticker_col = 'ticker'\n",
        "\n",
        "# Define date ranges based on your last code block (adjust if needed)\n",
        "train_start_year = 2002\n",
        "train_end_year = 2015\n",
        "test_start_year = 2019\n",
        "test_end_year = 2020\n",
        "\n",
        "\n",
        "# --- Functions ---\n",
        "# (load_and_combine_data function would typically be in the previous cell)\n",
        "\n",
        "# MODIFIED PREPROCESSING FUNCTION - ADDED pd.to_numeric\n",
        "def preprocess_combined_data(df, date_col, ticker_col, target_col, feature_cols):\n",
        "    \"\"\"Preprocesses the combined dataframe with numeric conversion and median+0 imputation for features.\"\"\"\n",
        "    print(\"--- Starting Data Preprocessing ---\")\n",
        "    if df is None:\n",
        "        print(\"ERROR: Input DataFrame is None.\")\n",
        "        return None\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        print(\"ERROR: Input is not a pandas DataFrame.\")\n",
        "        return None\n",
        "\n",
        "    df_processed = df.copy()\n",
        "    print(f\"Input shape: {df_processed.shape}\")\n",
        "\n",
        "    # Convert date column if it's not the index already\n",
        "    if date_col in df_processed.columns:\n",
        "        print(f\"Converting '{date_col}' to datetime...\")\n",
        "        df_processed[date_col] = pd.to_datetime(df_processed[date_col], errors='coerce')\n",
        "        df_processed = df_processed.dropna(subset=[date_col])\n",
        "        df_processed = df_processed.set_index(date_col)\n",
        "    elif not isinstance(df_processed.index, pd.DatetimeIndex):\n",
        "        print(f\"ERROR: DataFrame index is not a DatetimeIndex and '{date_col}' column not found.\")\n",
        "        return None\n",
        "    print(\"Date index set.\")\n",
        "\n",
        "    # Check for required columns AFTER potential date conversion drops\n",
        "    all_cols_needed = [ticker_col, target_col] + feature_cols\n",
        "    missing_cols = [col for col in all_cols_needed if col not in df_processed.columns]\n",
        "    if missing_cols:\n",
        "        print(f\"ERROR: Missing required columns: {missing_cols}\")\n",
        "        return None\n",
        "\n",
        "    # Select necessary columns AFTER setting index\n",
        "    df_processed = df_processed[[ticker_col, target_col] + feature_cols]\n",
        "\n",
        "    # --- Force Feature Columns to Numeric (Convert non-numeric to NaN) ---\n",
        "    print(\"Attempting to convert feature columns to numeric...\")\n",
        "    coerced_count = 0\n",
        "    for col in feature_cols:\n",
        "         if col in df_processed.columns:\n",
        "              initial_non_numeric = pd.to_numeric(df_processed[col], errors='coerce').isna().sum() - df_processed[col].isna().sum()\n",
        "              if initial_non_numeric > 0:\n",
        "                   print(f\"  Found {initial_non_numeric} non-numeric value(s) in '{col}'. Coercing to NaN.\")\n",
        "                   coerced_count += initial_non_numeric\n",
        "              df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "         else:\n",
        "              print(f\"Warning: Feature column {col} not found during numeric conversion.\")\n",
        "    if coerced_count > 0:\n",
        "         print(f\"Coerced a total of {coerced_count} non-numeric entries to NaN across features.\")\n",
        "\n",
        "    # Handle infinite values (do this *after* to_numeric)\n",
        "    df_processed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # --- Handle TARGET Variable Missing Values ---\n",
        "    initial_rows = len(df_processed)\n",
        "    n_target_na_initial = df_processed[target_col].isna().sum()\n",
        "    if n_target_na_initial > 0:\n",
        "         print(f\"Found {n_target_na_initial} missing values in '{target_col}'. Dropping rows...\")\n",
        "    df_processed.dropna(subset=[target_col], inplace=True)\n",
        "    rows_after_target_drop = len(df_processed)\n",
        "    rows_dropped = initial_rows - rows_after_target_drop\n",
        "    if rows_dropped > 0:\n",
        "         print(f\"Dropped {rows_dropped} rows due to missing '{target_col}'.\")\n",
        "    print(f\"Shape after handling target NaNs: {df_processed.shape}\")\n",
        "    if rows_after_target_drop == 0:\n",
        "         print(\"ERROR: All rows dropped due to missing target. Cannot proceed.\")\n",
        "         return None\n",
        "\n",
        "    # --- Handle FEATURE Missing Values (Median then 0 Imputation) ---\n",
        "    # Now includes NaNs created by coercing strings\n",
        "    print(\"Applying Median+0 imputation to features (including coerced NaNs)...\")\n",
        "    df_processed['month_period'] = df_processed.index.to_period('M')\n",
        "    imputed_count_zero = 0\n",
        "    for col in feature_cols:\n",
        "        if col in df_processed.columns:\n",
        "            n_initial_na = df_processed[col].isna().sum()\n",
        "            if n_initial_na > 0:\n",
        "                 # print(f\"  Processing feature '{col}': Found {n_initial_na} NaNs.\")\n",
        "                 # Use transform for median imputation\n",
        "                 df_processed[col] = df_processed.groupby('month_period')[col].transform(lambda x: x.fillna(x.median()))\n",
        "                 n_remaining_na = df_processed[col].isna().sum()\n",
        "                 if n_remaining_na > 0:\n",
        "                      df_processed[col].fillna(0, inplace=True)\n",
        "                      # print(f\"    Filled {n_remaining_na} remaining NAs with 0 in '{col}'.\")\n",
        "                      imputed_count_zero += n_remaining_na\n",
        "        else:\n",
        "             print(f\"Warning: Feature column {col} not found during imputation.\")\n",
        "    df_processed.drop(columns=['month_period'], inplace=True)\n",
        "    print(f\"Imputation complete. Filled {imputed_count_zero} values with 0 after median imputation.\")\n",
        "\n",
        "    # Final check for NaNs\n",
        "    nans_in_features = df_processed[feature_cols].isna().sum().sum()\n",
        "    if nans_in_features > 0:\n",
        "         print(f\"WARNING: {nans_in_features} NaNs still remain in feature columns after imputation!\")\n",
        "\n",
        "    print(f\"Preprocessing complete. Final shape: {df_processed.shape}\")\n",
        "    if not df_processed.empty:\n",
        "        print(f\"Final data date range: {df_processed.index.min().strftime('%Y-%m-%d')} to {df_processed.index.max().strftime('%Y-%m-%d')}\")\n",
        "    return df_processed\n",
        "\n",
        "def calculate_r2_oos(y_true, y_pred):\n",
        "    \"\"\"Calculates Out-of-Sample R-squared\"\"\"\n",
        "    numerator = ((y_true - y_pred) ** 2).sum()\n",
        "    denominator = (y_true ** 2).sum()\n",
        "    if denominator < 1e-10: return np.nan\n",
        "    return 1 - (numerator / denominator)\n",
        "\n",
        "# --- Main Execution ---\n",
        "print(\"\\n--- Starting OLS-3+H Model Fitting ---\")\n",
        "start_time = time.time()\n",
        "\n",
        "# ** ASSUMPTION: 'combined_data' DataFrame exists from the previous cell **\n",
        "if 'combined_data' not in locals() and 'combined_data' not in globals():\n",
        "     print(\"ERROR: 'combined_data' DataFrame not found. Please run the data combination cell first.\")\n",
        "     data = None\n",
        "else:\n",
        "    # 1. Preprocess the existing combined data (NOW includes numeric conversion)\n",
        "    data = preprocess_combined_data(combined_data, date_col, ticker_col, target_col, feature_cols)\n",
        "\n",
        "# 2. Proceed with model fitting if preprocessing was successful\n",
        "if data is not None and not data.empty:\n",
        "    all_predictions = []\n",
        "    all_true_values = []\n",
        "    test_year_r2 = {}\n",
        "\n",
        "    available_years = sorted(data.index.year.unique())\n",
        "    actual_test_years = [y for y in available_years if y >= test_start_year and y <= test_end_year]\n",
        "\n",
        "    if not actual_test_years:\n",
        "        print(f\"\\nERROR: No data available in the specified test period ({test_start_year}-{test_end_year}) after preprocessing. Cannot proceed.\")\n",
        "    else:\n",
        "        print(f\"\\nStarting OLS-3+H annual refitting from {min(actual_test_years)} to {max(actual_test_years)}...\")\n",
        "        print(f\"Using Training Start Year: {train_start_year}\")\n",
        "\n",
        "        for current_test_year in actual_test_years:\n",
        "            loop_start_time = time.time()\n",
        "            print(f\"  Processing test year: {current_test_year}\")\n",
        "\n",
        "            current_train_end_year = current_test_year - 1\n",
        "            if current_train_end_year < train_start_year:\n",
        "                 print(f\"    Skipping year {current_test_year}: Training end year ({current_train_end_year}) is before training start year ({train_start_year}).\")\n",
        "                 continue\n",
        "\n",
        "            train_mask = (data.index.year >= train_start_year) & (data.index.year <= current_train_end_year)\n",
        "            test_mask = (data.index.year == current_test_year)\n",
        "            train_df = data.loc[train_mask]\n",
        "            test_df = data.loc[test_mask]\n",
        "\n",
        "            if train_df.empty or test_df.empty:\n",
        "                print(f\"    Skipping year {current_test_year}: Not enough data for train ({len(train_df)}) / test ({len(test_df)}) split.\")\n",
        "                continue\n",
        "\n",
        "            # Final check for NaNs before scaling (should be handled by preprocessing)\n",
        "            if train_df[feature_cols].isna().any().any() or test_df[feature_cols].isna().any().any():\n",
        "                 print(f\"    ERROR: NaNs detected in features for year {current_test_year} before scaling, after preprocessing! Skipping.\")\n",
        "                 continue\n",
        "\n",
        "            X_train = train_df[feature_cols].values\n",
        "            y_train = train_df[target_col].values\n",
        "            X_test = test_df[feature_cols].values\n",
        "            y_test = test_df[target_col].values\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            try:\n",
        "                X_train_scaled = scaler.fit_transform(X_train)\n",
        "                X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "                 # Check for NaNs again AFTER scaling (can happen with constant cols)\n",
        "                if np.isnan(X_train_scaled).any() or np.isnan(X_test_scaled).any():\n",
        "                    print(f\"    Warning: NaNs generated during scaling for year {current_test_year}. Check for constant columns in training data.\")\n",
        "                    # Impute NaNs caused by scaling constant columns if necessary\n",
        "                    # X_train_scaled = np.nan_to_num(X_train_scaled)\n",
        "                    # X_test_scaled = np.nan_to_num(X_test_scaled)\n",
        "                    # Or skip the year if this is problematic\n",
        "                    # continue\n",
        "\n",
        "            except ValueError as e:\n",
        "                print(f\"    ERROR during scaling for year {current_test_year}: {e}. Skipping year.\")\n",
        "                continue\n",
        "\n",
        "            huber_model = HuberRegressor(fit_intercept=True, max_iter=300, tol=1e-4)\n",
        "            try:\n",
        "                # Check for NaNs before fitting model\n",
        "                if np.isnan(X_train_scaled).any() or np.isnan(y_train).any():\n",
        "                     print(f\"    ERROR: NaNs detected before fitting model for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "                if np.isnan(X_test_scaled).any():\n",
        "                     print(f\"    ERROR: NaNs detected in test features before prediction for year {current_test_year}. Skipping.\")\n",
        "                     continue\n",
        "\n",
        "                huber_model.fit(X_train_scaled, y_train)\n",
        "                predictions = huber_model.predict(X_test_scaled)\n",
        "\n",
        "                all_predictions.extend(predictions)\n",
        "                all_true_values.extend(y_test)\n",
        "                r2_this_year = calculate_r2_oos(y_test, predictions)\n",
        "                test_year_r2[current_test_year] = r2_this_year\n",
        "                print(f\"    Year {current_test_year} R2_OOS: {r2_this_year:.4f}. Time: {time.time() - loop_start_time:.2f}s\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"    ERROR during model fitting/prediction for year {current_test_year}: {e}\")\n",
        "\n",
        "\n",
        "        print(\"\\n--- Overall Results ---\")\n",
        "        if all_true_values:\n",
        "            overall_r2_oos = calculate_r2_oos(np.array(all_true_values), np.array(all_predictions))\n",
        "            print(f\"Overall Out-of-Sample R-squared (R2_OOS) for period {min(actual_test_years)}-{max(actual_test_years)}: {overall_r2_oos:.4f}\")\n",
        "            print(f\"(Note: This result is based on your data range and cannot be directly compared to the paper's 0.16%)\")\n",
        "            print(\"\\nR2_OOS per test year:\")\n",
        "            for year, r2 in sorted(test_year_r2.items()):\n",
        "                print(f\"  {year}: {r2:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nNo valid predictions were generated for the test period.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nData preprocessing failed or resulted in empty DataFrame. Cannot proceed with model fitting.\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nTotal execution time for this cell: {end_time - start_time:.2f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCKMvLix7nd6",
        "outputId": "02608644-5ffe-468a-eecc-b5a9e37a950a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting OLS-3+H Model Fitting ---\n",
            "--- Starting Data Preprocessing ---\n",
            "Input shape: (336740, 138)\n",
            "Converting 'date' to datetime...\n",
            "Date index set.\n",
            "Attempting to convert feature columns to numeric...\n",
            "  Found 1 non-numeric value(s) in 'BPR'. Coercing to NaN.\n",
            "Coerced a total of 1 non-numeric entries to NaN across features.\n",
            "Found 2196 missing values in 'target'. Dropping rows...\n",
            "Dropped 2196 rows due to missing 'target'.\n",
            "Shape after handling target NaNs: (334544, 5)\n",
            "Applying Median+0 imputation to features (including coerced NaNs)...\n",
            "Imputation complete. Filled 10078 values with 0 after median imputation.\n",
            "Preprocessing complete. Final shape: (334544, 5)\n",
            "Final data date range: 2002-05-01 to 2021-08-01\n",
            "\n",
            "Starting OLS-3+H annual refitting from 2019 to 2020...\n",
            "Using Training Start Year: 2002\n",
            "  Processing test year: 2019\n",
            "    Year 2019 R2_OOS: -0.0010. Time: 1.41s\n",
            "  Processing test year: 2020\n",
            "    Year 2020 R2_OOS: -0.0011. Time: 2.01s\n",
            "\n",
            "--- Overall Results ---\n",
            "Overall Out-of-Sample R-squared (R2_OOS) for period 2019-2020: -0.0011\n",
            "(Note: This result is based on your data range and cannot be directly compared to the paper's 0.16%)\n",
            "\n",
            "R2_OOS per test year:\n",
            "  2019: -0.0010\n",
            "  2020: -0.0011\n",
            "\n",
            "Total execution time for this cell: 6.20 seconds\n"
          ]
        }
      ]
    }
  ]
}