{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiIJEvmA6L4JTzjZSvLE+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kathy42xu/DL_TA/blob/main/lstm_rf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGT4Z85izHxM",
        "outputId": "7199f267-93d6-49e5-9b0f-a96353bb064f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: \n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=4432b4063a7973ed85aa19501dc435beff1d1b12a62686225ad5d8745b742b1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n",
            "YF.download() has changed argument auto_adjust default to True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "device = tf.test.gpu_device_name()\n",
        "print(\"GPU:\", device)\n",
        "\n",
        "!pip install yfinance ta\n",
        "\n",
        "import yfinance as yf\n",
        "df = yf.download(\"^GSPC\", start=\"2002-08-01\", end=\"2018-06-28\")\n",
        "df.to_csv(\"SP500.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#preprocessing_indicator"
      ],
      "metadata": {
        "id": "Ziu6Gpq503gK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1️⃣ 重新读取 CSV —— 明确告诉 pandas 用第一列作 index\n",
        "df = pd.read_csv(\"SP500.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "# 2️⃣ 确认列名正确，只保留 Open/High/Low/Close/Volume\n",
        "print(df.columns)\n",
        "df = df[['Open','High','Low','Close','Volume']]\n",
        "\n",
        "# 3️⃣ 强制转成浮点数（会把任何非数字变成 NaN）\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "# 4️⃣ 删除因转换失败产生的 NaN 行\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWmMlDzy07aB",
        "outputId": "1f74a8d4-547c-4218-f73d-2cf965b2f23d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Close', 'High', 'Low', 'Open', 'Volume'], dtype='object')\n",
            "                  Open        High         Low       Close        Volume\n",
            "Price                                                                   \n",
            "2002-08-01  911.619995  911.619995  882.479980  884.659973  1.672200e+09\n",
            "2002-08-02  884.400024  884.719971  853.950012  864.239990  1.538100e+09\n",
            "2002-08-05  864.239990  864.239990  833.440002  834.599976  1.425500e+09\n",
            "2002-08-06  834.599976  874.440002  834.599976  859.570007  1.514100e+09\n",
            "2002-08-07  859.570007  878.739990  854.150024  876.770020  1.490400e+09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-dcf6b7282b58>:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df = pd.read_csv(\"SP500.csv\", index_col=0, parse_dates=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import ta\n",
        "\n",
        "# -------------------------------\n",
        "# 1) Read CSV with date parsing\n",
        "# -------------------------------\n",
        "df = pd.read_csv(\"SP500.csv\", index_col=0, parse_dates=True)\n",
        "\n",
        "# -------------------------------\n",
        "# 2) Keep only OHLCV and convert to float\n",
        "# -------------------------------\n",
        "df = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "###############################################################################\n",
        "# 3) Compute 43 Technical Indicators (Computed Features)\n",
        "###############################################################################\n",
        "# We'll compute 43 computed indicators, so that with the 5 raw columns,\n",
        "# the final DataFrame has 43 + 5 = 48 columns.\n",
        "\n",
        "# (1) Standard Deviation over 20 days (SD_20)\n",
        "df['SD_20'] = df['Close'].rolling(20).std()\n",
        "\n",
        "# (2-4) Simple Moving Averages (SMA)\n",
        "df['SMA_5']  = ta.trend.sma_indicator(df['Close'], window=5)\n",
        "df['SMA_10'] = ta.trend.sma_indicator(df['Close'], window=10)\n",
        "df['SMA_20'] = ta.trend.sma_indicator(df['Close'], window=20)\n",
        "\n",
        "# (5-7) Exponential Moving Averages (EMA)\n",
        "df['EMA_6']  = ta.trend.ema_indicator(df['Close'], window=6)\n",
        "df['EMA_10'] = ta.trend.ema_indicator(df['Close'], window=10)\n",
        "df['EMA_14'] = ta.trend.ema_indicator(df['Close'], window=14)\n",
        "\n",
        "# (8) MACD with fast=6, slow=12\n",
        "df['MACD_6_12'] = ta.trend.macd_diff(df['Close'], window_slow=12, window_fast=6)\n",
        "\n",
        "# (9-10) Relative Strength Index (RSI)\n",
        "df['RSI_10'] = ta.momentum.rsi(df['Close'], window=10)\n",
        "df['RSI_14'] = ta.momentum.rsi(df['Close'], window=14)\n",
        "\n",
        "# (11) Commodity Channel Index (CCI)\n",
        "df['CCI_20'] = ta.trend.cci(df['High'], df['Low'], df['Close'], window=20)\n",
        "\n",
        "# (12-13) Bollinger Bands (Upper and Lower)\n",
        "df['BOLL_UB'] = ta.volatility.bollinger_hband(df['Close'], window=20)\n",
        "df['BOLL_LB'] = ta.volatility.bollinger_lband(df['Close'], window=20)\n",
        "\n",
        "# (14) Average True Range (ATR)\n",
        "df['ATR_14'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'], window=14)\n",
        "\n",
        "# (15-18) True Range Components: H-L, H-Cp, L-Cp, TR\n",
        "df['H-L']  = df['High'] - df['Low']\n",
        "df['H-Cp'] = (df['High'] - df['Close'].shift(1)).abs()\n",
        "df['L-Cp'] = (df['Low']  - df['Close'].shift(1)).abs()\n",
        "df['TR']   = df[['H-L', 'H-Cp', 'L-Cp']].max(axis=1)\n",
        "\n",
        "# (19) On-Balance Volume (OBV)\n",
        "df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])\n",
        "\n",
        "# (20) Money Flow Index (MFI) using library\n",
        "df['MFI'] = ta.volume.money_flow_index(df['High'], df['Low'], df['Close'], df['Volume'], window=14)\n",
        "\n",
        "# (21) Force Index (1-day)\n",
        "df['ForceIndex'] = ta.volume.force_index(df['Close'], df['Volume'], window=1)\n",
        "# (22) 5-day Force Index (FI_5)\n",
        "df['FI_5'] = ta.volume.force_index(df['Close'], df['Volume'], window=5)\n",
        "\n",
        "# (23-24) Price and Volume Change Percentages (C%, V%)\n",
        "df['C%'] = (df['Close'] - df['Close'].shift(1)) / df['Close'].shift(1)\n",
        "df['V%'] = (df['Volume'] - df['Volume'].shift(1)) / df['Volume'].shift(1)\n",
        "\n",
        "# (25) Negative Volume Index (NVI)\n",
        "df['NVI'] = ta.volume.negative_volume_index(df['Close'], df['Volume'])\n",
        "\n",
        "# (26) Ease of Movement (SEMV)\n",
        "df['SEMV'] = ta.volume.ease_of_movement(df['High'], df['Low'], df['Volume'])\n",
        "\n",
        "# (27) True Strength Index (TSI) – custom calculation\n",
        "def tsi(series, long=25, short=13):\n",
        "    diff = series.diff()\n",
        "    abs_diff = diff.abs()\n",
        "    ema1 = diff.ewm(span=long, adjust=False).mean()\n",
        "    ema2 = ema1.ewm(span=short, adjust=False).mean()\n",
        "    abs_ema1 = abs_diff.ewm(span=long, adjust=False).mean()\n",
        "    abs_ema2 = abs_ema1.ewm(span=short, adjust=False).mean()\n",
        "    return 100 * ema2 / abs_ema2\n",
        "\n",
        "df['TSI'] = tsi(df['Close'], long=25, short=13)\n",
        "\n",
        "# (28) Money Flow Ratio (MFR) – computed manually\n",
        "df['PMF'] = np.where(df['Close'] > df['Close'].shift(1), df['Close'] * df['Volume'], 0)\n",
        "df['NMF'] = np.where(df['Close'] < df['Close'].shift(1), df['Close'] * df['Volume'], 0)\n",
        "df['PMF_roll'] = df['PMF'].rolling(14).sum()\n",
        "df['NMF_roll'] = df['NMF'].rolling(14).sum()\n",
        "df['MFR'] = df['PMF_roll'] / df['NMF_roll']\n",
        "\n",
        "# (29) We'll include the library MFI as an additional indicator;\n",
        "#     This gives you two MFI-related features: MFI (from above) and a custom one.\n",
        "df['MFI_custom'] = 100 - (100 / (1 + df['MFR']))\n",
        "\n",
        "# (30-31) Vortex Indicators: Upper (UVI) and Lower (LVI)\n",
        "df['UVI'] = ta.trend.vortex_indicator_pos(df['High'], df['Low'], df['Close'], window=14)\n",
        "df['LVI'] = ta.trend.vortex_indicator_neg(df['High'], df['Low'], df['Close'], window=14)\n",
        "\n",
        "# (32-33) Know Sure Thing (KST) and its 9-day SMA signal (KST_9)\n",
        "df['KST'] = ta.trend.kst(df['Close'])\n",
        "df['KST_9'] = df['KST'].rolling(window=9).mean()\n",
        "\n",
        "# (34) Detrended Price Oscillator (DPO, 20-day)\n",
        "df['DPO_20'] = ta.trend.dpo(df['Close'], window=20)\n",
        "\n",
        "# (35) Directional Index (DX) computed from ADX+ and ADX-\n",
        "adx_pos_14 = ta.trend.adx_pos(df['High'], df['Low'], df['Close'], window=14)\n",
        "adx_neg_14 = ta.trend.adx_neg(df['High'], df['Low'], df['Close'], window=14)\n",
        "df['DX'] = 100 * (adx_pos_14 - adx_neg_14).abs() / (adx_pos_14 + adx_neg_14)\n",
        "\n",
        "# (36-37) Average Directional Index (ADX) for 7-day and 14-day\n",
        "df['ADX_7'] = ta.trend.adx(df['High'], df['Low'], df['Close'], window=7)\n",
        "df['ADX_14'] = ta.trend.adx(df['High'], df['Low'], df['Close'], window=14)\n",
        "\n",
        "# (38) Rate of Change (ROC, 12-day)\n",
        "df['ROC_12'] = ta.momentum.roc(df['Close'], window=12)\n",
        "\n",
        "# (39) Williams %R (with lookback period 14)\n",
        "df['Williams_%R'] = ta.momentum.williams_r(df['High'], df['Low'], df['Close'], lbp=14)\n",
        "\n",
        "# --- Extra 5 to reach 43 computed indicators ---\n",
        "\n",
        "# (40) EMA of Volume (10-day)\n",
        "df['EMA_Volume'] = ta.trend.ema_indicator(df['Volume'], window=10)\n",
        "\n",
        "# (41) Rate of Change (ROC, 5-day)\n",
        "df['ROC_5'] = ta.momentum.roc(df['Close'], window=5)\n",
        "\n",
        "# (42) Bollinger Bandwidth: (UB - LB) / SMA_20\n",
        "df['BOLL_BW'] = (df['BOLL_UB'] - df['BOLL_LB']) / df['SMA_20']\n",
        "\n",
        "# (43) ATR Ratio: ATR_14 divided by SMA_20\n",
        "df['ATR_Ratio'] = df['ATR_14'] / df['SMA_20']\n",
        "\n",
        "# (44) Volatility: Coefficient of Variation = SD_20 / SMA_20\n",
        "df['Vol_CV'] = df['SD_20'] / df['SMA_20']\n",
        "\n",
        "###############################################################################\n",
        "# Now, decide which computed indicators to keep.\n",
        "# We want exactly 43 computed indicators. Currently, after the raw OHLCV,\n",
        "# our DataFrame has extra intermediate columns.\n",
        "#\n",
        "# The current columns list (after computation) is:\n",
        "# ['Open', 'High', 'Low', 'Close', 'Volume', 'SD_20', 'SMA_5', 'SMA_10', 'SMA_20',\n",
        "#  'EMA_6', 'EMA_10', 'EMA_14', 'MACD_6_12', 'RSI_10', 'RSI_14', 'CCI_20',\n",
        "#  'BOLL_UB', 'BOLL_LB', 'ATR_14', 'H-L', 'H-Cp', 'L-Cp', 'TR', 'OBV', 'MFI',\n",
        "#  'ForceIndex', 'FI_5', 'C%', 'V%', 'NVI', 'SEMV', 'TSI', 'PMF', 'NMF', 'PMF_roll',\n",
        "#  'NMF_roll', 'MFR', 'MFI_custom', 'UVI', 'LVI', 'KST', 'KST_9', 'DPO_20', 'DX',\n",
        "#  'ADX_7', 'ADX_14', 'ROC_12', 'Williams_%R', 'EMA_Volume', 'ROC_5', 'BOLL_BW',\n",
        "#  'ATR_Ratio', 'Vol_CV']\n",
        "#\n",
        "# We need to select 43 computed features from these.\n",
        "#\n",
        "# One common choice (based on the paper's indicator list) is to keep:\n",
        "#    SMA_5, SMA_10, SMA_20,\n",
        "#    EMA_6, EMA_10, EMA_14,\n",
        "#    MACD_6_12,\n",
        "#    RSI_10, RSI_14,\n",
        "#    CCI_20,\n",
        "#    BOLL_UB, BOLL_LB,\n",
        "#    ATR_14,\n",
        "#    H-L, H-Cp, L-Cp, TR,\n",
        "#    OBV,\n",
        "#    MFI,            # (library version)\n",
        "#    ForceIndex, FI_5,\n",
        "#    C%, V%,\n",
        "#    NVI,\n",
        "#    SEMV,\n",
        "#    TSI,\n",
        "#    MFR,            # Money Flow Ratio\n",
        "#    UVI, LVI,\n",
        "#    KST, KST_9,\n",
        "#    DPO_20,\n",
        "#    DX,\n",
        "#    ADX_7, ADX_14,\n",
        "#    ROC_12,\n",
        "#    Williams_%R,\n",
        "#    EMA_Volume,     # extra\n",
        "#    ROC_5,          # extra\n",
        "#    BOLL_BW,        # extra\n",
        "#    ATR_Ratio,      # extra\n",
        "#    Vol_CV          # extra\n",
        "#\n",
        "# That is a total of 43 computed indicators.\n",
        "###############################################################################\n",
        "\n",
        "computed_features = ['SD_20','SMA_5', 'SMA_10', 'SMA_20',\n",
        "                     'EMA_6', 'EMA_10', 'EMA_14',\n",
        "                     'MACD_6_12',\n",
        "                     'RSI_10', 'RSI_14',\n",
        "                     'CCI_20',\n",
        "                     'BOLL_UB', 'BOLL_LB',\n",
        "                     'ATR_14',\n",
        "                     'H-L', 'H-Cp', 'L-Cp', 'TR',\n",
        "                     'OBV',\n",
        "                     'MFI',\n",
        "                     'ForceIndex', 'FI_5',\n",
        "                     'C%', 'V%',\n",
        "                     'NVI',\n",
        "                     'SEMV',\n",
        "                     'TSI',\n",
        "                     'MFR',\n",
        "                     'UVI', 'LVI',\n",
        "                     'KST', 'KST_9',\n",
        "                     'DPO_20',\n",
        "                     'DX',\n",
        "                     'ADX_7', 'ADX_14',\n",
        "                     'ROC_12',\n",
        "                     'Williams_%R',\n",
        "                     'EMA_Volume',\n",
        "                     'ROC_5',\n",
        "                     'BOLL_BW',\n",
        "                     'ATR_Ratio',\n",
        "                     'Vol_CV']\n",
        "\n",
        "# Now, the final DataFrame should contain the 5 raw columns + these 43 computed features.\n",
        "final_cols = ['Open', 'High', 'Low', 'Close', 'Volume'] + computed_features\n",
        "df_final = df[final_cols].copy()\n",
        "\n",
        "###############################################################################\n",
        "# 4) Normalization\n",
        "###############################################################################\n",
        "scaler = MinMaxScaler()\n",
        "df_final[final_cols] = scaler.fit_transform(df_final[final_cols])\n",
        "\n",
        "###############################################################################\n",
        "# 5) Construct 50-day supervised sequences\n",
        "###############################################################################\n",
        "window = 50\n",
        "# Compute next-day log returns (×100)\n",
        "returns = 100 * np.log(df_final['Close'] / df_final['Close'].shift(1))\n",
        "returns = returns.dropna()  # drop the first NaN row\n",
        "\n",
        "X, y_reg, y_clf = [], [], []\n",
        "for i in range(len(returns) - window):\n",
        "    seq = df_final.iloc[i : i+window].values  # shape: (50, 48)\n",
        "    X.append(seq)\n",
        "    ret_val = returns.iloc[i+window]\n",
        "    y_reg.append(ret_val)\n",
        "    y_clf.append(int(ret_val > 0))\n",
        "\n",
        "X = np.array(X)\n",
        "y_reg = np.array(y_reg)\n",
        "y_clf = np.array(y_clf)\n",
        "\n",
        "###############################################################################\n",
        "# 6) Save final dataset\n",
        "###############################################################################\n",
        "np.savez(\"SP500_preprocessed.npz\", X=X, y_reg=y_reg, y_clf=y_clf)\n",
        "\n",
        "print(\"Done!\")\n",
        "print(\"X shape:\", X.shape)\n",
        "print(\"y_reg shape:\", y_reg.shape)\n",
        "print(\"y_clf shape:\", y_clf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gO1VNKDgwnzP",
        "outputId": "802c37f9-a21f-49ea-ad6a-c429d83eab84"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-5a6f4f3969da>:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df = pd.read_csv(\"SP500.csv\", index_col=0, parse_dates=True)\n",
            "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n",
            "X shape: (3954, 50, 48)\n",
            "y_reg shape: (3954,)\n",
            "y_clf shape: (3954,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train LSTM"
      ],
      "metadata": {
        "id": "hR35v7XbyhnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = np.load(\"SP500_preprocessed.npz\")\n",
        "X, y_reg, y_clf = data['X'], data['y_reg'], data['y_clf']\n",
        "\n",
        "# 按 70% train / 10% validation / 20% test 切分\n",
        "X_temp, X_test, y_reg_temp, y_reg_test, y_clf_temp, y_clf_test = train_test_split(\n",
        "    X, y_reg, y_clf, test_size=0.2, shuffle=False)\n",
        "\n",
        "X_train, X_val, y_reg_train, y_reg_val, y_clf_train, y_clf_val = train_test_split(\n",
        "    X_temp, y_reg_temp, y_clf_temp, test_size=0.125, shuffle=False)\n",
        "\n",
        "print(\"Shapes → Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WDNkV-gypcX",
        "outputId": "8b97469e-9d95-4a05-e70d-04973c2aaeb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes → Train: (2767, 50, 48) Val: (396, 50, 48) Test: (791, 50, 48)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = (~np.isnan(y_reg)) & (~np.isinf(y_reg))\n",
        "X, y_reg, y_clf = X[mask], y_reg[mask], y_clf[mask]\n"
      ],
      "metadata": {
        "id": "U1oZ5rpxzHra"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LFM"
      ],
      "metadata": {
        "id": "k8URpXWHyuSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️⃣ Split data exactly as authors do\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_temp, X_test, y_reg_temp, y_reg_test, y_clf_temp, y_clf_test = train_test_split(X, y_reg, y_clf, test_size=0.2, shuffle=False)\n",
        "X_train, X_val, y_reg_train, y_reg_val, y_clf_train, y_clf_val = train_test_split(X_temp, y_reg_temp, y_clf_temp, test_size=0.125, shuffle=False)\n",
        "\n",
        "# 2️⃣ Build exactly as original script\n",
        "def build_model():\n",
        "    inp = Input(shape=(50, X.shape[2]))\n",
        "    x = LSTM(15, return_sequences=False)(inp)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    out_reg = Dense(1, name=\"regression\")(x)\n",
        "    out_clf = Dense(2, activation=\"softmax\", name=\"classification\")(x)\n",
        "    model = Model(inp, [out_reg, out_clf])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss={\"regression\":\"mse\", \"classification\":\"sparse_categorical_crossentropy\"},\n",
        "        loss_weights={\"regression\":0.1, \"classification\":1.0}\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# 3️⃣ Validate no NaNs in training batches\n",
        "print(\"Train X NaNs:\", np.isnan(X_train).sum(), \"y_reg NaNs:\", np.isnan(y_reg_train).sum())\n",
        "\n",
        "# 4️⃣ Train\n",
        "history = model.fit(\n",
        "    X_train, {\"regression\":y_reg_train, \"classification\":y_clf_train},\n",
        "    validation_data=(X_val, {\"regression\":y_reg_val, \"classification\":y_clf_val}),\n",
        "    batch_size=256, epochs=300, callbacks=[EarlyStopping(patience=20, restore_best_weights=True)]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKCjX-HEyvfb",
        "outputId": "cc778200-897a-482d-ccee-119336cadf97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train X NaNs: 0 y_reg NaNs: 0\n",
            "Epoch 1/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 92ms/step - classification_loss: 0.6887 - loss: 5.0865 - regression_loss: 43.8788 - val_classification_loss: 0.6907 - val_loss: 0.8137 - val_regression_loss: 1.2653\n",
            "Epoch 2/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - classification_loss: 0.6914 - loss: 5.1222 - regression_loss: 44.2889 - val_classification_loss: 0.6910 - val_loss: 0.8129 - val_regression_loss: 1.2580\n",
            "Epoch 3/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - classification_loss: 0.6890 - loss: 4.1000 - regression_loss: 34.0164 - val_classification_loss: 0.6914 - val_loss: 0.8134 - val_regression_loss: 1.2609\n",
            "Epoch 4/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6871 - loss: 3.7978 - regression_loss: 30.9953 - val_classification_loss: 0.6912 - val_loss: 0.8128 - val_regression_loss: 1.2579\n",
            "Epoch 5/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6905 - loss: 3.1821 - regression_loss: 25.5191 - val_classification_loss: 0.6908 - val_loss: 0.8125 - val_regression_loss: 1.2550\n",
            "Epoch 6/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6891 - loss: 4.6058 - regression_loss: 39.0919 - val_classification_loss: 0.6908 - val_loss: 0.8124 - val_regression_loss: 1.2557\n",
            "Epoch 7/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6898 - loss: 3.3434 - regression_loss: 26.4177 - val_classification_loss: 0.6910 - val_loss: 0.8129 - val_regression_loss: 1.2619\n",
            "Epoch 8/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6910 - loss: 4.1591 - regression_loss: 34.6381 - val_classification_loss: 0.6914 - val_loss: 0.8137 - val_regression_loss: 1.2672\n",
            "Epoch 9/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6890 - loss: 4.2982 - regression_loss: 35.9731 - val_classification_loss: 0.6912 - val_loss: 0.8140 - val_regression_loss: 1.2705\n",
            "Epoch 10/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6874 - loss: 4.0606 - regression_loss: 34.4244 - val_classification_loss: 0.6912 - val_loss: 0.8132 - val_regression_loss: 1.2627\n",
            "Epoch 11/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - classification_loss: 0.6881 - loss: 5.1969 - regression_loss: 45.0316 - val_classification_loss: 0.6912 - val_loss: 0.8127 - val_regression_loss: 1.2558\n",
            "Epoch 12/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6875 - loss: 3.3120 - regression_loss: 26.8937 - val_classification_loss: 0.6913 - val_loss: 0.8126 - val_regression_loss: 1.2557\n",
            "Epoch 13/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6915 - loss: 5.1771 - regression_loss: 44.8521 - val_classification_loss: 0.6911 - val_loss: 0.8130 - val_regression_loss: 1.2588\n",
            "Epoch 14/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 65ms/step - classification_loss: 0.6883 - loss: 4.1348 - regression_loss: 34.4424 - val_classification_loss: 0.6921 - val_loss: 0.8136 - val_regression_loss: 1.2604\n",
            "Epoch 15/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - classification_loss: 0.6874 - loss: 6.4119 - regression_loss: 57.1726 - val_classification_loss: 0.6914 - val_loss: 0.8136 - val_regression_loss: 1.2652\n",
            "Epoch 16/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6888 - loss: 3.6089 - regression_loss: 29.3186 - val_classification_loss: 0.6911 - val_loss: 0.8133 - val_regression_loss: 1.2654\n",
            "Epoch 17/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - classification_loss: 0.6891 - loss: 3.1247 - regression_loss: 25.0110 - val_classification_loss: 0.6914 - val_loss: 0.8132 - val_regression_loss: 1.2633\n",
            "Epoch 18/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - classification_loss: 0.6867 - loss: 6.9737 - regression_loss: 62.8267 - val_classification_loss: 0.6919 - val_loss: 0.8134 - val_regression_loss: 1.2601\n",
            "Epoch 19/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - classification_loss: 0.6879 - loss: 6.4605 - regression_loss: 57.6519 - val_classification_loss: 0.6915 - val_loss: 0.8131 - val_regression_loss: 1.2595\n",
            "Epoch 20/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - classification_loss: 0.6884 - loss: 3.5646 - regression_loss: 28.6579 - val_classification_loss: 0.6910 - val_loss: 0.8126 - val_regression_loss: 1.2591\n",
            "Epoch 21/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6886 - loss: 3.4917 - regression_loss: 28.0141 - val_classification_loss: 0.6910 - val_loss: 0.8129 - val_regression_loss: 1.2612\n",
            "Epoch 22/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - classification_loss: 0.6866 - loss: 4.3437 - regression_loss: 36.5807 - val_classification_loss: 0.6912 - val_loss: 0.8125 - val_regression_loss: 1.2591\n",
            "Epoch 23/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6888 - loss: 4.4119 - regression_loss: 37.2597 - val_classification_loss: 0.6910 - val_loss: 0.8119 - val_regression_loss: 1.2546\n",
            "Epoch 24/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6875 - loss: 6.0206 - regression_loss: 53.2372 - val_classification_loss: 0.6913 - val_loss: 0.8119 - val_regression_loss: 1.2518\n",
            "Epoch 25/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6891 - loss: 3.7243 - regression_loss: 30.2976 - val_classification_loss: 0.6912 - val_loss: 0.8119 - val_regression_loss: 1.2528\n",
            "Epoch 26/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6878 - loss: 4.2230 - regression_loss: 35.2664 - val_classification_loss: 0.6908 - val_loss: 0.8114 - val_regression_loss: 1.2512\n",
            "Epoch 27/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - classification_loss: 0.6870 - loss: 5.6234 - regression_loss: 49.2712 - val_classification_loss: 0.6908 - val_loss: 0.8113 - val_regression_loss: 1.2509\n",
            "Epoch 28/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - classification_loss: 0.6893 - loss: 3.9442 - regression_loss: 32.5259 - val_classification_loss: 0.6903 - val_loss: 0.8109 - val_regression_loss: 1.2487\n",
            "Epoch 29/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6881 - loss: 5.8301 - regression_loss: 51.3123 - val_classification_loss: 0.6909 - val_loss: 0.8116 - val_regression_loss: 1.2517\n",
            "Epoch 30/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - classification_loss: 0.6863 - loss: 3.8881 - regression_loss: 31.9848 - val_classification_loss: 0.6906 - val_loss: 0.8115 - val_regression_loss: 1.2514\n",
            "Epoch 31/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6891 - loss: 3.8088 - regression_loss: 31.1461 - val_classification_loss: 0.6909 - val_loss: 0.8114 - val_regression_loss: 1.2508\n",
            "Epoch 32/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6876 - loss: 5.0761 - regression_loss: 43.9911 - val_classification_loss: 0.6905 - val_loss: 0.8110 - val_regression_loss: 1.2494\n",
            "Epoch 33/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6883 - loss: 3.3070 - regression_loss: 26.0859 - val_classification_loss: 0.6904 - val_loss: 0.8109 - val_regression_loss: 1.2482\n",
            "Epoch 34/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6894 - loss: 3.5704 - regression_loss: 28.9140 - val_classification_loss: 0.6905 - val_loss: 0.8112 - val_regression_loss: 1.2489\n",
            "Epoch 35/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - classification_loss: 0.6864 - loss: 3.7921 - regression_loss: 31.1325 - val_classification_loss: 0.6910 - val_loss: 0.8115 - val_regression_loss: 1.2517\n",
            "Epoch 36/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - classification_loss: 0.6904 - loss: 3.8431 - regression_loss: 31.4688 - val_classification_loss: 0.6913 - val_loss: 0.8116 - val_regression_loss: 1.2489\n",
            "Epoch 37/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6876 - loss: 3.6556 - regression_loss: 30.2937 - val_classification_loss: 0.6914 - val_loss: 0.8116 - val_regression_loss: 1.2481\n",
            "Epoch 38/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - classification_loss: 0.6885 - loss: 3.6961 - regression_loss: 30.0330 - val_classification_loss: 0.6911 - val_loss: 0.8118 - val_regression_loss: 1.2505\n",
            "Epoch 39/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - classification_loss: 0.6864 - loss: 3.3786 - regression_loss: 26.9627 - val_classification_loss: 0.6915 - val_loss: 0.8122 - val_regression_loss: 1.2517\n",
            "Epoch 40/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - classification_loss: 0.6894 - loss: 3.5890 - regression_loss: 28.8805 - val_classification_loss: 0.6913 - val_loss: 0.8126 - val_regression_loss: 1.2563\n",
            "Epoch 41/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6870 - loss: 4.1408 - regression_loss: 34.5387 - val_classification_loss: 0.6905 - val_loss: 0.8114 - val_regression_loss: 1.2505\n",
            "Epoch 42/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6849 - loss: 3.6998 - regression_loss: 30.1083 - val_classification_loss: 0.6904 - val_loss: 0.8115 - val_regression_loss: 1.2491\n",
            "Epoch 43/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6884 - loss: 4.6622 - regression_loss: 39.6828 - val_classification_loss: 0.6906 - val_loss: 0.8118 - val_regression_loss: 1.2479\n",
            "Epoch 44/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6899 - loss: 6.2683 - regression_loss: 55.9194 - val_classification_loss: 0.6904 - val_loss: 0.8112 - val_regression_loss: 1.2449\n",
            "Epoch 45/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6894 - loss: 5.4581 - regression_loss: 47.5690 - val_classification_loss: 0.6905 - val_loss: 0.8111 - val_regression_loss: 1.2444\n",
            "Epoch 46/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - classification_loss: 0.6867 - loss: 3.4247 - regression_loss: 27.6107 - val_classification_loss: 0.6904 - val_loss: 0.8110 - val_regression_loss: 1.2460\n",
            "Epoch 47/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - classification_loss: 0.6860 - loss: 3.9758 - regression_loss: 32.7909 - val_classification_loss: 0.6904 - val_loss: 0.8112 - val_regression_loss: 1.2457\n",
            "Epoch 48/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6866 - loss: 4.8617 - regression_loss: 41.6355 - val_classification_loss: 0.6907 - val_loss: 0.8117 - val_regression_loss: 1.2488\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## paper logic:100 LSTM base model + learning rate halved every 50 epochs + 300 epochs fixed training + 13 randomly drawn features + bootstrap samples + final average prediction\n"
      ],
      "metadata": {
        "id": "8ObU0YAi0zR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.models import clone_model\n",
        "\n",
        "def build_model(input_dim):\n",
        "    inp = Input(shape=(50, input_dim))\n",
        "    x = LSTM(15)(inp)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    out_reg = Dense(1, name=\"regression\")(x)\n",
        "    out_clf = Dense(2, activation=\"softmax\", name=\"classification\")(x)\n",
        "    model = Model(inp, [out_reg, out_clf])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss={\"regression\":\"mse\", \"classification\":\"sparse_categorical_crossentropy\"},\n",
        "        loss_weights={\"regression\":0.1, \"classification\":1.0}\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Learning rate schedule: halve every 50 epochs\n",
        "def lr_schedule(epoch):\n",
        "    return 1e-3 * (0.5 ** (epoch // 50))\n",
        "\n",
        "lr_cb = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "ensemble_preds_reg = []\n",
        "ensemble_preds_clf = []\n",
        "\n",
        "for i in range(100):\n",
        "    # Randomly sample 13 features\n",
        "    features_idx = random.sample(range(X_train.shape[2]), 13)\n",
        "    X_tr_sub = X_train[:,:,features_idx]\n",
        "    X_val_sub = X_val[:,:,features_idx]\n",
        "    X_test_sub = X_test[:,:,features_idx]\n",
        "\n",
        "    # Bootstrap sample training data\n",
        "    idx = np.random.choice(len(X_tr_sub), size=len(X_tr_sub), replace=True)\n",
        "    X_boot, y_reg_boot, y_clf_boot = X_tr_sub[idx], y_reg_train[idx], y_clf_train[idx]\n",
        "\n",
        "    # Build & train\n",
        "    model_i = build_model(input_dim=X_train.shape[2] if False else X_boot.shape[2])\n",
        "    model_i.fit(\n",
        "        X_boot, {\"regression\": y_reg_boot, \"classification\": y_clf_boot},\n",
        "        validation_data=(X_val_sub, {\"regression\": y_reg_val, \"classification\": y_clf_val}),\n",
        "        epochs=300, batch_size=256, callbacks=[lr_cb], verbose=0\n",
        "    )\n",
        "\n",
        "    # Predict on test\n",
        "    reg_pred, clf_pred = model_i.predict(X_test_sub, verbose=0)\n",
        "    ensemble_preds_reg.append(reg_pred.flatten())\n",
        "    ensemble_preds_clf.append(clf_pred)\n",
        "\n",
        "# Average ensemble outputs\n",
        "final_reg = np.mean(np.vstack(ensemble_preds_reg), axis=0)\n",
        "final_clf = np.mean(np.stack(ensemble_preds_clf), axis=0).argmax(axis=1)\n",
        "\n",
        "print(\"Ensemble Test RMSE:\", np.sqrt(((final_reg - y_reg_test)**2).mean()))\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "print(\"Ensemble Test BACC:\", balanced_accuracy_score(y_clf_test, final_clf))\n"
      ],
      "metadata": {
        "id": "Eex-ZZcv0-tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#multi task\n"
      ],
      "metadata": {
        "id": "ZpK-bFNIEH9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# --- Assume df_final has been created as in your preprocessing cell ---\n",
        "# final_cols = ['Open','High','Low','Close','Volume'] + computed_features\n",
        "# df_final = df[final_cols].copy()\n",
        "\n",
        "# Check if any NaNs remain in df_final\n",
        "print(\"NaNs per column in final DataFrame:\")\n",
        "print(df_final.isna().sum())\n",
        "\n",
        "# Normalize the final DataFrame\n",
        "scaler = MinMaxScaler()\n",
        "final_cols = df_final.columns.tolist()\n",
        "df_final[final_cols] = scaler.fit_transform(df_final[final_cols])\n",
        "\n",
        "# Compute next-day log returns and drop the first row\n",
        "returns = 100 * np.log(df_final['Close'] / df_final['Close'].shift(1))\n",
        "returns = returns.dropna()\n",
        "\n",
        "# Construct 50-day sequences\n",
        "window = 50\n",
        "X, y_reg, y_clf = [], [], []\n",
        "for i in range(len(returns) - window):\n",
        "    seq = df_final.iloc[i : i+window].values  # each seq: shape (50, 48)\n",
        "    X.append(seq)\n",
        "    ret_val = returns.iloc[i+window]\n",
        "    y_reg.append(ret_val)\n",
        "    y_clf.append(int(ret_val > 0))\n",
        "\n",
        "X = np.array(X)\n",
        "y_reg = np.array(y_reg)\n",
        "y_clf = np.array(y_clf)\n",
        "\n",
        "print(\"Before filtering, X shape:\", X.shape)\n",
        "\n",
        "# Filter out any sequences that contain NaN values\n",
        "valid_mask = ~np.isnan(X).any(axis=(1, 2))\n",
        "X = X[valid_mask]\n",
        "y_reg = y_reg[valid_mask]\n",
        "y_clf = y_clf[valid_mask]\n",
        "\n",
        "print(\"After filtering, X shape:\", X.shape)\n",
        "print(\"y_reg shape:\", y_reg.shape, \"y_clf shape:\", y_clf.shape)\n",
        "\n",
        "# Save the dataset\n",
        "np.savez(\"SP500_preprocessed.npz\", X=X, y_reg=y_reg, y_clf=y_clf)\n",
        "print(\"Dataset saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dEY1Gmu1JVSn",
        "outputId": "7eccdc60-5876-4a23-8c10-1ff9dc6127b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaNs per column in final DataFrame:\n",
            "Open            0\n",
            "High            0\n",
            "Low             0\n",
            "Close           0\n",
            "Volume          0\n",
            "SD_20          19\n",
            "SMA_5           4\n",
            "SMA_10          9\n",
            "SMA_20         19\n",
            "EMA_6           5\n",
            "EMA_10          9\n",
            "EMA_14         13\n",
            "MACD_6_12      19\n",
            "RSI_10          9\n",
            "RSI_14         13\n",
            "CCI_20         19\n",
            "BOLL_UB        19\n",
            "BOLL_LB        19\n",
            "ATR_14          0\n",
            "H-L             0\n",
            "H-Cp            1\n",
            "L-Cp            1\n",
            "TR              0\n",
            "OBV             0\n",
            "MFI            13\n",
            "ForceIndex      1\n",
            "FI_5            5\n",
            "C%              1\n",
            "V%              1\n",
            "NVI             0\n",
            "SEMV            1\n",
            "TSI             1\n",
            "MFR            13\n",
            "UVI            14\n",
            "LVI            14\n",
            "KST            14\n",
            "KST_9          22\n",
            "DPO_20         19\n",
            "DX             15\n",
            "ADX_7           0\n",
            "ADX_14          0\n",
            "ROC_12         12\n",
            "Williams_%R    13\n",
            "EMA_Volume      9\n",
            "ROC_5           5\n",
            "BOLL_BW        19\n",
            "ATR_Ratio      19\n",
            "Vol_CV         19\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before filtering, X shape: (3954, 50, 48)\n",
            "After filtering, X shape: (3932, 50, 48)\n",
            "y_reg shape: (3932,) y_clf shape: (3932,)\n",
            "Dataset saved successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, balanced_accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 1) Load preprocessed dataset (assumed normalized and with 48 features per timestep)\n",
        "# ------------------------------------------------------------------------------\n",
        "data = np.load(\"SP500_preprocessed.npz\")\n",
        "X, y_reg, y_clf = data['X'], data['y_reg'], data['y_clf']\n",
        "print(\"Preprocessed dataset shapes:\")\n",
        "print(\"X:\", X.shape, \"y_reg:\", y_reg.shape, \"y_clf:\", y_clf.shape)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 2) Data cleaning: remove samples with NaN/Inf and clip regression targets\n",
        "# ------------------------------------------------------------------------------\n",
        "mask = (~np.isnan(y_reg)) & (~np.isinf(y_reg))\n",
        "X, y_reg, y_clf = X[mask], y_reg[mask], y_clf[mask]\n",
        "y_reg = np.clip(y_reg, -10, 10)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 3) Split data exactly as authors do:\n",
        "#    70% train / 10% validation / 20% test (without shuffling)\n",
        "# ------------------------------------------------------------------------------\n",
        "X_temp, X_test, y_reg_temp, y_reg_test, y_clf_temp, y_clf_test = train_test_split(\n",
        "    X, y_reg, y_clf, test_size=0.2, shuffle=False)\n",
        "X_train, X_val, y_reg_train, y_reg_val, y_clf_train, y_clf_val = train_test_split(\n",
        "    X_temp, y_reg_temp, y_clf_temp, test_size=0.125, shuffle=False)\n",
        "print(\"Shapes → Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 4) Model building function\n",
        "# ------------------------------------------------------------------------------\n",
        "def build_model(input_dim):\n",
        "    inp = Input(shape=(50, input_dim))\n",
        "    # Shared layers:\n",
        "    x = LSTM(15, return_sequences=False)(inp)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "\n",
        "    # Regression branch: 3 Dense layers\n",
        "    reg_branch = Dense(30, activation=\"relu\")(x)\n",
        "    reg_branch = Dense(30, activation=\"relu\")(reg_branch)\n",
        "    reg_branch = Dense(30, activation=\"relu\")(reg_branch)\n",
        "    out_reg = Dense(1, name=\"regression\")(reg_branch)\n",
        "\n",
        "    # Classification branch: 3 Dense layers\n",
        "    clf_branch = Dense(30, activation=\"relu\")(x)\n",
        "    clf_branch = Dense(30, activation=\"relu\")(clf_branch)\n",
        "    clf_branch = Dense(30, activation=\"relu\")(clf_branch)\n",
        "    out_clf = Dense(2, activation=\"softmax\", name=\"classification\")(clf_branch)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=[out_reg, out_clf])\n",
        "    # Use MSE for regression and cross-entropy for classification\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                  loss={\"regression\": \"mse\",\n",
        "                        \"classification\": \"sparse_categorical_crossentropy\"},\n",
        "                  loss_weights={\"regression\": 0.1, \"classification\": 1.0})\n",
        "    return model\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 5) Learning rate scheduler and early stopping\n",
        "# ------------------------------------------------------------------------------\n",
        "def lr_schedule(epoch, lr):\n",
        "    # Halve the learning rate every 50 epochs\n",
        "    return 1e-3 * (0.5 ** (epoch // 50))\n",
        "lr_cb = LearningRateScheduler(lr_schedule)\n",
        "es_cb = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 6) Ensemble training settings\n",
        "# ------------------------------------------------------------------------------\n",
        "num_models = 100      # Number of ensemble models (as in the paper)\n",
        "feature_subset = 13   # Number of features to randomly sample per model\n",
        "\n",
        "ensemble_preds_reg = []\n",
        "ensemble_preds_clf = []\n",
        "\n",
        "for i in range(num_models):\n",
        "    print(f\"Training model {i+1}/{num_models} ...\")\n",
        "    # Randomly sample feature indices from 0 to total features-1 (total features = 48)\n",
        "    feature_idx = sorted(random.sample(range(X_train.shape[2]), feature_subset))\n",
        "\n",
        "    # Select features from training, validation, and test sets accordingly\n",
        "    X_train_sub = X_train[:, :, feature_idx]\n",
        "    X_val_sub   = X_val[:, :, feature_idx]\n",
        "    X_test_sub  = X_test[:, :, feature_idx]\n",
        "\n",
        "    # Bootstrap sample the training data\n",
        "    indices = np.random.choice(len(X_train_sub), size=len(X_train_sub), replace=True)\n",
        "    X_boot = X_train_sub[indices]\n",
        "    y_reg_boot = y_reg_train[indices]\n",
        "    y_clf_boot = y_clf_train[indices]\n",
        "\n",
        "    # Build and train the model on the bootstrap sample\n",
        "    model_i = build_model(input_dim=feature_subset)\n",
        "    es_inner = EarlyStopping(patience=20, restore_best_weights=True)\n",
        "\n",
        "    model_i.fit(\n",
        "        X_boot, {\"regression\": y_reg_boot, \"classification\": y_clf_boot},\n",
        "        validation_data=(X_val_sub, {\"regression\": y_reg_val, \"classification\": y_clf_val}),\n",
        "        epochs=300,\n",
        "        batch_size=256,\n",
        "        callbacks=[lr_cb, es_inner],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Predict on the test set with the same feature subset\n",
        "    pred_reg_i, pred_clf_i = model_i.predict(X_test_sub, verbose=0)\n",
        "    ensemble_preds_reg.append(pred_reg_i.flatten())\n",
        "    ensemble_preds_clf.append(pred_clf_i)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 7) Aggregate ensemble predictions\n",
        "# ------------------------------------------------------------------------------\n",
        "final_pred_reg = np.mean(np.vstack(ensemble_preds_reg), axis=0)\n",
        "final_pred_clf_prob = np.mean(np.stack(ensemble_preds_clf), axis=0)\n",
        "final_pred_clf = final_pred_clf_prob.argmax(axis=1)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# 8) Evaluate ensemble performance\n",
        "# ------------------------------------------------------------------------------\n",
        "rmse = math.sqrt(mean_squared_error(y_reg_test, final_pred_reg))\n",
        "acc = accuracy_score(y_clf_test, final_pred_clf)\n",
        "bacc = balanced_accuracy_score(y_clf_test, final_pred_clf)\n",
        "\n",
        "print(\"Ensemble Test RMSE:\", rmse)\n",
        "print(\"Ensemble Test Accuracy:\", acc)\n",
        "print(\"Ensemble Test Balanced Accuracy:\", bacc)\n"
      ],
      "metadata": {
        "id": "3sIBrW2st1jf",
        "outputId": "5c189a6e-6eeb-48c3-cae5-e4c19f318efb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed dataset shapes:\n",
            "X: (3932, 50, 48) y_reg: (3932,) y_clf: (3932,)\n",
            "Shapes → Train: (2751, 50, 48) Val: (393, 50, 48) Test: (786, 50, 48)\n",
            "Training model 1/100 ...\n",
            "Training model 2/100 ...\n",
            "Training model 3/100 ...\n",
            "Training model 4/100 ...\n",
            "Training model 5/100 ...\n",
            "Training model 6/100 ...\n",
            "Training model 7/100 ...\n",
            "Training model 8/100 ...\n",
            "Training model 9/100 ...\n",
            "Training model 10/100 ...\n",
            "Training model 11/100 ...\n",
            "Training model 12/100 ...\n",
            "Training model 13/100 ...\n",
            "Training model 14/100 ...\n",
            "Training model 15/100 ...\n",
            "Training model 16/100 ...\n",
            "Training model 17/100 ...\n",
            "Training model 18/100 ...\n",
            "Training model 19/100 ...\n",
            "Training model 20/100 ...\n",
            "Training model 21/100 ...\n",
            "Training model 22/100 ...\n",
            "Training model 23/100 ...\n",
            "Training model 24/100 ...\n",
            "Training model 25/100 ...\n",
            "Training model 26/100 ...\n",
            "Training model 27/100 ...\n",
            "Training model 28/100 ...\n",
            "Training model 29/100 ...\n",
            "Training model 30/100 ...\n",
            "Training model 31/100 ...\n",
            "Training model 32/100 ...\n",
            "Training model 33/100 ...\n",
            "Training model 34/100 ...\n",
            "Training model 35/100 ...\n",
            "Training model 36/100 ...\n",
            "Training model 37/100 ...\n",
            "Training model 38/100 ...\n",
            "Training model 39/100 ...\n",
            "Training model 40/100 ...\n",
            "Training model 41/100 ...\n",
            "Training model 42/100 ...\n",
            "Training model 43/100 ...\n",
            "Training model 44/100 ...\n",
            "Training model 45/100 ...\n",
            "Training model 46/100 ...\n",
            "Training model 47/100 ...\n",
            "Training model 48/100 ...\n",
            "Training model 49/100 ...\n",
            "Training model 50/100 ...\n",
            "Training model 51/100 ...\n",
            "Training model 52/100 ...\n",
            "Training model 53/100 ...\n",
            "Training model 54/100 ...\n",
            "Training model 55/100 ...\n",
            "Training model 56/100 ...\n",
            "Training model 57/100 ...\n",
            "Training model 58/100 ...\n",
            "Training model 59/100 ...\n",
            "Training model 60/100 ...\n",
            "Training model 61/100 ...\n",
            "Training model 62/100 ...\n",
            "Training model 63/100 ...\n",
            "Training model 64/100 ...\n",
            "Training model 65/100 ...\n",
            "Training model 66/100 ...\n",
            "Training model 67/100 ...\n",
            "Training model 68/100 ...\n",
            "Training model 69/100 ...\n",
            "Training model 70/100 ...\n",
            "Training model 71/100 ...\n",
            "Training model 72/100 ...\n",
            "Training model 73/100 ...\n",
            "Training model 74/100 ...\n",
            "Training model 75/100 ...\n",
            "Training model 76/100 ...\n",
            "Training model 77/100 ...\n",
            "Training model 78/100 ...\n",
            "Training model 79/100 ...\n",
            "Training model 80/100 ...\n",
            "Training model 81/100 ...\n",
            "Training model 82/100 ...\n",
            "Training model 83/100 ...\n",
            "Training model 84/100 ...\n",
            "Training model 85/100 ...\n",
            "Training model 86/100 ...\n",
            "Training model 87/100 ...\n",
            "Training model 88/100 ...\n",
            "Training model 89/100 ...\n",
            "Training model 90/100 ...\n",
            "Training model 91/100 ...\n",
            "Training model 92/100 ...\n",
            "Training model 93/100 ...\n",
            "Training model 94/100 ...\n",
            "Training model 95/100 ...\n",
            "Training model 96/100 ...\n",
            "Training model 97/100 ...\n",
            "Training model 98/100 ...\n",
            "Training model 99/100 ...\n",
            "Training model 100/100 ...\n",
            "Ensemble Test RMSE: 1.1970973068793187\n",
            "Ensemble Test Accuracy: 0.5330788804071247\n",
            "Ensemble Test Balanced Accuracy: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1) Load & split data\n",
        "data = np.load(\"SP500_preprocessed.npz\")\n",
        "X, y_reg, y_clf = data['X'], data['y_reg'], data['y_clf']\n",
        "mask = (~np.isnan(y_reg)) & (~np.isinf(y_reg))\n",
        "X, y_reg, y_clf = X[mask], y_reg[mask], y_clf[mask]\n",
        "y_reg = np.clip(y_reg, -10, 10)\n",
        "\n",
        "X_temp, X_test, y_reg_temp, y_reg_test, y_clf_temp, y_clf_test = train_test_split(\n",
        "    X, y_reg, y_clf, test_size=0.2, shuffle=False)\n",
        "X_train, X_val, y_reg_train, y_reg_val, y_clf_train, y_clf_val = train_test_split(\n",
        "    X_temp, y_reg_temp, y_clf_temp, test_size=0.125, shuffle=False)\n",
        "\n",
        "# 2) Build model function\n",
        "def build_model(input_dim):\n",
        "    inp = Input(shape=(50, input_dim))\n",
        "    x = LSTM(15)(inp)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    # regression branch\n",
        "    r = Dense(30, activation=\"relu\")(x)\n",
        "    r = Dense(30, activation=\"relu\")(r)\n",
        "    r = Dense(30, activation=\"relu\")(r)\n",
        "    out_reg = Dense(1, name=\"regression\")(r)\n",
        "    # classification branch\n",
        "    c = Dense(30, activation=\"relu\")(x)\n",
        "    c = Dense(30, activation=\"relu\")(c)\n",
        "    c = Dense(30, activation=\"relu\")(c)\n",
        "    out_clf = Dense(2, activation=\"softmax\", name=\"classification\")(c)\n",
        "    model = Model(inp, [out_reg, out_clf])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss={\"regression\":\"mse\",\"classification\":\"sparse_categorical_crossentropy\"},\n",
        "        loss_weights={\"regression\":0.1,\"classification\":1.0}\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 3) Learning rate scheduler\n",
        "def lr_schedule(epoch, lr):\n",
        "    return 1e-3 * (0.5 ** (epoch // 50))\n",
        "lr_cb = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# 4) Prepare one subset of features (first 13 for example)\n",
        "feature_subset = 13\n",
        "feature_idx = list(range(feature_subset))\n",
        "X_train_sub = X_train[:, :, feature_idx]\n",
        "X_val_sub   = X_val[:, :, feature_idx]\n",
        "\n",
        "# 5) Train one model without EarlyStopping\n",
        "model = build_model(input_dim=feature_subset)\n",
        "history = model.fit(\n",
        "    X_train_sub, {\"regression\": y_reg_train, \"classification\": y_clf_train},\n",
        "    validation_data=(X_val_sub, {\"regression\": y_reg_val, \"classification\": y_clf_val}),\n",
        "    epochs=300,\n",
        "    batch_size=256,\n",
        "    callbacks=[lr_cb],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 6) Plot loss curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Train total loss')\n",
        "plt.plot(history.history['val_loss'], label='Val total loss')\n",
        "plt.plot(history.history['regression_loss'], label='Train MSE loss')\n",
        "plt.plot(history.history['val_regression_loss'], label='Val MSE loss')\n",
        "plt.plot(history.history['classification_loss'], label='Train CE loss')\n",
        "plt.plot(history.history['val_classification_loss'], label='Val CE loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Multi-Task Training Curves (no EarlyStopping)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "13cFJ4n05xEZ",
        "outputId": "1715ed0c-8d0a-42e1-ef13-d78adc58f2fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 164ms/step - classification_loss: 0.6902 - loss: 1.6525 - regression_loss: 9.6198 - val_classification_loss: 0.6940 - val_loss: 0.8144 - val_regression_loss: 1.2579 - learning_rate: 0.0010\n",
            "Epoch 2/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - classification_loss: 0.6891 - loss: 1.7102 - regression_loss: 10.2013 - val_classification_loss: 0.6940 - val_loss: 0.8142 - val_regression_loss: 1.2564 - learning_rate: 0.0010\n",
            "Epoch 3/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 72ms/step - classification_loss: 0.6884 - loss: 1.6891 - regression_loss: 10.0136 - val_classification_loss: 0.6941 - val_loss: 0.8143 - val_regression_loss: 1.2567 - learning_rate: 0.0010\n",
            "Epoch 4/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 68ms/step - classification_loss: 0.6921 - loss: 1.6825 - regression_loss: 9.8930 - val_classification_loss: 0.6925 - val_loss: 0.8133 - val_regression_loss: 1.2583 - learning_rate: 0.0010\n",
            "Epoch 5/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step - classification_loss: 0.6890 - loss: 1.6590 - regression_loss: 9.7079 - val_classification_loss: 0.6939 - val_loss: 0.8142 - val_regression_loss: 1.2569 - learning_rate: 0.0010\n",
            "Epoch 6/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 91ms/step - classification_loss: 0.6866 - loss: 1.6652 - regression_loss: 9.7833 - val_classification_loss: 0.6929 - val_loss: 0.8134 - val_regression_loss: 1.2566 - learning_rate: 0.0010\n",
            "Epoch 7/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - classification_loss: 0.6889 - loss: 1.6071 - regression_loss: 9.1849 - val_classification_loss: 0.6917 - val_loss: 0.8126 - val_regression_loss: 1.2570 - learning_rate: 0.0010\n",
            "Epoch 8/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 59ms/step - classification_loss: 0.6875 - loss: 1.6593 - regression_loss: 9.7303 - val_classification_loss: 0.6924 - val_loss: 0.8133 - val_regression_loss: 1.2594 - learning_rate: 0.0010\n",
            "Epoch 9/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - classification_loss: 0.6896 - loss: 1.6926 - regression_loss: 10.0317 - val_classification_loss: 0.6923 - val_loss: 0.8130 - val_regression_loss: 1.2573 - learning_rate: 0.0010\n",
            "Epoch 10/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6890 - loss: 1.6143 - regression_loss: 9.2578 - val_classification_loss: 0.6925 - val_loss: 0.8131 - val_regression_loss: 1.2561 - learning_rate: 0.0010\n",
            "Epoch 11/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - classification_loss: 0.6869 - loss: 1.6730 - regression_loss: 9.8597 - val_classification_loss: 0.6925 - val_loss: 0.8132 - val_regression_loss: 1.2568 - learning_rate: 0.0010\n",
            "Epoch 12/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - classification_loss: 0.6891 - loss: 1.6506 - regression_loss: 9.6263 - val_classification_loss: 0.6920 - val_loss: 0.8132 - val_regression_loss: 1.2609 - learning_rate: 0.0010\n",
            "Epoch 13/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - classification_loss: 0.6875 - loss: 1.6392 - regression_loss: 9.5219 - val_classification_loss: 0.6921 - val_loss: 0.8131 - val_regression_loss: 1.2596 - learning_rate: 0.0010\n",
            "Epoch 14/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6887 - loss: 1.6672 - regression_loss: 9.7837 - val_classification_loss: 0.6916 - val_loss: 0.8127 - val_regression_loss: 1.2584 - learning_rate: 0.0010\n",
            "Epoch 15/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - classification_loss: 0.6867 - loss: 1.5669 - regression_loss: 8.8048 - val_classification_loss: 0.6925 - val_loss: 0.8134 - val_regression_loss: 1.2593 - learning_rate: 0.0010\n",
            "Epoch 16/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - classification_loss: 0.6895 - loss: 1.7266 - regression_loss: 10.3709 - val_classification_loss: 0.6916 - val_loss: 0.8128 - val_regression_loss: 1.2593 - learning_rate: 0.0010\n",
            "Epoch 17/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6881 - loss: 1.6764 - regression_loss: 9.8769 - val_classification_loss: 0.6934 - val_loss: 0.8137 - val_regression_loss: 1.2553 - learning_rate: 0.0010\n",
            "Epoch 18/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6869 - loss: 1.6783 - regression_loss: 9.9224 - val_classification_loss: 0.6926 - val_loss: 0.8130 - val_regression_loss: 1.2555 - learning_rate: 0.0010\n",
            "Epoch 19/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - classification_loss: 0.6888 - loss: 1.6642 - regression_loss: 9.7558 - val_classification_loss: 0.6920 - val_loss: 0.8128 - val_regression_loss: 1.2568 - learning_rate: 0.0010\n",
            "Epoch 20/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6907 - loss: 1.6641 - regression_loss: 9.7334 - val_classification_loss: 0.6916 - val_loss: 0.8130 - val_regression_loss: 1.2613 - learning_rate: 0.0010\n",
            "Epoch 21/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6874 - loss: 1.6712 - regression_loss: 9.8443 - val_classification_loss: 0.6920 - val_loss: 0.8132 - val_regression_loss: 1.2615 - learning_rate: 0.0010\n",
            "Epoch 22/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6862 - loss: 1.6021 - regression_loss: 9.1550 - val_classification_loss: 0.6927 - val_loss: 0.8134 - val_regression_loss: 1.2584 - learning_rate: 0.0010\n",
            "Epoch 23/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6885 - loss: 1.7024 - regression_loss: 10.1337 - val_classification_loss: 0.6918 - val_loss: 0.8133 - val_regression_loss: 1.2640 - learning_rate: 0.0010\n",
            "Epoch 24/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - classification_loss: 0.6896 - loss: 1.6446 - regression_loss: 9.5546 - val_classification_loss: 0.6919 - val_loss: 0.8135 - val_regression_loss: 1.2652 - learning_rate: 0.0010\n",
            "Epoch 25/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - classification_loss: 0.6903 - loss: 1.6534 - regression_loss: 9.6264 - val_classification_loss: 0.6916 - val_loss: 0.8129 - val_regression_loss: 1.2602 - learning_rate: 0.0010\n",
            "Epoch 26/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - classification_loss: 0.6881 - loss: 1.6345 - regression_loss: 9.4702 - val_classification_loss: 0.6927 - val_loss: 0.8135 - val_regression_loss: 1.2589 - learning_rate: 0.0010\n",
            "Epoch 27/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - classification_loss: 0.6889 - loss: 1.6624 - regression_loss: 9.7410 - val_classification_loss: 0.6920 - val_loss: 0.8132 - val_regression_loss: 1.2611 - learning_rate: 0.0010\n",
            "Epoch 28/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - classification_loss: 0.6872 - loss: 1.6722 - regression_loss: 9.8518 - val_classification_loss: 0.6915 - val_loss: 0.8124 - val_regression_loss: 1.2567 - learning_rate: 0.0010\n",
            "Epoch 29/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6862 - loss: 1.6566 - regression_loss: 9.7082 - val_classification_loss: 0.6917 - val_loss: 0.8131 - val_regression_loss: 1.2627 - learning_rate: 0.0010\n",
            "Epoch 30/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - classification_loss: 0.6873 - loss: 1.6628 - regression_loss: 9.7427 - val_classification_loss: 0.6911 - val_loss: 0.8137 - val_regression_loss: 1.2726 - learning_rate: 0.0010\n",
            "Epoch 31/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6889 - loss: 1.6901 - regression_loss: 10.0184 - val_classification_loss: 0.6912 - val_loss: 0.8126 - val_regression_loss: 1.2605 - learning_rate: 0.0010\n",
            "Epoch 32/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6879 - loss: 1.6825 - regression_loss: 9.9570 - val_classification_loss: 0.6914 - val_loss: 0.8126 - val_regression_loss: 1.2593 - learning_rate: 0.0010\n",
            "Epoch 33/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6891 - loss: 1.6989 - regression_loss: 10.0942 - val_classification_loss: 0.6913 - val_loss: 0.8134 - val_regression_loss: 1.2676 - learning_rate: 0.0010\n",
            "Epoch 34/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6878 - loss: 1.6553 - regression_loss: 9.6719 - val_classification_loss: 0.6913 - val_loss: 0.8124 - val_regression_loss: 1.2575 - learning_rate: 0.0010\n",
            "Epoch 35/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6880 - loss: 1.6145 - regression_loss: 9.2638 - val_classification_loss: 0.6914 - val_loss: 0.8134 - val_regression_loss: 1.2679 - learning_rate: 0.0010\n",
            "Epoch 36/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - classification_loss: 0.6866 - loss: 1.6805 - regression_loss: 9.9479 - val_classification_loss: 0.6914 - val_loss: 0.8124 - val_regression_loss: 1.2579 - learning_rate: 0.0010\n",
            "Epoch 37/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6858 - loss: 1.6193 - regression_loss: 9.3298 - val_classification_loss: 0.6915 - val_loss: 0.8125 - val_regression_loss: 1.2579 - learning_rate: 0.0010\n",
            "Epoch 38/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6870 - loss: 1.6897 - regression_loss: 10.0191 - val_classification_loss: 0.6911 - val_loss: 0.8132 - val_regression_loss: 1.2676 - learning_rate: 0.0010\n",
            "Epoch 39/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6894 - loss: 1.7293 - regression_loss: 10.3940 - val_classification_loss: 0.6911 - val_loss: 0.8131 - val_regression_loss: 1.2661 - learning_rate: 0.0010\n",
            "Epoch 40/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6875 - loss: 1.7054 - regression_loss: 10.1734 - val_classification_loss: 0.6915 - val_loss: 0.8125 - val_regression_loss: 1.2575 - learning_rate: 0.0010\n",
            "Epoch 41/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6858 - loss: 1.7480 - regression_loss: 10.6119 - val_classification_loss: 0.6919 - val_loss: 0.8128 - val_regression_loss: 1.2578 - learning_rate: 0.0010\n",
            "Epoch 42/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6874 - loss: 1.5930 - regression_loss: 9.0655 - val_classification_loss: 0.6913 - val_loss: 0.8130 - val_regression_loss: 1.2642 - learning_rate: 0.0010\n",
            "Epoch 43/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6891 - loss: 1.6642 - regression_loss: 9.7585 - val_classification_loss: 0.6911 - val_loss: 0.8124 - val_regression_loss: 1.2591 - learning_rate: 0.0010\n",
            "Epoch 44/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6872 - loss: 1.6600 - regression_loss: 9.7383 - val_classification_loss: 0.6917 - val_loss: 0.8126 - val_regression_loss: 1.2576 - learning_rate: 0.0010\n",
            "Epoch 45/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6884 - loss: 1.7270 - regression_loss: 10.3782 - val_classification_loss: 0.6911 - val_loss: 0.8135 - val_regression_loss: 1.2706 - learning_rate: 0.0010\n",
            "Epoch 46/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6870 - loss: 1.6852 - regression_loss: 9.9815 - val_classification_loss: 0.6908 - val_loss: 0.8124 - val_regression_loss: 1.2606 - learning_rate: 0.0010\n",
            "Epoch 47/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6891 - loss: 1.7057 - regression_loss: 10.1631 - val_classification_loss: 0.6908 - val_loss: 0.8127 - val_regression_loss: 1.2620 - learning_rate: 0.0010\n",
            "Epoch 48/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6854 - loss: 1.6939 - regression_loss: 10.0810 - val_classification_loss: 0.6918 - val_loss: 0.8129 - val_regression_loss: 1.2593 - learning_rate: 0.0010\n",
            "Epoch 49/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6899 - loss: 1.7036 - regression_loss: 10.1390 - val_classification_loss: 0.6909 - val_loss: 0.8134 - val_regression_loss: 1.2702 - learning_rate: 0.0010\n",
            "Epoch 50/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - classification_loss: 0.6872 - loss: 1.5918 - regression_loss: 9.0624 - val_classification_loss: 0.6909 - val_loss: 0.8126 - val_regression_loss: 1.2626 - learning_rate: 0.0010\n",
            "Epoch 51/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - classification_loss: 0.6897 - loss: 1.6480 - regression_loss: 9.5806 - val_classification_loss: 0.6909 - val_loss: 0.8128 - val_regression_loss: 1.2645 - learning_rate: 5.0000e-04\n",
            "Epoch 52/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6877 - loss: 1.6541 - regression_loss: 9.6707 - val_classification_loss: 0.6910 - val_loss: 0.8128 - val_regression_loss: 1.2635 - learning_rate: 5.0000e-04\n",
            "Epoch 53/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - classification_loss: 0.6893 - loss: 1.6435 - regression_loss: 9.5467 - val_classification_loss: 0.6910 - val_loss: 0.8128 - val_regression_loss: 1.2637 - learning_rate: 5.0000e-04\n",
            "Epoch 54/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - classification_loss: 0.6905 - loss: 1.6152 - regression_loss: 9.2471 - val_classification_loss: 0.6910 - val_loss: 0.8130 - val_regression_loss: 1.2660 - learning_rate: 5.0000e-04\n",
            "Epoch 55/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6901 - loss: 1.6885 - regression_loss: 9.9987 - val_classification_loss: 0.6912 - val_loss: 0.8129 - val_regression_loss: 1.2642 - learning_rate: 5.0000e-04\n",
            "Epoch 56/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - classification_loss: 0.6873 - loss: 1.6889 - regression_loss: 10.0179 - val_classification_loss: 0.6914 - val_loss: 0.8128 - val_regression_loss: 1.2619 - learning_rate: 5.0000e-04\n",
            "Epoch 57/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6906 - loss: 1.6948 - regression_loss: 10.0418 - val_classification_loss: 0.6911 - val_loss: 0.8129 - val_regression_loss: 1.2649 - learning_rate: 5.0000e-04\n",
            "Epoch 58/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6869 - loss: 1.7039 - regression_loss: 10.1847 - val_classification_loss: 0.6913 - val_loss: 0.8125 - val_regression_loss: 1.2597 - learning_rate: 5.0000e-04\n",
            "Epoch 59/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6843 - loss: 1.6585 - regression_loss: 9.7439 - val_classification_loss: 0.6911 - val_loss: 0.8131 - val_regression_loss: 1.2662 - learning_rate: 5.0000e-04\n",
            "Epoch 60/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6872 - loss: 1.6451 - regression_loss: 9.5771 - val_classification_loss: 0.6910 - val_loss: 0.8136 - val_regression_loss: 1.2728 - learning_rate: 5.0000e-04\n",
            "Epoch 61/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6881 - loss: 1.6408 - regression_loss: 9.5432 - val_classification_loss: 0.6909 - val_loss: 0.8141 - val_regression_loss: 1.2786 - learning_rate: 5.0000e-04\n",
            "Epoch 62/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6881 - loss: 1.6854 - regression_loss: 9.9617 - val_classification_loss: 0.6910 - val_loss: 0.8131 - val_regression_loss: 1.2671 - learning_rate: 5.0000e-04\n",
            "Epoch 63/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6873 - loss: 1.6530 - regression_loss: 9.6605 - val_classification_loss: 0.6910 - val_loss: 0.8131 - val_regression_loss: 1.2673 - learning_rate: 5.0000e-04\n",
            "Epoch 64/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6877 - loss: 1.6396 - regression_loss: 9.5226 - val_classification_loss: 0.6912 - val_loss: 0.8130 - val_regression_loss: 1.2656 - learning_rate: 5.0000e-04\n",
            "Epoch 65/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6890 - loss: 1.6733 - regression_loss: 9.8313 - val_classification_loss: 0.6911 - val_loss: 0.8132 - val_regression_loss: 1.2676 - learning_rate: 5.0000e-04\n",
            "Epoch 66/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6889 - loss: 1.7227 - regression_loss: 10.3356 - val_classification_loss: 0.6910 - val_loss: 0.8128 - val_regression_loss: 1.2637 - learning_rate: 5.0000e-04\n",
            "Epoch 67/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6857 - loss: 1.6269 - regression_loss: 9.4122 - val_classification_loss: 0.6912 - val_loss: 0.8125 - val_regression_loss: 1.2602 - learning_rate: 5.0000e-04\n",
            "Epoch 68/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6861 - loss: 1.6424 - regression_loss: 9.5553 - val_classification_loss: 0.6912 - val_loss: 0.8132 - val_regression_loss: 1.2674 - learning_rate: 5.0000e-04\n",
            "Epoch 69/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - classification_loss: 0.6868 - loss: 1.6031 - regression_loss: 9.1668 - val_classification_loss: 0.6911 - val_loss: 0.8133 - val_regression_loss: 1.2685 - learning_rate: 5.0000e-04\n",
            "Epoch 70/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6873 - loss: 1.6439 - regression_loss: 9.5595 - val_classification_loss: 0.6911 - val_loss: 0.8129 - val_regression_loss: 1.2652 - learning_rate: 5.0000e-04\n",
            "Epoch 71/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - classification_loss: 0.6857 - loss: 1.6545 - regression_loss: 9.6834 - val_classification_loss: 0.6911 - val_loss: 0.8128 - val_regression_loss: 1.2639 - learning_rate: 5.0000e-04\n",
            "Epoch 72/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6897 - loss: 1.6285 - regression_loss: 9.3897 - val_classification_loss: 0.6909 - val_loss: 0.8132 - val_regression_loss: 1.2684 - learning_rate: 5.0000e-04\n",
            "Epoch 73/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6847 - loss: 1.6322 - regression_loss: 9.4750 - val_classification_loss: 0.6911 - val_loss: 0.8128 - val_regression_loss: 1.2643 - learning_rate: 5.0000e-04\n",
            "Epoch 74/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6876 - loss: 1.6754 - regression_loss: 9.8772 - val_classification_loss: 0.6910 - val_loss: 0.8141 - val_regression_loss: 1.2784 - learning_rate: 5.0000e-04\n",
            "Epoch 75/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6853 - loss: 1.6559 - regression_loss: 9.7068 - val_classification_loss: 0.6910 - val_loss: 0.8135 - val_regression_loss: 1.2715 - learning_rate: 5.0000e-04\n",
            "Epoch 76/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - classification_loss: 0.6870 - loss: 1.6584 - regression_loss: 9.7223 - val_classification_loss: 0.6910 - val_loss: 0.8135 - val_regression_loss: 1.2723 - learning_rate: 5.0000e-04\n",
            "Epoch 77/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - classification_loss: 0.6857 - loss: 1.7321 - regression_loss: 10.4725 - val_classification_loss: 0.6911 - val_loss: 0.8146 - val_regression_loss: 1.2837 - learning_rate: 5.0000e-04\n",
            "Epoch 78/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6855 - loss: 1.6229 - regression_loss: 9.3718 - val_classification_loss: 0.6910 - val_loss: 0.8147 - val_regression_loss: 1.2848 - learning_rate: 5.0000e-04\n",
            "Epoch 79/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6867 - loss: 1.6277 - regression_loss: 9.4127 - val_classification_loss: 0.6910 - val_loss: 0.8151 - val_regression_loss: 1.2890 - learning_rate: 5.0000e-04\n",
            "Epoch 80/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6883 - loss: 1.6133 - regression_loss: 9.2577 - val_classification_loss: 0.6910 - val_loss: 0.8137 - val_regression_loss: 1.2739 - learning_rate: 5.0000e-04\n",
            "Epoch 81/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - classification_loss: 0.6861 - loss: 1.6326 - regression_loss: 9.4683 - val_classification_loss: 0.6910 - val_loss: 0.8141 - val_regression_loss: 1.2785 - learning_rate: 5.0000e-04\n",
            "Epoch 82/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6888 - loss: 1.6407 - regression_loss: 9.5278 - val_classification_loss: 0.6910 - val_loss: 0.8145 - val_regression_loss: 1.2822 - learning_rate: 5.0000e-04\n",
            "Epoch 83/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6868 - loss: 1.6700 - regression_loss: 9.8222 - val_classification_loss: 0.6909 - val_loss: 0.8153 - val_regression_loss: 1.2919 - learning_rate: 5.0000e-04\n",
            "Epoch 84/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - classification_loss: 0.6880 - loss: 1.6564 - regression_loss: 9.6755 - val_classification_loss: 0.6909 - val_loss: 0.8145 - val_regression_loss: 1.2836 - learning_rate: 5.0000e-04\n",
            "Epoch 85/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6898 - loss: 1.7139 - regression_loss: 10.2510 - val_classification_loss: 0.6910 - val_loss: 0.8155 - val_regression_loss: 1.2928 - learning_rate: 5.0000e-04\n",
            "Epoch 86/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6859 - loss: 1.6682 - regression_loss: 9.8211 - val_classification_loss: 0.6911 - val_loss: 0.8134 - val_regression_loss: 1.2706 - learning_rate: 5.0000e-04\n",
            "Epoch 87/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6887 - loss: 1.6692 - regression_loss: 9.8057 - val_classification_loss: 0.6910 - val_loss: 0.8147 - val_regression_loss: 1.2842 - learning_rate: 5.0000e-04\n",
            "Epoch 88/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6840 - loss: 1.6472 - regression_loss: 9.6268 - val_classification_loss: 0.6912 - val_loss: 0.8143 - val_regression_loss: 1.2800 - learning_rate: 5.0000e-04\n",
            "Epoch 89/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6853 - loss: 1.6447 - regression_loss: 9.5912 - val_classification_loss: 0.6911 - val_loss: 0.8163 - val_regression_loss: 1.3011 - learning_rate: 5.0000e-04\n",
            "Epoch 90/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6871 - loss: 1.6975 - regression_loss: 10.0941 - val_classification_loss: 0.6911 - val_loss: 0.8164 - val_regression_loss: 1.3021 - learning_rate: 5.0000e-04\n",
            "Epoch 91/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6859 - loss: 1.7295 - regression_loss: 10.4474 - val_classification_loss: 0.6911 - val_loss: 0.8161 - val_regression_loss: 1.2985 - learning_rate: 5.0000e-04\n",
            "Epoch 92/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - classification_loss: 0.6885 - loss: 1.6108 - regression_loss: 9.2143 - val_classification_loss: 0.6911 - val_loss: 0.8198 - val_regression_loss: 1.3372 - learning_rate: 5.0000e-04\n",
            "Epoch 93/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6876 - loss: 1.6416 - regression_loss: 9.5341 - val_classification_loss: 0.6911 - val_loss: 0.8167 - val_regression_loss: 1.3052 - learning_rate: 5.0000e-04\n",
            "Epoch 94/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6879 - loss: 1.6714 - regression_loss: 9.8351 - val_classification_loss: 0.6911 - val_loss: 0.8172 - val_regression_loss: 1.3100 - learning_rate: 5.0000e-04\n",
            "Epoch 95/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6873 - loss: 1.6765 - regression_loss: 9.8954 - val_classification_loss: 0.6912 - val_loss: 0.8155 - val_regression_loss: 1.2924 - learning_rate: 5.0000e-04\n",
            "Epoch 96/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - classification_loss: 0.6863 - loss: 1.6690 - regression_loss: 9.8238 - val_classification_loss: 0.6911 - val_loss: 0.8153 - val_regression_loss: 1.2911 - learning_rate: 5.0000e-04\n",
            "Epoch 97/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6855 - loss: 1.6460 - regression_loss: 9.6050 - val_classification_loss: 0.6910 - val_loss: 0.8155 - val_regression_loss: 1.2933 - learning_rate: 5.0000e-04\n",
            "Epoch 98/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - classification_loss: 0.6842 - loss: 1.6668 - regression_loss: 9.8228 - val_classification_loss: 0.6910 - val_loss: 0.8145 - val_regression_loss: 1.2829 - learning_rate: 5.0000e-04\n",
            "Epoch 99/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - classification_loss: 0.6865 - loss: 1.6886 - regression_loss: 10.0263 - val_classification_loss: 0.6909 - val_loss: 0.8152 - val_regression_loss: 1.2899 - learning_rate: 5.0000e-04\n",
            "Epoch 100/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - classification_loss: 0.6864 - loss: 1.6304 - regression_loss: 9.4389 - val_classification_loss: 0.6910 - val_loss: 0.8143 - val_regression_loss: 1.2809 - learning_rate: 5.0000e-04\n",
            "Epoch 101/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6868 - loss: 1.6745 - regression_loss: 9.8775 - val_classification_loss: 0.6909 - val_loss: 0.8159 - val_regression_loss: 1.2982 - learning_rate: 2.5000e-04\n",
            "Epoch 102/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6861 - loss: 1.6239 - regression_loss: 9.3774 - val_classification_loss: 0.6909 - val_loss: 0.8152 - val_regression_loss: 1.2906 - learning_rate: 2.5000e-04\n",
            "Epoch 103/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6880 - loss: 1.6448 - regression_loss: 9.5768 - val_classification_loss: 0.6909 - val_loss: 0.8150 - val_regression_loss: 1.2876 - learning_rate: 2.5000e-04\n",
            "Epoch 104/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6863 - loss: 1.6462 - regression_loss: 9.6083 - val_classification_loss: 0.6910 - val_loss: 0.8136 - val_regression_loss: 1.2733 - learning_rate: 2.5000e-04\n",
            "Epoch 105/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6874 - loss: 1.6153 - regression_loss: 9.2807 - val_classification_loss: 0.6910 - val_loss: 0.8162 - val_regression_loss: 1.3012 - learning_rate: 2.5000e-04\n",
            "Epoch 106/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6869 - loss: 1.6892 - regression_loss: 10.0180 - val_classification_loss: 0.6910 - val_loss: 0.8166 - val_regression_loss: 1.3054 - learning_rate: 2.5000e-04\n",
            "Epoch 107/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6896 - loss: 1.6952 - regression_loss: 10.0459 - val_classification_loss: 0.6910 - val_loss: 0.8164 - val_regression_loss: 1.3031 - learning_rate: 2.5000e-04\n",
            "Epoch 108/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6878 - loss: 1.6865 - regression_loss: 9.9773 - val_classification_loss: 0.6911 - val_loss: 0.8164 - val_regression_loss: 1.3028 - learning_rate: 2.5000e-04\n",
            "Epoch 109/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6885 - loss: 1.6440 - regression_loss: 9.5597 - val_classification_loss: 0.6911 - val_loss: 0.8159 - val_regression_loss: 1.2977 - learning_rate: 2.5000e-04\n",
            "Epoch 110/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6843 - loss: 1.6370 - regression_loss: 9.5257 - val_classification_loss: 0.6911 - val_loss: 0.8164 - val_regression_loss: 1.3028 - learning_rate: 2.5000e-04\n",
            "Epoch 111/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6857 - loss: 1.6442 - regression_loss: 9.5778 - val_classification_loss: 0.6912 - val_loss: 0.8168 - val_regression_loss: 1.3067 - learning_rate: 2.5000e-04\n",
            "Epoch 112/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6845 - loss: 1.6161 - regression_loss: 9.3213 - val_classification_loss: 0.6911 - val_loss: 0.8167 - val_regression_loss: 1.3068 - learning_rate: 2.5000e-04\n",
            "Epoch 113/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6837 - loss: 1.6433 - regression_loss: 9.5948 - val_classification_loss: 0.6911 - val_loss: 0.8184 - val_regression_loss: 1.3244 - learning_rate: 2.5000e-04\n",
            "Epoch 114/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6838 - loss: 1.6916 - regression_loss: 10.0711 - val_classification_loss: 0.6912 - val_loss: 0.8214 - val_regression_loss: 1.3540 - learning_rate: 2.5000e-04\n",
            "Epoch 115/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6880 - loss: 1.6384 - regression_loss: 9.5023 - val_classification_loss: 0.6911 - val_loss: 0.8208 - val_regression_loss: 1.3495 - learning_rate: 2.5000e-04\n",
            "Epoch 116/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6883 - loss: 1.6989 - regression_loss: 10.1003 - val_classification_loss: 0.6911 - val_loss: 0.8183 - val_regression_loss: 1.3226 - learning_rate: 2.5000e-04\n",
            "Epoch 117/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6886 - loss: 1.6301 - regression_loss: 9.4160 - val_classification_loss: 0.6910 - val_loss: 0.8181 - val_regression_loss: 1.3212 - learning_rate: 2.5000e-04\n",
            "Epoch 118/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6890 - loss: 1.6342 - regression_loss: 9.4515 - val_classification_loss: 0.6911 - val_loss: 0.8162 - val_regression_loss: 1.3012 - learning_rate: 2.5000e-04\n",
            "Epoch 119/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6909 - loss: 1.7033 - regression_loss: 10.1197 - val_classification_loss: 0.6911 - val_loss: 0.8172 - val_regression_loss: 1.3117 - learning_rate: 2.5000e-04\n",
            "Epoch 120/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6845 - loss: 1.6691 - regression_loss: 9.8483 - val_classification_loss: 0.6911 - val_loss: 0.8189 - val_regression_loss: 1.3294 - learning_rate: 2.5000e-04\n",
            "Epoch 121/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6857 - loss: 1.6091 - regression_loss: 9.2346 - val_classification_loss: 0.6911 - val_loss: 0.8191 - val_regression_loss: 1.3326 - learning_rate: 2.5000e-04\n",
            "Epoch 122/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - classification_loss: 0.6862 - loss: 1.6306 - regression_loss: 9.4447 - val_classification_loss: 0.6911 - val_loss: 0.8204 - val_regression_loss: 1.3454 - learning_rate: 2.5000e-04\n",
            "Epoch 123/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - classification_loss: 0.6884 - loss: 1.6845 - regression_loss: 9.9578 - val_classification_loss: 0.6910 - val_loss: 0.8199 - val_regression_loss: 1.3408 - learning_rate: 2.5000e-04\n",
            "Epoch 124/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - classification_loss: 0.6865 - loss: 1.7171 - regression_loss: 10.3055 - val_classification_loss: 0.6911 - val_loss: 0.8192 - val_regression_loss: 1.3339 - learning_rate: 2.5000e-04\n",
            "Epoch 125/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6880 - loss: 1.6948 - regression_loss: 10.0621 - val_classification_loss: 0.6911 - val_loss: 0.8213 - val_regression_loss: 1.3554 - learning_rate: 2.5000e-04\n",
            "Epoch 126/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6840 - loss: 1.6899 - regression_loss: 10.0573 - val_classification_loss: 0.6911 - val_loss: 0.8219 - val_regression_loss: 1.3608 - learning_rate: 2.5000e-04\n",
            "Epoch 127/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6860 - loss: 1.6827 - regression_loss: 9.9645 - val_classification_loss: 0.6911 - val_loss: 0.8197 - val_regression_loss: 1.3376 - learning_rate: 2.5000e-04\n",
            "Epoch 128/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6869 - loss: 1.6487 - regression_loss: 9.6047 - val_classification_loss: 0.6911 - val_loss: 0.8199 - val_regression_loss: 1.3401 - learning_rate: 2.5000e-04\n",
            "Epoch 129/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6852 - loss: 1.6679 - regression_loss: 9.8189 - val_classification_loss: 0.6912 - val_loss: 0.8193 - val_regression_loss: 1.3339 - learning_rate: 2.5000e-04\n",
            "Epoch 130/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6886 - loss: 1.6808 - regression_loss: 9.9286 - val_classification_loss: 0.6911 - val_loss: 0.8171 - val_regression_loss: 1.3118 - learning_rate: 2.5000e-04\n",
            "Epoch 131/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6860 - loss: 1.6394 - regression_loss: 9.5374 - val_classification_loss: 0.6911 - val_loss: 0.8188 - val_regression_loss: 1.3296 - learning_rate: 2.5000e-04\n",
            "Epoch 132/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6870 - loss: 1.7190 - regression_loss: 10.3151 - val_classification_loss: 0.6912 - val_loss: 0.8200 - val_regression_loss: 1.3421 - learning_rate: 2.5000e-04\n",
            "Epoch 133/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6855 - loss: 1.6997 - regression_loss: 10.1415 - val_classification_loss: 0.6913 - val_loss: 0.8217 - val_regression_loss: 1.3587 - learning_rate: 2.5000e-04\n",
            "Epoch 134/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6894 - loss: 1.6633 - regression_loss: 9.7343 - val_classification_loss: 0.6913 - val_loss: 0.8210 - val_regression_loss: 1.3522 - learning_rate: 2.5000e-04\n",
            "Epoch 135/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6886 - loss: 1.6513 - regression_loss: 9.6359 - val_classification_loss: 0.6912 - val_loss: 0.8198 - val_regression_loss: 1.3407 - learning_rate: 2.5000e-04\n",
            "Epoch 136/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6883 - loss: 1.6522 - regression_loss: 9.6351 - val_classification_loss: 0.6912 - val_loss: 0.8187 - val_regression_loss: 1.3283 - learning_rate: 2.5000e-04\n",
            "Epoch 137/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6855 - loss: 1.6758 - regression_loss: 9.8905 - val_classification_loss: 0.6912 - val_loss: 0.8169 - val_regression_loss: 1.3083 - learning_rate: 2.5000e-04\n",
            "Epoch 138/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6886 - loss: 1.5976 - regression_loss: 9.0869 - val_classification_loss: 0.6913 - val_loss: 0.8191 - val_regression_loss: 1.3326 - learning_rate: 2.5000e-04\n",
            "Epoch 139/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6854 - loss: 1.6949 - regression_loss: 10.0995 - val_classification_loss: 0.6914 - val_loss: 0.8235 - val_regression_loss: 1.3788 - learning_rate: 2.5000e-04\n",
            "Epoch 140/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6865 - loss: 1.6716 - regression_loss: 9.8577 - val_classification_loss: 0.6914 - val_loss: 0.8233 - val_regression_loss: 1.3764 - learning_rate: 2.5000e-04\n",
            "Epoch 141/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6870 - loss: 1.6841 - regression_loss: 9.9694 - val_classification_loss: 0.6914 - val_loss: 0.8228 - val_regression_loss: 1.3692 - learning_rate: 2.5000e-04\n",
            "Epoch 142/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6851 - loss: 1.6441 - regression_loss: 9.5827 - val_classification_loss: 0.6914 - val_loss: 0.8183 - val_regression_loss: 1.3218 - learning_rate: 2.5000e-04\n",
            "Epoch 143/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6871 - loss: 1.6189 - regression_loss: 9.3230 - val_classification_loss: 0.6913 - val_loss: 0.8171 - val_regression_loss: 1.3094 - learning_rate: 2.5000e-04\n",
            "Epoch 144/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - classification_loss: 0.6874 - loss: 1.6431 - regression_loss: 9.5640 - val_classification_loss: 0.6913 - val_loss: 0.8166 - val_regression_loss: 1.3036 - learning_rate: 2.5000e-04\n",
            "Epoch 145/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - classification_loss: 0.6879 - loss: 1.6134 - regression_loss: 9.2580 - val_classification_loss: 0.6913 - val_loss: 0.8175 - val_regression_loss: 1.3140 - learning_rate: 2.5000e-04\n",
            "Epoch 146/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step - classification_loss: 0.6856 - loss: 1.6264 - regression_loss: 9.4161 - val_classification_loss: 0.6913 - val_loss: 0.8180 - val_regression_loss: 1.3182 - learning_rate: 2.5000e-04\n",
            "Epoch 147/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6897 - loss: 1.6782 - regression_loss: 9.8909 - val_classification_loss: 0.6914 - val_loss: 0.8188 - val_regression_loss: 1.3261 - learning_rate: 2.5000e-04\n",
            "Epoch 148/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6896 - loss: 1.6355 - regression_loss: 9.4624 - val_classification_loss: 0.6912 - val_loss: 0.8162 - val_regression_loss: 1.2993 - learning_rate: 2.5000e-04\n",
            "Epoch 149/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6866 - loss: 1.6597 - regression_loss: 9.7317 - val_classification_loss: 0.6912 - val_loss: 0.8157 - val_regression_loss: 1.2941 - learning_rate: 2.5000e-04\n",
            "Epoch 150/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6873 - loss: 1.6761 - regression_loss: 9.8839 - val_classification_loss: 0.6914 - val_loss: 0.8164 - val_regression_loss: 1.3008 - learning_rate: 2.5000e-04\n",
            "Epoch 151/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6860 - loss: 1.6288 - regression_loss: 9.4400 - val_classification_loss: 0.6913 - val_loss: 0.8162 - val_regression_loss: 1.2988 - learning_rate: 1.2500e-04\n",
            "Epoch 152/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6866 - loss: 1.6562 - regression_loss: 9.6854 - val_classification_loss: 0.6914 - val_loss: 0.8175 - val_regression_loss: 1.3124 - learning_rate: 1.2500e-04\n",
            "Epoch 153/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6881 - loss: 1.6359 - regression_loss: 9.4726 - val_classification_loss: 0.6914 - val_loss: 0.8180 - val_regression_loss: 1.3177 - learning_rate: 1.2500e-04\n",
            "Epoch 154/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6873 - loss: 1.7303 - regression_loss: 10.4336 - val_classification_loss: 0.6914 - val_loss: 0.8181 - val_regression_loss: 1.3187 - learning_rate: 1.2500e-04\n",
            "Epoch 155/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6866 - loss: 1.6789 - regression_loss: 9.9167 - val_classification_loss: 0.6914 - val_loss: 0.8171 - val_regression_loss: 1.3085 - learning_rate: 1.2500e-04\n",
            "Epoch 156/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6873 - loss: 1.6367 - regression_loss: 9.4943 - val_classification_loss: 0.6914 - val_loss: 0.8168 - val_regression_loss: 1.3051 - learning_rate: 1.2500e-04\n",
            "Epoch 157/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6880 - loss: 1.6099 - regression_loss: 9.2109 - val_classification_loss: 0.6913 - val_loss: 0.8172 - val_regression_loss: 1.3095 - learning_rate: 1.2500e-04\n",
            "Epoch 158/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6869 - loss: 1.6976 - regression_loss: 10.0984 - val_classification_loss: 0.6913 - val_loss: 0.8170 - val_regression_loss: 1.3076 - learning_rate: 1.2500e-04\n",
            "Epoch 159/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6869 - loss: 1.6079 - regression_loss: 9.2165 - val_classification_loss: 0.6913 - val_loss: 0.8172 - val_regression_loss: 1.3103 - learning_rate: 1.2500e-04\n",
            "Epoch 160/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6861 - loss: 1.6668 - regression_loss: 9.8077 - val_classification_loss: 0.6913 - val_loss: 0.8169 - val_regression_loss: 1.3071 - learning_rate: 1.2500e-04\n",
            "Epoch 161/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - classification_loss: 0.6874 - loss: 1.6712 - regression_loss: 9.8416 - val_classification_loss: 0.6913 - val_loss: 0.8164 - val_regression_loss: 1.3018 - learning_rate: 1.2500e-04\n",
            "Epoch 162/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6858 - loss: 1.6806 - regression_loss: 9.9577 - val_classification_loss: 0.6913 - val_loss: 0.8159 - val_regression_loss: 1.2968 - learning_rate: 1.2500e-04\n",
            "Epoch 163/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6878 - loss: 1.6357 - regression_loss: 9.4906 - val_classification_loss: 0.6913 - val_loss: 0.8169 - val_regression_loss: 1.3069 - learning_rate: 1.2500e-04\n",
            "Epoch 164/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6877 - loss: 1.6016 - regression_loss: 9.1401 - val_classification_loss: 0.6913 - val_loss: 0.8166 - val_regression_loss: 1.3043 - learning_rate: 1.2500e-04\n",
            "Epoch 165/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6859 - loss: 1.6630 - regression_loss: 9.7593 - val_classification_loss: 0.6912 - val_loss: 0.8158 - val_regression_loss: 1.2953 - learning_rate: 1.2500e-04\n",
            "Epoch 166/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6871 - loss: 1.6367 - regression_loss: 9.4921 - val_classification_loss: 0.6912 - val_loss: 0.8163 - val_regression_loss: 1.3010 - learning_rate: 1.2500e-04\n",
            "Epoch 167/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - classification_loss: 0.6859 - loss: 1.6038 - regression_loss: 9.1802 - val_classification_loss: 0.6913 - val_loss: 0.8178 - val_regression_loss: 1.3162 - learning_rate: 1.2500e-04\n",
            "Epoch 168/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6845 - loss: 1.6625 - regression_loss: 9.7760 - val_classification_loss: 0.6913 - val_loss: 0.8184 - val_regression_loss: 1.3238 - learning_rate: 1.2500e-04\n",
            "Epoch 169/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - classification_loss: 0.6868 - loss: 1.6641 - regression_loss: 9.7683 - val_classification_loss: 0.6912 - val_loss: 0.8168 - val_regression_loss: 1.3072 - learning_rate: 1.2500e-04\n",
            "Epoch 170/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - classification_loss: 0.6872 - loss: 1.6339 - regression_loss: 9.4707 - val_classification_loss: 0.6912 - val_loss: 0.8171 - val_regression_loss: 1.3104 - learning_rate: 1.2500e-04\n",
            "Epoch 171/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6881 - loss: 1.6866 - regression_loss: 9.9905 - val_classification_loss: 0.6912 - val_loss: 0.8172 - val_regression_loss: 1.3122 - learning_rate: 1.2500e-04\n",
            "Epoch 172/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6850 - loss: 1.6991 - regression_loss: 10.1320 - val_classification_loss: 0.6912 - val_loss: 0.8169 - val_regression_loss: 1.3083 - learning_rate: 1.2500e-04\n",
            "Epoch 173/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6860 - loss: 1.6680 - regression_loss: 9.8206 - val_classification_loss: 0.6912 - val_loss: 0.8169 - val_regression_loss: 1.3081 - learning_rate: 1.2500e-04\n",
            "Epoch 174/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6853 - loss: 1.6235 - regression_loss: 9.3914 - val_classification_loss: 0.6912 - val_loss: 0.8167 - val_regression_loss: 1.3066 - learning_rate: 1.2500e-04\n",
            "Epoch 175/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6881 - loss: 1.6842 - regression_loss: 9.9554 - val_classification_loss: 0.6912 - val_loss: 0.8171 - val_regression_loss: 1.3104 - learning_rate: 1.2500e-04\n",
            "Epoch 176/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6889 - loss: 1.6948 - regression_loss: 10.0654 - val_classification_loss: 0.6912 - val_loss: 0.8176 - val_regression_loss: 1.3158 - learning_rate: 1.2500e-04\n",
            "Epoch 177/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6856 - loss: 1.6657 - regression_loss: 9.8125 - val_classification_loss: 0.6912 - val_loss: 0.8173 - val_regression_loss: 1.3125 - learning_rate: 1.2500e-04\n",
            "Epoch 178/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6852 - loss: 1.5914 - regression_loss: 9.0633 - val_classification_loss: 0.6912 - val_loss: 0.8178 - val_regression_loss: 1.3188 - learning_rate: 1.2500e-04\n",
            "Epoch 179/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6873 - loss: 1.6989 - regression_loss: 10.1148 - val_classification_loss: 0.6913 - val_loss: 0.8187 - val_regression_loss: 1.3276 - learning_rate: 1.2500e-04\n",
            "Epoch 180/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - classification_loss: 0.6871 - loss: 1.7224 - regression_loss: 10.3520 - val_classification_loss: 0.6913 - val_loss: 0.8179 - val_regression_loss: 1.3194 - learning_rate: 1.2500e-04\n",
            "Epoch 181/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6873 - loss: 1.6450 - regression_loss: 9.5788 - val_classification_loss: 0.6913 - val_loss: 0.8179 - val_regression_loss: 1.3194 - learning_rate: 1.2500e-04\n",
            "Epoch 182/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6887 - loss: 1.6537 - regression_loss: 9.6560 - val_classification_loss: 0.6913 - val_loss: 0.8173 - val_regression_loss: 1.3119 - learning_rate: 1.2500e-04\n",
            "Epoch 183/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6857 - loss: 1.6224 - regression_loss: 9.3715 - val_classification_loss: 0.6913 - val_loss: 0.8172 - val_regression_loss: 1.3107 - learning_rate: 1.2500e-04\n",
            "Epoch 184/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6873 - loss: 1.7062 - regression_loss: 10.1823 - val_classification_loss: 0.6913 - val_loss: 0.8187 - val_regression_loss: 1.3274 - learning_rate: 1.2500e-04\n",
            "Epoch 185/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6863 - loss: 1.5854 - regression_loss: 8.9948 - val_classification_loss: 0.6914 - val_loss: 0.8192 - val_regression_loss: 1.3326 - learning_rate: 1.2500e-04\n",
            "Epoch 186/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6863 - loss: 1.6448 - regression_loss: 9.5868 - val_classification_loss: 0.6914 - val_loss: 0.8189 - val_regression_loss: 1.3294 - learning_rate: 1.2500e-04\n",
            "Epoch 187/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6871 - loss: 1.6382 - regression_loss: 9.5123 - val_classification_loss: 0.6914 - val_loss: 0.8184 - val_regression_loss: 1.3240 - learning_rate: 1.2500e-04\n",
            "Epoch 188/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step - classification_loss: 0.6884 - loss: 1.6614 - regression_loss: 9.7342 - val_classification_loss: 0.6914 - val_loss: 0.8181 - val_regression_loss: 1.3199 - learning_rate: 1.2500e-04\n",
            "Epoch 189/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - classification_loss: 0.6866 - loss: 1.6771 - regression_loss: 9.9014 - val_classification_loss: 0.6914 - val_loss: 0.8183 - val_regression_loss: 1.3218 - learning_rate: 1.2500e-04\n",
            "Epoch 190/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - classification_loss: 0.6875 - loss: 1.6434 - regression_loss: 9.5559 - val_classification_loss: 0.6914 - val_loss: 0.8193 - val_regression_loss: 1.3319 - learning_rate: 1.2500e-04\n",
            "Epoch 191/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6880 - loss: 1.6287 - regression_loss: 9.4170 - val_classification_loss: 0.6915 - val_loss: 0.8192 - val_regression_loss: 1.3311 - learning_rate: 1.2500e-04\n",
            "Epoch 192/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6878 - loss: 1.6475 - regression_loss: 9.5903 - val_classification_loss: 0.6915 - val_loss: 0.8192 - val_regression_loss: 1.3310 - learning_rate: 1.2500e-04\n",
            "Epoch 193/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6884 - loss: 1.6551 - regression_loss: 9.6788 - val_classification_loss: 0.6914 - val_loss: 0.8173 - val_regression_loss: 1.3108 - learning_rate: 1.2500e-04\n",
            "Epoch 194/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6863 - loss: 1.7085 - regression_loss: 10.2193 - val_classification_loss: 0.6915 - val_loss: 0.8174 - val_regression_loss: 1.3110 - learning_rate: 1.2500e-04\n",
            "Epoch 195/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6845 - loss: 1.6426 - regression_loss: 9.5847 - val_classification_loss: 0.6915 - val_loss: 0.8181 - val_regression_loss: 1.3189 - learning_rate: 1.2500e-04\n",
            "Epoch 196/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6881 - loss: 1.7182 - regression_loss: 10.2986 - val_classification_loss: 0.6915 - val_loss: 0.8200 - val_regression_loss: 1.3400 - learning_rate: 1.2500e-04\n",
            "Epoch 197/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6872 - loss: 1.6574 - regression_loss: 9.6942 - val_classification_loss: 0.6914 - val_loss: 0.8183 - val_regression_loss: 1.3214 - learning_rate: 1.2500e-04\n",
            "Epoch 198/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6872 - loss: 1.6675 - regression_loss: 9.8036 - val_classification_loss: 0.6914 - val_loss: 0.8173 - val_regression_loss: 1.3111 - learning_rate: 1.2500e-04\n",
            "Epoch 199/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6882 - loss: 1.7108 - regression_loss: 10.2247 - val_classification_loss: 0.6914 - val_loss: 0.8172 - val_regression_loss: 1.3104 - learning_rate: 1.2500e-04\n",
            "Epoch 200/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6862 - loss: 1.6498 - regression_loss: 9.6389 - val_classification_loss: 0.6914 - val_loss: 0.8167 - val_regression_loss: 1.3047 - learning_rate: 1.2500e-04\n",
            "Epoch 201/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6863 - loss: 1.5828 - regression_loss: 8.9718 - val_classification_loss: 0.6913 - val_loss: 0.8169 - val_regression_loss: 1.3072 - learning_rate: 6.2500e-05\n",
            "Epoch 202/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6848 - loss: 1.7313 - regression_loss: 10.4646 - val_classification_loss: 0.6913 - val_loss: 0.8170 - val_regression_loss: 1.3077 - learning_rate: 6.2500e-05\n",
            "Epoch 203/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6904 - loss: 1.6759 - regression_loss: 9.8588 - val_classification_loss: 0.6913 - val_loss: 0.8171 - val_regression_loss: 1.3096 - learning_rate: 6.2500e-05\n",
            "Epoch 204/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6873 - loss: 1.6733 - regression_loss: 9.8514 - val_classification_loss: 0.6913 - val_loss: 0.8170 - val_regression_loss: 1.3083 - learning_rate: 6.2500e-05\n",
            "Epoch 205/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6862 - loss: 1.6524 - regression_loss: 9.6571 - val_classification_loss: 0.6914 - val_loss: 0.8174 - val_regression_loss: 1.3117 - learning_rate: 6.2500e-05\n",
            "Epoch 206/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6888 - loss: 1.6485 - regression_loss: 9.5961 - val_classification_loss: 0.6913 - val_loss: 0.8171 - val_regression_loss: 1.3095 - learning_rate: 6.2500e-05\n",
            "Epoch 207/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6888 - loss: 1.7086 - regression_loss: 10.1935 - val_classification_loss: 0.6913 - val_loss: 0.8169 - val_regression_loss: 1.3068 - learning_rate: 6.2500e-05\n",
            "Epoch 208/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6871 - loss: 1.6938 - regression_loss: 10.0635 - val_classification_loss: 0.6913 - val_loss: 0.8172 - val_regression_loss: 1.3103 - learning_rate: 6.2500e-05\n",
            "Epoch 209/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6861 - loss: 1.6155 - regression_loss: 9.2966 - val_classification_loss: 0.6913 - val_loss: 0.8172 - val_regression_loss: 1.3102 - learning_rate: 6.2500e-05\n",
            "Epoch 210/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - classification_loss: 0.6849 - loss: 1.6984 - regression_loss: 10.1312 - val_classification_loss: 0.6913 - val_loss: 0.8174 - val_regression_loss: 1.3125 - learning_rate: 6.2500e-05\n",
            "Epoch 211/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - classification_loss: 0.6864 - loss: 1.6115 - regression_loss: 9.2525 - val_classification_loss: 0.6913 - val_loss: 0.8173 - val_regression_loss: 1.3119 - learning_rate: 6.2500e-05\n",
            "Epoch 212/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6860 - loss: 1.6071 - regression_loss: 9.2136 - val_classification_loss: 0.6914 - val_loss: 0.8173 - val_regression_loss: 1.3110 - learning_rate: 6.2500e-05\n",
            "Epoch 213/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6895 - loss: 1.7151 - regression_loss: 10.2485 - val_classification_loss: 0.6914 - val_loss: 0.8174 - val_regression_loss: 1.3122 - learning_rate: 6.2500e-05\n",
            "Epoch 214/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6869 - loss: 1.6716 - regression_loss: 9.8450 - val_classification_loss: 0.6914 - val_loss: 0.8179 - val_regression_loss: 1.3177 - learning_rate: 6.2500e-05\n",
            "Epoch 215/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6892 - loss: 1.6688 - regression_loss: 9.7965 - val_classification_loss: 0.6914 - val_loss: 0.8180 - val_regression_loss: 1.3186 - learning_rate: 6.2500e-05\n",
            "Epoch 216/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6860 - loss: 1.6609 - regression_loss: 9.7484 - val_classification_loss: 0.6914 - val_loss: 0.8181 - val_regression_loss: 1.3199 - learning_rate: 6.2500e-05\n",
            "Epoch 217/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6853 - loss: 1.6169 - regression_loss: 9.3202 - val_classification_loss: 0.6914 - val_loss: 0.8183 - val_regression_loss: 1.3220 - learning_rate: 6.2500e-05\n",
            "Epoch 218/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - classification_loss: 0.6856 - loss: 1.6124 - regression_loss: 9.2807 - val_classification_loss: 0.6915 - val_loss: 0.8182 - val_regression_loss: 1.3201 - learning_rate: 6.2500e-05\n",
            "Epoch 219/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6835 - loss: 1.6875 - regression_loss: 10.0386 - val_classification_loss: 0.6915 - val_loss: 0.8176 - val_regression_loss: 1.3138 - learning_rate: 6.2500e-05\n",
            "Epoch 220/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6848 - loss: 1.6585 - regression_loss: 9.7396 - val_classification_loss: 0.6915 - val_loss: 0.8175 - val_regression_loss: 1.3120 - learning_rate: 6.2500e-05\n",
            "Epoch 221/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6858 - loss: 1.5949 - regression_loss: 9.0894 - val_classification_loss: 0.6915 - val_loss: 0.8178 - val_regression_loss: 1.3161 - learning_rate: 6.2500e-05\n",
            "Epoch 222/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - classification_loss: 0.6868 - loss: 1.6082 - regression_loss: 9.2204 - val_classification_loss: 0.6915 - val_loss: 0.8183 - val_regression_loss: 1.3216 - learning_rate: 6.2500e-05\n",
            "Epoch 223/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 64ms/step - classification_loss: 0.6866 - loss: 1.6276 - regression_loss: 9.4154 - val_classification_loss: 0.6915 - val_loss: 0.8181 - val_regression_loss: 1.3195 - learning_rate: 6.2500e-05\n",
            "Epoch 224/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6863 - loss: 1.6660 - regression_loss: 9.8036 - val_classification_loss: 0.6915 - val_loss: 0.8186 - val_regression_loss: 1.3244 - learning_rate: 6.2500e-05\n",
            "Epoch 225/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6876 - loss: 1.6653 - regression_loss: 9.7715 - val_classification_loss: 0.6915 - val_loss: 0.8184 - val_regression_loss: 1.3222 - learning_rate: 6.2500e-05\n",
            "Epoch 226/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6843 - loss: 1.6967 - regression_loss: 10.1228 - val_classification_loss: 0.6915 - val_loss: 0.8181 - val_regression_loss: 1.3192 - learning_rate: 6.2500e-05\n",
            "Epoch 227/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6880 - loss: 1.6106 - regression_loss: 9.2242 - val_classification_loss: 0.6915 - val_loss: 0.8173 - val_regression_loss: 1.3109 - learning_rate: 6.2500e-05\n",
            "Epoch 228/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6888 - loss: 1.6534 - regression_loss: 9.6417 - val_classification_loss: 0.6914 - val_loss: 0.8170 - val_regression_loss: 1.3074 - learning_rate: 6.2500e-05\n",
            "Epoch 229/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6840 - loss: 1.6742 - regression_loss: 9.8976 - val_classification_loss: 0.6914 - val_loss: 0.8170 - val_regression_loss: 1.3076 - learning_rate: 6.2500e-05\n",
            "Epoch 230/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - classification_loss: 0.6897 - loss: 1.6133 - regression_loss: 9.2437 - val_classification_loss: 0.6914 - val_loss: 0.8174 - val_regression_loss: 1.3121 - learning_rate: 6.2500e-05\n",
            "Epoch 231/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6844 - loss: 1.6890 - regression_loss: 10.0428 - val_classification_loss: 0.6914 - val_loss: 0.8175 - val_regression_loss: 1.3124 - learning_rate: 6.2500e-05\n",
            "Epoch 232/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - classification_loss: 0.6864 - loss: 1.6469 - regression_loss: 9.6010 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3143 - learning_rate: 6.2500e-05\n",
            "Epoch 233/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6857 - loss: 1.6888 - regression_loss: 10.0324 - val_classification_loss: 0.6914 - val_loss: 0.8175 - val_regression_loss: 1.3127 - learning_rate: 6.2500e-05\n",
            "Epoch 234/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6852 - loss: 1.6933 - regression_loss: 10.0756 - val_classification_loss: 0.6914 - val_loss: 0.8165 - val_regression_loss: 1.3017 - learning_rate: 6.2500e-05\n",
            "Epoch 235/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6842 - loss: 1.6305 - regression_loss: 9.4602 - val_classification_loss: 0.6914 - val_loss: 0.8165 - val_regression_loss: 1.3014 - learning_rate: 6.2500e-05\n",
            "Epoch 236/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6845 - loss: 1.6283 - regression_loss: 9.4426 - val_classification_loss: 0.6915 - val_loss: 0.8168 - val_regression_loss: 1.3052 - learning_rate: 6.2500e-05\n",
            "Epoch 237/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6871 - loss: 1.6756 - regression_loss: 9.8903 - val_classification_loss: 0.6915 - val_loss: 0.8171 - val_regression_loss: 1.3085 - learning_rate: 6.2500e-05\n",
            "Epoch 238/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6853 - loss: 1.6332 - regression_loss: 9.4723 - val_classification_loss: 0.6915 - val_loss: 0.8175 - val_regression_loss: 1.3126 - learning_rate: 6.2500e-05\n",
            "Epoch 239/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6864 - loss: 1.6859 - regression_loss: 9.9968 - val_classification_loss: 0.6915 - val_loss: 0.8175 - val_regression_loss: 1.3127 - learning_rate: 6.2500e-05\n",
            "Epoch 240/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6867 - loss: 1.6698 - regression_loss: 9.8347 - val_classification_loss: 0.6914 - val_loss: 0.8174 - val_regression_loss: 1.3120 - learning_rate: 6.2500e-05\n",
            "Epoch 241/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6866 - loss: 1.6488 - regression_loss: 9.6219 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3137 - learning_rate: 6.2500e-05\n",
            "Epoch 242/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6873 - loss: 1.5648 - regression_loss: 8.7889 - val_classification_loss: 0.6914 - val_loss: 0.8177 - val_regression_loss: 1.3149 - learning_rate: 6.2500e-05\n",
            "Epoch 243/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6876 - loss: 1.6544 - regression_loss: 9.6700 - val_classification_loss: 0.6914 - val_loss: 0.8175 - val_regression_loss: 1.3135 - learning_rate: 6.2500e-05\n",
            "Epoch 244/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6864 - loss: 1.6774 - regression_loss: 9.9139 - val_classification_loss: 0.6914 - val_loss: 0.8179 - val_regression_loss: 1.3178 - learning_rate: 6.2500e-05\n",
            "Epoch 245/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6857 - loss: 1.6708 - regression_loss: 9.8454 - val_classification_loss: 0.6914 - val_loss: 0.8174 - val_regression_loss: 1.3128 - learning_rate: 6.2500e-05\n",
            "Epoch 246/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6854 - loss: 1.6431 - regression_loss: 9.5726 - val_classification_loss: 0.6913 - val_loss: 0.8170 - val_regression_loss: 1.3084 - learning_rate: 6.2500e-05\n",
            "Epoch 247/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6858 - loss: 1.6410 - regression_loss: 9.5434 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3145 - learning_rate: 6.2500e-05\n",
            "Epoch 248/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6866 - loss: 1.6940 - regression_loss: 10.0726 - val_classification_loss: 0.6914 - val_loss: 0.8180 - val_regression_loss: 1.3193 - learning_rate: 6.2500e-05\n",
            "Epoch 249/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6891 - loss: 1.6116 - regression_loss: 9.2299 - val_classification_loss: 0.6914 - val_loss: 0.8181 - val_regression_loss: 1.3196 - learning_rate: 6.2500e-05\n",
            "Epoch 250/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6869 - loss: 1.6710 - regression_loss: 9.8456 - val_classification_loss: 0.6914 - val_loss: 0.8178 - val_regression_loss: 1.3171 - learning_rate: 6.2500e-05\n",
            "Epoch 251/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6882 - loss: 1.6371 - regression_loss: 9.4912 - val_classification_loss: 0.6914 - val_loss: 0.8180 - val_regression_loss: 1.3181 - learning_rate: 3.1250e-05\n",
            "Epoch 252/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - classification_loss: 0.6882 - loss: 1.6366 - regression_loss: 9.4824 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3146 - learning_rate: 3.1250e-05\n",
            "Epoch 253/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - classification_loss: 0.6872 - loss: 1.6714 - regression_loss: 9.8406 - val_classification_loss: 0.6914 - val_loss: 0.8173 - val_regression_loss: 1.3109 - learning_rate: 3.1250e-05\n",
            "Epoch 254/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6865 - loss: 1.6668 - regression_loss: 9.8003 - val_classification_loss: 0.6914 - val_loss: 0.8172 - val_regression_loss: 1.3099 - learning_rate: 3.1250e-05\n",
            "Epoch 255/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - classification_loss: 0.6885 - loss: 1.6821 - regression_loss: 9.9464 - val_classification_loss: 0.6914 - val_loss: 0.8172 - val_regression_loss: 1.3098 - learning_rate: 3.1250e-05\n",
            "Epoch 256/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6842 - loss: 1.6066 - regression_loss: 9.2325 - val_classification_loss: 0.6914 - val_loss: 0.8173 - val_regression_loss: 1.3111 - learning_rate: 3.1250e-05\n",
            "Epoch 257/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6861 - loss: 1.6187 - regression_loss: 9.3186 - val_classification_loss: 0.6914 - val_loss: 0.8172 - val_regression_loss: 1.3097 - learning_rate: 3.1250e-05\n",
            "Epoch 258/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6857 - loss: 1.6787 - regression_loss: 9.9243 - val_classification_loss: 0.6914 - val_loss: 0.8172 - val_regression_loss: 1.3101 - learning_rate: 3.1250e-05\n",
            "Epoch 259/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6841 - loss: 1.7258 - regression_loss: 10.4093 - val_classification_loss: 0.6914 - val_loss: 0.8174 - val_regression_loss: 1.3118 - learning_rate: 3.1250e-05\n",
            "Epoch 260/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6888 - loss: 1.6759 - regression_loss: 9.8775 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3139 - learning_rate: 3.1250e-05\n",
            "Epoch 261/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6887 - loss: 1.6117 - regression_loss: 9.2255 - val_classification_loss: 0.6914 - val_loss: 0.8177 - val_regression_loss: 1.3152 - learning_rate: 3.1250e-05\n",
            "Epoch 262/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6848 - loss: 1.6834 - regression_loss: 9.9784 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3144 - learning_rate: 3.1250e-05\n",
            "Epoch 263/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6873 - loss: 1.6608 - regression_loss: 9.7295 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3137 - learning_rate: 3.1250e-05\n",
            "Epoch 264/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6860 - loss: 1.6574 - regression_loss: 9.7066 - val_classification_loss: 0.6914 - val_loss: 0.8179 - val_regression_loss: 1.3173 - learning_rate: 3.1250e-05\n",
            "Epoch 265/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6873 - loss: 1.6577 - regression_loss: 9.7010 - val_classification_loss: 0.6914 - val_loss: 0.8181 - val_regression_loss: 1.3195 - learning_rate: 3.1250e-05\n",
            "Epoch 266/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6860 - loss: 1.6580 - regression_loss: 9.7096 - val_classification_loss: 0.6914 - val_loss: 0.8179 - val_regression_loss: 1.3177 - learning_rate: 3.1250e-05\n",
            "Epoch 267/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6887 - loss: 1.7140 - regression_loss: 10.2448 - val_classification_loss: 0.6914 - val_loss: 0.8177 - val_regression_loss: 1.3156 - learning_rate: 3.1250e-05\n",
            "Epoch 268/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6878 - loss: 1.6628 - regression_loss: 9.7510 - val_classification_loss: 0.6914 - val_loss: 0.8175 - val_regression_loss: 1.3130 - learning_rate: 3.1250e-05\n",
            "Epoch 269/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - classification_loss: 0.6844 - loss: 1.6522 - regression_loss: 9.6820 - val_classification_loss: 0.6914 - val_loss: 0.8173 - val_regression_loss: 1.3104 - learning_rate: 3.1250e-05\n",
            "Epoch 270/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6867 - loss: 1.6704 - regression_loss: 9.8395 - val_classification_loss: 0.6914 - val_loss: 0.8174 - val_regression_loss: 1.3116 - learning_rate: 3.1250e-05\n",
            "Epoch 271/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6874 - loss: 1.6397 - regression_loss: 9.5172 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3135 - learning_rate: 3.1250e-05\n",
            "Epoch 272/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6860 - loss: 1.6729 - regression_loss: 9.8695 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3135 - learning_rate: 3.1250e-05\n",
            "Epoch 273/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6847 - loss: 1.6897 - regression_loss: 10.0448 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3136 - learning_rate: 3.1250e-05\n",
            "Epoch 274/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6866 - loss: 1.6479 - regression_loss: 9.6125 - val_classification_loss: 0.6914 - val_loss: 0.8177 - val_regression_loss: 1.3145 - learning_rate: 3.1250e-05\n",
            "Epoch 275/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - classification_loss: 0.6861 - loss: 1.6246 - regression_loss: 9.3773 - val_classification_loss: 0.6914 - val_loss: 0.8178 - val_regression_loss: 1.3154 - learning_rate: 3.1250e-05\n",
            "Epoch 276/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - classification_loss: 0.6828 - loss: 1.6276 - regression_loss: 9.4480 - val_classification_loss: 0.6914 - val_loss: 0.8176 - val_regression_loss: 1.3140 - learning_rate: 3.1250e-05\n",
            "Epoch 277/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - classification_loss: 0.6869 - loss: 1.6842 - regression_loss: 9.9715 - val_classification_loss: 0.6914 - val_loss: 0.8177 - val_regression_loss: 1.3146 - learning_rate: 3.1250e-05\n",
            "Epoch 278/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - classification_loss: 0.6856 - loss: 1.6588 - regression_loss: 9.7244 - val_classification_loss: 0.6915 - val_loss: 0.8177 - val_regression_loss: 1.3145 - learning_rate: 3.1250e-05\n",
            "Epoch 279/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - classification_loss: 0.6863 - loss: 1.6113 - regression_loss: 9.2584 - val_classification_loss: 0.6915 - val_loss: 0.8178 - val_regression_loss: 1.3163 - learning_rate: 3.1250e-05\n",
            "Epoch 280/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6875 - loss: 1.5940 - regression_loss: 9.0757 - val_classification_loss: 0.6915 - val_loss: 0.8177 - val_regression_loss: 1.3147 - learning_rate: 3.1250e-05\n",
            "Epoch 281/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6840 - loss: 1.6299 - regression_loss: 9.4606 - val_classification_loss: 0.6915 - val_loss: 0.8176 - val_regression_loss: 1.3136 - learning_rate: 3.1250e-05\n",
            "Epoch 282/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6863 - loss: 1.6587 - regression_loss: 9.7291 - val_classification_loss: 0.6915 - val_loss: 0.8177 - val_regression_loss: 1.3153 - learning_rate: 3.1250e-05\n",
            "Epoch 283/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6854 - loss: 1.6643 - regression_loss: 9.7883 - val_classification_loss: 0.6914 - val_loss: 0.8177 - val_regression_loss: 1.3152 - learning_rate: 3.1250e-05\n",
            "Epoch 284/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6853 - loss: 1.6345 - regression_loss: 9.4968 - val_classification_loss: 0.6914 - val_loss: 0.8178 - val_regression_loss: 1.3160 - learning_rate: 3.1250e-05\n",
            "Epoch 285/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6865 - loss: 1.6241 - regression_loss: 9.3785 - val_classification_loss: 0.6914 - val_loss: 0.8177 - val_regression_loss: 1.3151 - learning_rate: 3.1250e-05\n",
            "Epoch 286/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6823 - loss: 1.6587 - regression_loss: 9.7491 - val_classification_loss: 0.6914 - val_loss: 0.8177 - val_regression_loss: 1.3148 - learning_rate: 3.1250e-05\n",
            "Epoch 287/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6843 - loss: 1.6397 - regression_loss: 9.5572 - val_classification_loss: 0.6914 - val_loss: 0.8177 - val_regression_loss: 1.3154 - learning_rate: 3.1250e-05\n",
            "Epoch 288/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6862 - loss: 1.6405 - regression_loss: 9.5371 - val_classification_loss: 0.6914 - val_loss: 0.8175 - val_regression_loss: 1.3130 - learning_rate: 3.1250e-05\n",
            "Epoch 289/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6860 - loss: 1.6521 - regression_loss: 9.6607 - val_classification_loss: 0.6915 - val_loss: 0.8176 - val_regression_loss: 1.3140 - learning_rate: 3.1250e-05\n",
            "Epoch 290/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - classification_loss: 0.6849 - loss: 1.6405 - regression_loss: 9.5570 - val_classification_loss: 0.6915 - val_loss: 0.8176 - val_regression_loss: 1.3137 - learning_rate: 3.1250e-05\n",
            "Epoch 291/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6844 - loss: 1.6485 - regression_loss: 9.6461 - val_classification_loss: 0.6915 - val_loss: 0.8177 - val_regression_loss: 1.3141 - learning_rate: 3.1250e-05\n",
            "Epoch 292/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6878 - loss: 1.6216 - regression_loss: 9.3379 - val_classification_loss: 0.6915 - val_loss: 0.8180 - val_regression_loss: 1.3175 - learning_rate: 3.1250e-05\n",
            "Epoch 293/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6882 - loss: 1.6368 - regression_loss: 9.4767 - val_classification_loss: 0.6915 - val_loss: 0.8178 - val_regression_loss: 1.3158 - learning_rate: 3.1250e-05\n",
            "Epoch 294/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6873 - loss: 1.6616 - regression_loss: 9.7445 - val_classification_loss: 0.6915 - val_loss: 0.8177 - val_regression_loss: 1.3147 - learning_rate: 3.1250e-05\n",
            "Epoch 295/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6868 - loss: 1.6222 - regression_loss: 9.3571 - val_classification_loss: 0.6915 - val_loss: 0.8179 - val_regression_loss: 1.3174 - learning_rate: 3.1250e-05\n",
            "Epoch 296/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - classification_loss: 0.6890 - loss: 1.6489 - regression_loss: 9.5980 - val_classification_loss: 0.6915 - val_loss: 0.8178 - val_regression_loss: 1.3161 - learning_rate: 3.1250e-05\n",
            "Epoch 297/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6849 - loss: 1.6991 - regression_loss: 10.1295 - val_classification_loss: 0.6915 - val_loss: 0.8177 - val_regression_loss: 1.3143 - learning_rate: 3.1250e-05\n",
            "Epoch 298/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - classification_loss: 0.6877 - loss: 1.7250 - regression_loss: 10.3644 - val_classification_loss: 0.6915 - val_loss: 0.8176 - val_regression_loss: 1.3133 - learning_rate: 3.1250e-05\n",
            "Epoch 299/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6883 - loss: 1.6643 - regression_loss: 9.7601 - val_classification_loss: 0.6914 - val_loss: 0.8174 - val_regression_loss: 1.3117 - learning_rate: 3.1250e-05\n",
            "Epoch 300/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - classification_loss: 0.6901 - loss: 1.6862 - regression_loss: 9.9559 - val_classification_loss: 0.6914 - val_loss: 0.8175 - val_regression_loss: 1.3126 - learning_rate: 3.1250e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+QAAAIjCAYAAACKx9GpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAyBZJREFUeJzs3XVYVOnbB/DvDN0lCAYpCipiYuCaKBZid2BhrLq66666a+eaa62x6mInFrZgi9hrKxalIkh3zZz3D36c15EQER3X/X6uay6dc55zzj2HGZj7SYkgCAKIiIiIiIiI6IuSKjsAIiIiIiIiov8iJuRERERERERESsCEnIiIiIiIiEgJmJATERERERERKQETciIiIiIiIiIlYEJOREREREREpARMyImIiIiIiIiUgAk5ERERERERkRIwISciIiIiIiJSAibkRERfsRkzZkAikRSr7KZNmyCRSBAaGvp5gyqG0NBQSCQSLF68WNmhFOjcuXOQSCQ4d+7cRx+b99o2bdpU6nHRh+3ZswfGxsZISUlRdiiflZeXF6ytrZUdhlJIJBLMmDHjs15j7dq1sLS0RGZm5me9DhHRhzAhJyIqobwEWCKR4NKlS/n2C4KAihUrQiKRoEOHDqV23Xnz5uHgwYMffVxerB96lCRJLS1eXl7FitHLy0tpMX4NoqKiMGHCBDg4OEBbWxs6OjqoU6cO5syZg4SEBGWH99nIZDJMnz4dY8aMga6u7he/flHvyREjRnzxeArz9u1b/PDDD3BwcICWlhbMzMzg4uKCiRMnKlRk7NixA8uWLVNeoErk5eWFrKwsrFu3TtmhENF/nKqyAyAi+rfT1NTEjh070LhxY4Xt58+fx8uXL6GhoVGq15s3bx66deuGTp06KWzv378/evXqVej1tm7dqvB8y5Yt8Pf3z7fd0dGxVOP9GMOHD4ebm5v4PCQkBNOmTYO3tze+++47cbudnd0nXadJkyZIT0+Hurr6Rx9rZWWF9PR0qKmpfVIMJXX9+nW0a9cOKSkp6NevH+rUqQMAuHHjBn7//XdcuHABp06dUkpsn9vhw4cRHBwMb29vpcXQqlUrDBgwIN/2ypUrKyGa/OLi4lC3bl0kJSVh8ODBcHBwQGxsLO7evYs1a9Zg5MiRYmXGjh07cP/+fYwbN065Qb8nPT0dqqqf9yuqpqYmBg4ciKVLl2LMmDHF7olERFTamJATEX2idu3aYe/evVixYoXCl8gdO3agTp06iImJ+SJxqKioQEVFpdD9/fr1U3h+5coV+Pv759uuTA0bNkTDhg3F5zdu3MC0adPQsGHDIuNMTU2Fjo5Osa8jlUqhqalZohglEkmJj/1UCQkJ6Ny5M1RUVPDPP//AwcFBYf/cuXOxfv36UrnWx97TL8HHxweurq4oX7680mKoXLnyZ/3MfOp937hxI8LDwxEYGIhGjRop7EtKSipRJdSX9qU+Xz169MDChQtx9uxZtGjR4otck4jofeyyTkT0iXr37o3Y2Fj4+/uL27KysuDr64s+ffrkK1/Y+OXijE2WSCRITU3F5s2b83XfLo0x5D4+PmjRogXMzMygoaGBqlWrYs2aNfnK3bhxA+7u7ihTpgy0tLRgY2ODwYMHF3luQRDg7e0NdXV17N+/v8Qx5r3O8+fPY9SoUTAzM0OFChUAAGFhYRg1ahSqVKkCLS0tmJiYoHv37vnuSUE/g2bNmqF69ep4+PAhmjdvDm1tbZQvXx4LFy5UOLagn5OXlxd0dXXx6tUrdOrUCbq6ujA1NcWECRMgk8kUjo+NjUX//v2hr68PQ0NDDBw4EHfu3CnWuPR169bh1atXWLp0ab5kHADKli2LKVOmiM8LG4trbW2t0O2/sHvq6+srbi8oFolEgvv374vbHj9+jG7dusHY2BiampqoW7cu/Pz8FI7Lzs7GzJkzYW9vD01NTZiYmKBx48YKn5+CZGRk4MSJEwo9KN59naNHj8bBgwdRvXp1aGhooFq1ajhx4kS+sv/88w/atm0LfX196OrqomXLlrhy5UqR1/5YFy9eRPfu3WFpaQkNDQ1UrFgR48ePR3p6ukK5vPfN8+fP0a5dO+jp6aFv3775zicIAqytreHp6ZlvX0ZGBgwMDDB8+HAAwPPnz6GiooIGDRrkK6uvry8mu82aNcPRo0cRFhYm/i55d8x6dHQ0hgwZgrJly0JTUxPOzs7YvHmzwvnenSvijz/+gJWVFbS0tNC0aVOF98W7r/XFixdwd3eHjo4OypUrh1mzZkEQBIWy779v8+bSePbsGby8vGBoaAgDAwMMGjQIaWlpCsemp6dj7NixKFOmDPT09NCxY0e8evWqwM9CnTp1YGxsjEOHDuW7V0REXwpbyImIPpG1tTUaNmyInTt3om3btgCA48ePIzExEb169cKKFStK7Vpbt27F0KFD4eLiInbb/dTu2+9as2YNqlWrho4dO0JVVRWHDx/GqFGjIJfL8f333wPI/aLeunVrmJqaYtKkSTA0NERoaGiRSbZMJsPgwYOxe/duHDhwAO3bt//kWEeNGgVTU1NMmzYNqampAHK7c1++fBm9evVChQoVEBoaijVr1qBZs2Z4+PAhtLW1izxnfHw82rRpgy5duqBHjx7w9fXFxIkT4eTkJP5si3qN7u7uqF+/PhYvXoyAgAAsWbIEdnZ2GDlyJABALpfDw8MD165dw8iRI+Hg4IBDhw5h4MCBxXrNfn5+0NLSQrdu3YpV/mO9f0/bt28PXV1d7NmzB02bNlUou3v3blSrVg3Vq1cHADx48EBsvZ40aRJ0dHSwZ88edOrUCfv27UPnzp0B5CZX8+fPF9/HSUlJuHHjBm7duoVWrVoVGtvNmzeRlZWF2rVrF7j/0qVL2L9/P0aNGgU9PT2sWLECXbt2RXh4OExMTMQYv/vuO+jr6+OXX36Bmpoa1q1bh2bNmuH8+fOoX7/+B+9RRkZGgb1e9PX1xdbnvXv3Ii0tDSNHjoSJiQmuXbuGlStX4uXLl9i7d6/CcTk5OXB3d0fjxo2xePHiAt+jEokE/fr1w8KFCxEXFwdjY2Nx3+HDh5GUlCS22ltZWUEmk2Hr1q1Fvq9+++03JCYm4uXLl/jjjz8AQOzKnp6ejmbNmuHZs2cYPXo0bGxssHfvXnh5eSEhIQE//PCDwrm2bNmC5ORkfP/998jIyMDy5cvRokUL3Lt3D2XLlhXLyWQytGnTBg0aNMDChQtx4sQJTJ8+HTk5OZg1a1aR9x3IbdG2sbHB/PnzcevWLWzYsAFmZmZYsGCBWMbLywt79uxB//790aBBA5w/f77I3ze1a9dGYGDgB69NRPTZCEREVCI+Pj4CAOH69evCqlWrBD09PSEtLU0QBEHo3r270Lx5c0EQBMHKykpo3769eNzZs2cFAMLZs2cVzhcSEiIAEHx8fMRt06dPF97/Va2joyMMHDiw0HhCQkKKFf/333+f79x58b/L3d1dsLW1FZ8fOHBAfN2FyXstixYtErKzs4WePXsKWlpawsmTJ4sVW57r16/nuyd5r7Nx48ZCTk7OB+MPCgoSAAhbtmwRtxX0M2jatGm+cpmZmYK5ubnQtWvXfK/t3ZgGDhwoABBmzZqlcO1atWoJderUEZ/v27dPACAsW7ZM3CaTyYQWLVrkO2dBjIyMBGdn5yLLvAuAMH369HzbraysFN5DRd3T3r17C2ZmZgrbIyMjBalUqvB6W7ZsKTg5OQkZGRniNrlcLjRq1Eiwt7cXtzk7Oyt8Hoprw4YNAgDh3r17Bb5OdXV14dmzZ+K2O3fuCACElStXits6deokqKurC8+fPxe3vX79WtDT0xOaNGnywRgAFPrYuXOnWK6g9+H8+fMFiUQihIWFidvy3jeTJk3KV37gwIGClZWV+Dw4OFgAIKxZs0ahXMeOHQVra2tBLpcLgiAIb968EUxNTQUAgoODgzBixAhhx44dQkJCQr5rtG/fXuEaeZYtWyYAELZt2yZuy8rKEho2bCjo6uoKSUlJgiD8/2dBS0tLePnypVj26tWrAgBh/Pjx+V7rmDFjxG1yuVxo3769oK6uLrx9+1bc/v77Nu/34ODBgxXi7Ny5s2BiYiI+v3nzpgBAGDdunEI5Ly+vQj8L3t7egpaWVr7tRERfCrusExGVgh49eiA9PR1HjhxBcnIyjhw5UmB39a+dlpaW+P/ExETExMSgadOmePHiBRITEwEAhoaGAIAjR44gOzu7yPNlZWWhe/fuOHLkCI4dO4bWrVuXWqzDhg3LN2b+3fizs7MRGxuLSpUqwdDQELdu3frgOXV1dRXGB6urq8PFxQUvXrwoVkzvz7T93XffKRx74sQJqKmpYdiwYeI2qVQq9j74kKSkJOjp6RWrbEkUdE979uyJ6Ohohe79vr6+kMvl6NmzJ4DcicTOnDmDHj16IDk5GTExMYiJiUFsbCzc3d3x9OlTvHr1CkDu++fBgwd4+vTpR8UWGxsLADAyMipwv5ubm0JvkRo1akBfX1+8/zKZDKdOnUKnTp1ga2srlrOwsECfPn1w6dIlJCUlfTAOT09P+Pv753s0b95cLPPu+zA1NRUxMTFo1KgRBEHAP//8k++ceT0oilK5cmXUr18f27dvF7fFxcXh+PHj6Nu3rzgpWdmyZXHnzh2MGDEC8fHxWLt2Lfr06QMzMzPMnj07X/fwghw7dgzm5ubo3bu3uE1NTQ1jx45FSkpKviEMnTp1UhjX7+Ligvr16+PYsWP5zj169Gjx/3lDDbKyshAQEPDBuAr6fMXGxoo/t7whCqNGjVIoN2bMmELPaWRkhPT09Hxd34mIvhQm5EREpcDU1BRubm7YsWMH9u/fD5lM9tm6FRdXYmIi3rx5Iz7i4uI+eExgYCDc3Nygo6MDQ0NDmJqa4tdffxXPBwBNmzZF165dMXPmTJQpUwaenp7w8fEpcD3f+fPn4+DBg/D19UWzZs1K9fXZ2Njk25aeno5p06ahYsWK0NDQQJkyZWBqaoqEhAQx/qJUqFAh32zLRkZGiI+P/+CxmpqaMDU1LfLYsLAwWFhY5OuWXKlSpQ+eH8jtFp2cnFyssiVR0D1t06YNDAwMsHv3bnHb7t27UbNmTXFm8WfPnkEQBEydOhWmpqYKj+nTpwPIHeoAALNmzUJCQgIqV64MJycn/Pzzz7h7926xYywsobS0tMy37d37//btW6SlpaFKlSr5yjk6OkIulyMiIuKD169QoQLc3NzyPd7tmh0eHg4vLy8YGxuL8wnkdfl//32oqqoqzoHwIQMGDEBgYCDCwsIA5HaNz87ORv/+/RXKWVhYYM2aNYiMjERwcDBWrFghDkXYuHHjB68TFhYGe3t7SKWKXxPzVmDIu34ee3v7fOeoXLlyvrkbpFKpQmVIXjkAxZr74v2fcV7lTN7POCwsDFKpNN/7uKjPV977ibOsE5GyMCEnIiolffr0wfHjx7F27Vq0bdtWbEl+X2Ff/N6f/OtT/fDDD7CwsBAfXbp0KbL88+fP0bJlS8TExGDp0qU4evQo/P39MX78eAC545/z4vf19UVQUBBGjx6NV69eYfDgwahTp47CGscAxMmbFi5ciIyMjFJ9fe+2QuYZM2YM5s6dix49emDPnj04deoU/P39YWJiIsZflMJmqS9Oq2JRM9yXFgcHBzx58gRZWVmfdJ7C3msF3VMNDQ106tQJBw4cQE5ODl69eoXAwECxdRz4//fGhAkTCmw99vf3F5OiJk2a4Pnz5/j7779RvXp1bNiwAbVr18aGDRuKjDlvHHhhlSOf8rMrTTKZDK1atcLRo0cxceJEHDx4EP7+/uKEfe+/DzU0NPIlvoXp1asX1NTUxFbybdu2oW7dugVWMgC5n9XKlStjzJgxuHDhAqRSqUIL+7/N5/gZx8fHQ1tbu8D3PhHRl8BJ3YiISknnzp0xfPhwXLlyRaE18X15rToJCQkK299vdSpMcVtyfvnlF4Xu14V19c1z+PBhZGZmws/PT6El6uzZswWWb9CgARo0aIC5c+dix44d6Nu3L3bt2oWhQ4cqlBkxYgQ6dOiA7t2748CBA591fWFfX18MHDgQS5YsEbdlZGTku9fKYmVlhbNnzyItLU2hlfzZs2fFOt7DwwNBQUHYt2+fQnfiwhgZGeV77VlZWYiMjPyouHv27InNmzfj9OnTePToEQRBUEjI81o91dTUCpwF/X3GxsYYNGgQBg0ahJSUFDRp0gQzZsxQeO+8L29W+ZCQEDg5OX1U/EBuLxZtbW0EBwfn2/f48WNIpVJUrFjxo8/7vnv37uHJkyfYvHmzwnrlH5pFvjiMjY3Rvn17bN++HX379kVgYCCWLVtWrGNtbW1hZGSk8LMv7HeJlZUV7t69C7lcrlBZ8PjxY3H/uwoafvDkyROFWduB3MqIFy9eKKzZ/uTJEwDIV7YkrKysIJfLERISotBqX9TnKyQkRGz5JyJSBraQExGVEl1dXaxZswYzZsyAh4dHoeWsrKygoqKCCxcuKGxfvXp1sa6jo6NTrASzatWqCl1q69SpU2T5vNand1ubEhMT4ePjo1AuPj4+X4tUzZo1AaDAbutubm7YtWsXTpw4gf79+xerpbqkVFRU8sW2cuXKUu99UFLu7u7Izs5WWCtcLpfjzz//LNbxI0aMgIWFBX766ScxkXlXdHQ05syZIz63s7PL9z7766+/Pvp+uLm5wdjYGLt378bu3bvh4uKi0C3YzMwMzZo1w7p16wpM9t++fSv+P28seB5dXV1UqlSpwPfOu+rUqQN1dXXcuHHjo2LPo6KigtatW+PQoUMK3aOjoqKwY8cONG7cGPr6+iU69/vXARQ/R4IgYPny5Z98bgDo378/Hj58iJ9//hkqKiro1auXwv6rV6+Kqw6869q1a4iNjVVoTdfR0SlwKEe7du3w5s0bhYrFnJwcrFy5Erq6uvlm3D948KA4R0Deta5evVrgygSrVq0S/y8IAlatWgU1NTW0bNmyGK++aO7u7gDy/y5duXJlocfcunUr33rtRERfElvIiYhKUXGWrzIwMED37t2xcuVKSCQS2NnZ4ciRI+IY2w+pU6cOAgICsHTpUpQrVw42NjbFWq7pQ1q3bg11dXV4eHhg+PDhSElJwfr162FmZqaQZG3evBmrV69G586dYWdnh+TkZKxfvx76+vpo165dgefu1KkTfHx8MGDAAOjr62PdunWfHG9BOnTogK1bt8LAwABVq1ZFUFAQAgICxO7OytapUye4uLjgp59+wrNnz+Dg4AA/Pz9xfP+Hej8YGRnhwIEDaNeuHWrWrIl+/fqJFS23bt3Czp070bBhQ7H80KFDMWLECHTt2hWtWrXCnTt3cPLkSZQpU+aj4lZTU0OXLl2wa9cupKamYvHixfnK/Pnnn2jcuDGcnJwwbNgw2NraIioqCkFBQXj58iXu3LkDILeiqFmzZuIa0Ddu3ICvr6/CZF8F0dTUROvWrREQEFCsJbIKMmfOHPj7+6Nx48YYNWoUVFVVsW7dOmRmZuZbb74wT548wbZt2/JtL1u2LFq1agUHBwfY2dlhwoQJePXqFfT19bFv375izUNQHO3bt4eJiQn27t2Ltm3bwszMTGH/1q1bsX37dnTu3FmsxHj06BH+/vtvaGpqinNCALm/S3bv3o0ff/wR9erVg66uLjw8PODt7Y1169bBy8sLN2/ehLW1NXx9fcUW+fcnFqxUqRIaN26MkSNHIjMzE8uWLYOJiQl++eUXhXKampo4ceIEBg4ciPr16+P48eM4evQofv3113zzL5REnTp10LVrVyxbtgyxsbHismd5lVfvf75u3ryJuLi4Atd3JyL6UpiQExEpwcqVK5GdnY21a9dCQ0MDPXr0wKJFi8Q1nYuydOlSeHt7Y8qUKUhPTxe/3H6qKlWqwNfXF1OmTMGECRNgbm6OkSNHwtTUFIMHDxbLNW3aFNeuXcOuXbsQFRUFAwMDuLi4YPv27QVOCpanX79+SE5OxqhRo6Cvr49FixZ9cszvW758OVRUVLB9+3ZkZGTA1dUVAQEBYsuZsqmoqODo0aP44YcfsHnzZkilUnTu3BnTp0+Hq6srNDU1P3iO+vXr4/79+1i0aBGOHj2KrVu3QiqVwtHREZMmTVJIbIcNG4aQkBBs3LgRJ06cwHfffQd/f/8StUb27NkTGzZsgEQiQY8ePfLtr1q1Km7cuIGZM2di06ZNiI2NhZmZGWrVqoVp06aJ5caOHQs/Pz+cOnUKmZmZsLKywpw5c/Dzzz9/MIbBgweja9euiIiIKFH38mrVquHixYuYPHky5s+fD7lcjvr162Pbtm3F/gzljYl/X9OmTdGqVSuoqanh8OHDGDt2LObPnw9NTU107twZo0ePhrOz80fH/D51dXX07NkTq1evzjeZGwAMHz4c2traOH36NA4dOoSkpCSYmpqidevWmDx5MmrVqiWWHTVqFG7fvg0fHx/88ccfsLKygoeHB7S0tHDu3DlMmjQJmzdvRlJSEqpUqQIfHx94eXnlu+aAAQMglUqxbNkyREdHw8XFBatWrYKFhYVCORUVFZw4cQIjR47Ezz//DD09PUyfPl3h/fGptmzZAnNzc+zcuRMHDhyAm5sbdu/ejSpVquT7fO3duxeWlpZo0aJFqV2fiOhjSYQvPdsJERERKTh48CA6d+6MS5cuwdXVVdnhfLVkMhmqVq2KHj16YPbs2coOR2nGjx+PjRs34s2bN/lm7P+SQkNDYWNjg0WLFmHChAlFlvXy8oKvr2++iR+/hNu3b6NWrVrYtm0b+vbtCyB3eI21tTUmTZqEH3744YvHRESUh2PIiYiIvqD09HSF5zKZDCtXroS+vj5q166tpKj+HVRUVDBr1iz8+eefSknsvgYZGRnYtm0bunbtqtRk/Gv1/ucLAJYtWwapVIomTZqI23x8fKCmppZvbXMioi+NXdaJiIi+oDFjxiA9PR0NGzZEZmYm9u/fj8uXL2PevHlceqkYevbsqTDD+39FdHQ0AgIC4Ovri9jYWLbqFmLhwoW4efMmmjdvDlVVVRw/fhzHjx+Ht7e3wjCHESNGMBknoq8CE3IiIqIvqEWLFliyZAmOHDmCjIwMVKpUCStXrvzgpGb03/bw4UP07dsXZmZmWLFihbiyASlq1KgR/P39MXv2bKSkpMDS0hIzZszAb7/9puzQiIgKxDHkRERERERERErAMeRERERERERESsCEnIiIiIiIiEgJvvkx5HK5HK9fv4aenh4kEomywyEiIiIiIqJvnCAISE5ORrly5SCVFt4O/s0n5K9fv1aYVZOIiIiIiIjoS4iIiECFChUK3f/NJ+R6enoAcm+Evr6+kqMhIiIiIiKib11SUhIqVqwo5qOF+eYT8rxu6vr6+kzIiYiIiIiI6Iv50LBpTupGREREREREpARMyImIiIiIiIiUgAk5ERERERERkRIwISciIiIiIiJSAibkRERERERERErAhJyIiIiIiIhICZiQExERERERESkBE3IiIiIiIiIiJWBCTkRERERERKQETMiJiIiIiIiIlIAJOREREREREZESKDUhv3DhAjw8PFCuXDlIJBIcPHhQYb8gCJg2bRosLCygpaUFNzc3PH36VDnBEhEREREREZUipSbkqampcHZ2xp9//lng/oULF2LFihVYu3Ytrl69Ch0dHbi7uyMjI+MLR0pERERERERUulSVefG2bduibdu2Be4TBAHLli3DlClT4OnpCQDYsmULypYti4MHD6JXr15fMlQiIiIiIiKiUvXVjiEPCQnBmzdv4ObmJm4zMDBA/fr1ERQUVOhxmZmZSEpKUngQERERERERfW2+2oT8zZs3AICyZcsqbC9btqy4ryDz58+HgYGB+KhYseJnjZOIiIiIiIioJL7ahLykJk+ejMTERPERERGh7JCUIiwpDK9SXhWrrCAIyMjhuHwiIiIiIqIv6atNyM3NzQEAUVFRCtujoqLEfQXR0NCAvr6+wuNbk5adhqMvjiItOy3fPkEQsP3Rdnge9ITnQU+cDT9b5LnkghwTzk9Awx0NsfzWcmTKMksUkyAIyJJllejYz+1F4gvMvzofR18cLVb5pKwkRCR/WkVOclYy3qa9/aRzFCQiKQIDjw/EtofbPvrYsKQwxGXElfjaadlpmHhhIrY/2l7icxARERER0f/7ahNyGxsbmJub4/Tp0+K2pKQkXL16FQ0bNlRiZF9OYmYivj/9PdbcWaOwfe7VuZh0cRLGnBmDbFm2uD1TlompgVPx+7XfIRNkyJRlYvy58Tj47CAEQUB4UjguvLyAxMxE8Zi/7/+NU2GnkCPkYMO9Deh+uDv+if7no+KUC3KMPzceDXc0xPq765Ejz8lXJi07Dfuf7kdIYkih55HJZUjKyj/m/2HsQ5wOP42HsQ8RnxEPQRCKFVdEcgR+vfgrOh/qjB2Pd+DXS7/i+pvrRR7zMPYh2u9vj44HOn70fciTI89Bn6N94L7PHf5h/uL2TFkm/J77ITQxtETnTctOww/nfsCt6FtYfGMxHsc9Vjj33bd3IRfkBR77NP4pOh3qhG5+3RCVGlVgme2PtqPXkV4K533X7uDdOBZyDH/c/KPAyiAiIiIiIvo4EqG42c1nkJKSgmfPngEAatWqhaVLl6J58+YwNjaGpaUlFixYgN9//x2bN2+GjY0Npk6dirt37+Lhw4fQ1NQs1jWSkpJgYGCAxMTEf11r+bTAaTjw7AAAYEvbLahlVgtP4p+gm183CMj9sXW064g5rnPwNOEpplyagkdxjyCVSPFjnR/xNP4pDj0/BAAw1DBEQmYCAMBY0xgzGs6AjpoOhvkPg1yQo2eVnggIC0BsRiwAoKFFQwx2GowaZWogIjkCr1JeQUtVC+V0y8FCxwLqKupinBvubcDyW8vF505lnPBbg99QxagKVCQqOBN+BguuL0BkaiR01XSxttVaOJs6K7zW+Ix4DPcfjmcJzzDHdQ7a2bYDkJsk/n7td4Wyeup6sDOwQwW9CkjJSkF0ejQEQcDv3/0OW0NbAEBMegw8D3qKCX553fJ4lfIKZlpm8O3oCyNNo3z3+5/ofzAqYBRSslMAADYGNtjrsRcaKhof9XM7HX4a486OAwBIJVLMajQLlYwqYcqlKXiW8AxmWmY43PkwtNW0izzPsRfH8Dr1NTpX6gxjTWNMvDgRx0OOi/urmVTD9nbbkZaTBu9T3rgfex91ytbBHNc5qKBXQeFcP5//GSdCT4jHbWqzCZqq//8ZOvTsEKYETgEAWOtbY4/HHmipaon7M2WZaLOvDWLSYwAAy5svRwvLFh91Xz4kS5al8L76FK9SXuFezD00r9j8o39+X1pseiwMNQyhIlVRdihEREREVEqKm4cqNSE/d+4cmjdvnm/7wIEDsWnTJgiCgOnTp+Ovv/5CQkICGjdujNWrV6Ny5crFvsa/JSE/G34Wb9Pfonvl7pBIJAh6HQRvf29xv4OxA3a134Vx58bhXMQ5OBg74Gn8U8gEGRqXb4wrkVeQI8+BgYYBFjZZiEblGkEQBPxx6w/43PcBAKhJ1aCvri8m3ZoqmsiQZcDTzhOzXWcjKSsJS28uxaFnhyATZIXGqiJRQQfbDvix7o8ITwqH1wkvyAQZuth3gX+oP5KzkwEAqhJVmGiZICotSnyeI+RAR00Ha9zWoJZZLQBAQkYChp4aiuD4YACABBLMcp2FbHk2ZgXNAgBUMqyEhMwEMSEsiLOpM7a03QKpRIoZl2dg39N9qGRYCXMaz4GNvg16HumJ0KRQNKnQBKtarIJEIkG2PBuPYh/h+pvrWHd3HdJz0lHbrDbCk8MRkx6DYU7DMLb22I/6WQ49NRRXI6/CXMccb1LfiPfs3Xs61Gkofqj9Q6HnOPLiCCZfnAwA0FDRQG2z2giKDIKqRBW/N/kdMy/PRHJ2MsbUGoOLLy/i9tvb4rHaqtr4pd4v6GLfBRKJBC8SX6DTwU4QIEBPTQ/J2cloa90WC5osgEQiwZXIKxjpPxI5Qg7UpGrIlmejt0Nv/Fr/V/Gce4L3YPaV2eLzzpU6Y5brrI+6L0XZ+Xgnfr/2O2Y1mgXPSp6fdK64jDh09+uO6PRoWOhYYHSt0Whv077QhDcuIw4nQ0+itVVrmGiZfNK1P1bQ6yAM9x+OLvZdMKPRjC96bSIiIiL6fP4VCfmX8G9IyBMyEuB5yBNxGXFoVK4RJrlMwsiAkXiV8godbDvg/MvzSM5KhqedJw49PwQViQoOeB7A9TfXFZKkZhWbYVqDaTDVNlU4/403N6AqVUVVk6oQIGDVP6uw+cFmCBBQ2agytrXbptAa+jL5JbY83IIDTw8gQ5YBAw0DlNctj/ScdESmRCJDljsBnL66PjRUNPA2/S3a2bTD79/9jqi0KMy/Oh+XX18Wy6lJ1eBVzQv9qvbDhPMTcP3NdWipaqGDbQdU1KuIYyHH8DjuMUw0TeBi4SK2AksggQABXtW88GOdHyGRSJApy0RYUhheJLzAy5SXMNAwgJ66HqYFTkN6TjpmNJyB6mWqo8eRHpALcmxusxm1y9YGAATHBaPP0T7IkmfBVMsUMkGGlKwUZMn/f+y7a3lX/NHsDwS+CsT4c+OhKlHFzg474WDsUKyf5YuEF/A85AmpRIrjXY5j26Nt2PpwKwCgtVVrNC7fGNMuT4OaVA0HPQ/CUt8SIYkhOBdxDs0rNoe1gTVuRd3C0FNDkS3PhoWOBSJTI8XzT3KZhL6OffMlyHrqepjtOhtbHmzBrehbAICRziMxquYo/HbpN/g990Ozis0woOoAeJ/yRo6Qg0qGlWCkaYRHsY+Qkp2CttZt4VnJEyMCRgAA1rqthWt5V+TIc9DhQAe8SnmF5hWb42zEWRhrGuNM9zNQkargUewjrLmzBm5Wbuhg2wFSyceNhMmWZaP1vtaISY+BjpoODnoehLlO7jwRgiBAgFDsc8oFOb4//T0uvbqksL2aSTX82fLPfAn3k/gnGHN6DF6nvkaNMjWwpe0WMXGPSI7As/hnaFaxGSQSSb5rRSRFYNrlafiuwncYVG1QgWU+ZETACAS+CoQEEvh29EVlo/yVjVmyLMSmx6KMVhmoqah99DWKQxAEPEt4BlsDW7bUExEREZUCJuT/829IyOWCHDse7cCyW8uQKcsUE1ELHQsc8DwAv+d+mHd1nli+q31XsTVt/d31OPLiCLxreKOdTbtiJwU3o24iICwA/av2RzndcgWWSc9JR7Y8G/rq/3/fBEHAnbd3MPfqXHGscUW9itjTYQ901XUVXlN0WjQikiNQUa+imGCl56Rj7JmxuBJ5ReFaxprG8HH3gY2BDRZcXyBOHNarSi/8Wv/XD76uzQ82Y/GNxTDQMICtgS3+if4Hra1aY0mzJQrl3k9kAcBAwwC1zGqhgUUDdK/cXew2/eO5H+Ef5g9jTWPUNquNSkaVoCpRRVJWUu4jM/dfqUSKIU5D0KhcI8y9Mhe7gnehpWVLLGu+DIIg4GTYSWiraqNJhSYQBAEjA0Yi8HUgmlZoilpmtbD69mqxUqBx+ca4H3MfCZkJcLN0w5JmS3Az6iZ2Pd4FW0NbjHIeBYlEArkgx6ATg3Ar+hZ01XSxvvV6VC9THTK5DBvubcCq26sAAL0demNP8B7IBBl2tt+J6mWqF3gPapvVxvrW66Guoo55V+dh5+OdMNY0xqBqgyCVSLHoxiIYaRjhaJejcPd1R3J2Mra23QqnMk7ofqQ7nsY/BQA4GjtiQt0JcLFwUTj/hZcXEJ8Rj2YVm8FAw0Bh3/GQ4/jlwi/i88blG2N1y9V4nfoaP5//GdFp0ZjXeJ54zmx5Ni6+vAgHY4d8791N9zdhyc0l0FDRgI+7D65HXceGexuQnJWMSoaV8Lf73+JwhfMR5/HLhV+QlvP/4+F/rvszBlQbgBeJL9D/WH8kZSWhq31XTG0wVSFRFQQB3v7e4vt4ULVBGF9n/Ecl5ZEpkXDf5y4OP2lesTlWtFihUCZblo2+x/riUdwjAMittDJ3wUSXiaXWmp8ty8Zvgb/heMhxuFm6YWmzpSWqXHiV8grLbi7D4OqD4WjiWGCZw88PIz0nHT2q9PjUsOkTvE55jV8u/AJVqSrWtVr3wWEdfs/9sO/JPixsshBldcoWWZaIiL483ye+CAgPwBzXOSijVUbZ4dD/MCH/n39DQp4nJDEEUy5Nwd2YuwCANW5r0Lh8Y+TIc9DzSE88iX8Cdak6jnY5Kia4ypIjz8Hu4N249OoSxtcZX2DLXmGyZFnwD/PHi8QX4mzmI2qMEMd/C4KA/U/3Iz0nHX0c+xSrdfTdewTktsr7dfLLN5YaAJ7FP0OmLBPqKuriuPiCrhGTHoNeR3qJXe6LIpVIMdJ5JHzu+yAtJw0bWm9AfYv6BZZ9kfgCXQ91RY7w/5Pf2RjYIDQxVEzOqplUg08bH4WeC+97k/oGmx9shoedB6qaVFXY9/f9v/HHzT/E567lXLG21VrxeVhSGMKTwpGclQypRIqmFZuK10rPSUefo33wLOGZwjnH1BoD7xre+OXCLzgechxDqg+BnaEdfr30K3TUdCCBRBx/v+C7BeI8AFcjr2LoqaEAcoctNCjXAEOqD0Fd87oAgP7H+uP229toZ9MO/mH+yJZnY0DVAfB77ifOeyCVSDG21lhYG1hj2c1lCE0KhZGGEf52/xuVjCoByO0JMuzUMOQIOZjaYKqY9IUlhWHQiUF4m/4WVYyqoF/Vfjj07BBuRN0AALiYu+T2jLj5BzRVNLG+9XpMujhJYdnAdjbtMKfxHKhJc1uoT4aexITzE8RhGEBu5UedsnUQHBeMpKwkjHAeUeQfxdW3V2PNnTWwNbBFaFJobsVcux1wMnUSy7w/P0MeE00TzG08F67lXQs9f3GkZafhx/M/IvBVoLjt3Xv3MfJ6YlQ1qYpd7XflS+qD44LR7XA3AMCmNptQp2ydAs+TkJEAAw0DhePTstOQlJWk9N97eWRyGWYGzYShhiF+rPvjRx37JvUN0nPSYWNg81HHRaVGISU7BXaGdoWWuRV1C2V1yqK8bvlCyzyLf4bhAcMRnRYNABhbayyG1RhWaPmUrBS09m2N5OxkjKo5CiOdR35U3J/qbdpbHHlxBK2tWxf5upTt+pvrWHR9EeqZ18PP9X5Wdjj0DYlIioCptqnCvC8lJRfkeBT3CI7Gjh/dm42+XoIgoPme5ojNiEUH2w6Y/918ZYdE/8OE/H/+TQk5kJtYHnx2EJqqmuhg20Hcfu/tPYw7Ow79qvbDoOqDlBjh1+t29G30P94fADCk+hCMqzPuk8+Zmp2Ku2/v4mn8U7xIfAEgt6u+voZ+7r/q+rj06pI4eR4A2BrY4qDnwSJbGRdfX4zNDzdDT10PE+tNREe7johIjsDOxzsRmRqJ3+r/lm/owcdad2ed2FKeNylgcaVkpeBYyDEcCzmGm1E3UUarDA51OgR9dX2xRdta3xqZskxEpkZifJ3x6FSpE5bcWAK/537QVdPNnTxPwwhd/LrgVcorGGsai8uuqUvVsbbVWuio6aDnkZ5QlajiVLdTOPjsIFb88/+txFVNqsLWwBZHXhwpMM4yWmXg4+6DS68uYcnNJciR58Dd2h2LmixSuP8hiSEYdGKQOH8CkJvkd6/cHRNdJkJVooqhp4bi2ptrkEqkkAtyVNCtgEHVB2H+1fnIEXLwXfnvMLXBVBhoGKDjwY6ISovCCOcRMNUyzdfjIC92H3efAifvk8llaLO/Dd6kvsGC7xYg8HUg/J77oaFFQ/zV+i8AuV/COvt1RqYsE/Maz0Pj8o3xPOE55l6dK1aWtLRsCdfyrmhg0QAV9SoWeI9uR9/GpgebEJseiyYVmqC1dWuoSFRwNfIq9jzZg4exD6GlqgU3SzccfnEYmiqa2N1hN2wNbSEIAmSCDKpS1SLfL+k56Wi+pzlSs1MBAH+1+gsNyymuhpHX4wQAGpVrhHWt1uU7T17vje/Kf4c5jefAWNMY199cxy8XfkF8RjwWN10MNyu3ImP5Et6d42Nr262oaVazWMelZaeh3f52SMxKxOY2m1HDtEaR5eWCHIGvArHnyR5ceHkBALCx9UaxMutd+5/ux/TL0wEAtcxqob1Ne3hW8lT4En87+ja+P/09krKSxM+jlqoWjnQ+AjNtswJj8Lnvg6U3lwIAaprWxNZ2Wwss9zD2IYw0jGCha/HhG1GIbHk2VCQqConC6NOjcf7leWioaGCY0zB0rdwVAWEBOPz8MLRUtbCy5coiKy4/txx5DtbcWYP1d9eLFap+nfw+usLl3yY9Jx0pWSmf/HfqayUIAh7EPkBFvYr5enV9SWfCz+CHsz+gvW17/P7d7x8+4AMWXV+ELQ+34IfaP2Co09BSiFD5rr+5jjV31uBt2lv81eqvT/od9G+VN1wyz8d+56PPhwn5//zbEnL6NJvub8K9mHuY5ToLOmo6X+SagiBgV/AuLLy2EDlCDn6r/xt6OfQq8pgceQ7OvzwPZ1Pnz9q16OCzg8iR56Bb5W4lPkdMegzUpGril5KkrCQ03dVUbBk20zbD0c5HoamqiRx5DgadGITbb2+jllktVDaqjN3Bu1FOpxz2e+5HVFoUltxYggsvL0BXTRfVTKrh6puraGvTFgubLES2PBv9jvXDw9iH8LD1wLSG06ChogHfp76Yf3U+VCQq6F+1P7pW7ooxZ8bgafxTaKhoIFOWCQBoZdUKc1znFJgEP094jhEBIyCFFJ3sO6Fzpc4KLa4RSRHo4tdFnDdhW9ttsDawxvmI8/jx3I/IkmdBXaoORxNH3Hl7B+V1y+OgZ27l2cFnB/HHzT9goWMBB2MHnAk/g/jMeLiWd8XKFisR9DoIa++shYpEBdMaTkNkaiS+P/09DDQMcLr7abxNewuPgx7IkedgXO1x6Fa5G34+/zOCIoNQ36I+1rdaL1YwZORkYOnNpdj5eKfC62tl1QrTG04Xf055X1I+tNSfvro+/mz5J2qY1sAI/xEIigxCJcNKqGxUGdfeXEN6TjpWtliJeub1Cj3HiZAT+PnC/7cKNrBogPWt14vP81rHJZBAKpFCJsjy9QZIz0lHm31txEobMy0ztLZujR2Pd4jL+alJ1bCu1boiY/lYoYmheBj7EBHJEYhJj0ELyxb5KhPelzdpJJA7d8fKFiuLda13V40or1sevh6+CkN93rfw+kJxDoo8LuYu2Oi+UWFbUlYSPA54iPcuj1MZJ6xssRImWia4+PIifjr/E9Jz0lHDtAZWtViF0WdG4+7bu+ho1xFzG8/Nd/0sWRba7GuDt+lvAeROTnmh1wWFYUzA/ycNqhJVdKzUEcOchhXYO6kwoYmh2Hh/I448PwJ3G3cx8Xib9hatfFsVOcnouNrjMMRpSL7td97ewZ///ImulbvC3dq9yOvLBTmuvL4CU21T2BvZFyvm9Jx0nAg5ge2PtosTkuZVcvRx6IPJ9ScXeJ2Y9BgYaRqJvW2KIyEjAWcjzqKWWS1YG1gX+7h3BccF48iLI+jr2Ffh996BpweQkJmAgdUGFqvFNCI5Arse78KBpweQlpOGmY1mftREnIIgICkrSalJLpC7ssuzhGeoW7Zuvsrzu2/vYuH1hbjz9g5qmdXClrZblBJjtjwbnQ91RlhSGNSl6rjQ68Infa8JjgsW59cx1TLFyW4nP+p9GJkSCVNt0w9W0BbmYexDzL06F62tWmNA1QElGhr1ruC4YCy6sQhXI6+K294d0vm1S8tOw4WXF9DSsuUnzw+z+/FuzLk6R3zuaOyIne13flNzwsjkMuwK3gUHY4dCe9h9jZiQ/w8TcvpSHsQ8wIPYB+hq3/Wb+iVYkLyZ5AFgVqNZ6GzfWdz3Mvkluh3uJraWAootphk5GRjuP1ycfA5QbGVMy05DSGIIqppUVfiDHZ0WDTWpmjgGPDY9FoNODkJIYghUpaqYUHcC+jj0KfKPvCAIRe7P+4L9U92fFFo9H8U+wsLrC8Vu7gCwovkKNLfMv0oEkPuFbsjJIciQZcBM20zsHgzkJpXldMshLCkM/Rz7YaLLRADA/KvzsePxDgD/vyKBulQd+z33w0rfKt81HsY+xIWXFxD0Ogh33t6BTJChrHZZjK09FsdeHEPg69xu6KpSVXjaeaKqSVWcCT8j/tycTJ1Q36I+PO08xeTpbdpbdPXrivjMeIVr6ajp4G/3v/MNjcgz5vQYnHt5Dh1sO+B4yHHIBBl2ddiFaibVAADjzo7D6fDTaGPdBuoq6uIkg+8msnnJqrmOObRUtRCSGCLu62jXESlZKTgTcQa6arrwaeOjMNHim9Q3uB19G3XN6xZYwRWVGoUToSeQKctEt8rdYKxpDJlchjV31mDd3fwt9e1t2+Pnuj8XOE4/W56N5nuaIzEzUdx2oOMBcehEYbLl2Wi/vz0iUyPF1QyK6lr4JP4Juh/uDrkgRx+HPmhu2RwjA0YiR56Tr/Ujr9XLxsAGa93Wwj/MH+vvrUdiZiIq6lVEzyo9sezmMuQIOXAt74qlTZdCW00b997eQ59jfQAAu9rvQrUy1RRiyGt1N9M2g5aqFsKSwrC02VK0smollknKSkKng53EpB3Iff82rdgUbW3aommFpoV2tU3LTsPcq3Nx5MURsdIFAHw9fFHFuIrYOu9s6ow+Dn2w+MZivE1/C3sjezgaO8LvuV9ur52uxxUqCSKSItDnWB9xyEveZJjvEwQBl19fxvJby/Eo7hEkkKCLfReMrT0WxprG+conZSXhyusrCHwdCP8wfyRn5a4ooqemh2mNpkFfXR/D/YdDW1Ubp7ufzlfZMitoFvY+2QupRAoLHQs4mzpjsstkGGoaivFcfHURFXQriEO4IlMi4e3vjdCkUABAZaPKaGPdBp0qdSp263REcgT6Hu2L+Mx4WOtbY0vbLTDSNMKWB1uw6MYiAMDEehPRr2o/ALnv1WMvjqGSYSXxPZGRk4HFNxZjT/AesSdAnin1p6Bb5W44GnIUPvd9YKplirG1x6J6meoK5aJSo/DLhV9w++1t/OryK3o69CxW/EDuZ/xk6EnUNquN6mWqfzCZS8xMhI6aToHJoyAI8DrhhVvRtzDCeQS+r/k9gNzlPedcmYODzw4qlN/Xcd9HDcsDciuzjoccx9EXR6GlqoUqxlXgaOyIxhUaKyTBiZmJiE6LLrAi6P35XhY3XfzByqXCvPua8yxrtgwtrVoW6/itD7di4fWFcDR2xMImCz+6Yihblo0eR3qIvbtaW7XGbNfZCpXn2bJsnI44jcycTLS2bl1oz5cceQ42PdiEP2//iRx5DlSlqmhRsQVOhZ2CqkQVR7ocKXJ4y5P4J/C574OeVXoWu3fT5zDh/AScDD2JQdUH4cc6Hzf06X0/nfsJp8JOoa9jX/g980NydjKmN5yerzEmJSsFiVmJEAQBOfIcvE59jZfJL8W/R1+6oixvQlk7Q7sPVghuvLcRy24tg6aKJra33/7Rn0llYUL+P0zIiUrfjkc7MP/afNgZ2MG3o2++Lz2Hnx/Gr5dyl00rqMY6KSsJg04MwpP4J3A0dsTuDrtLVFsekx6DXY93obllczH5+1wEQcD5l+ex8d5GVDWpikkuk4qM+Wz4WYw7Nw5yQQ4NFQ30duiNkMQQnH95XizzbiKXLc+G7xNf7HuyT2xxG11zNIY7D/9gbA9iH2DihYkISwoTt6lKVNG1clcMdRqq0CKWmp0KCSQF9iIAcsfib36wGQ4mDqhXth7W3V2Ha2+uwVjTGJvbbM73RSw+Ix4t9rRAjpCDQ56HsP5e7kSTeZMqPo57jO6Hu0MCCQ54HoBUIoXnQU8IEMTEK0uWhXb72yEqLQpTG0xFB9sOWHh9Ic6/PI8xtcagc6XOyJJnYbj/cNyMugkAsNSzRGWjyghPDhfnjiinUw4bWm9ARf3c7vtXIq9g472NuBp5VUwi9NT0MLRG7vCEvLHzNUxrwEbfBgIEHH5+GAIEGGgYYEXzFeIqDXkCXwViRMAIGGsaw9nUGWcjzhbawvyuoy+OYtLFSTDWNMaCJgsw3H845IIc8xrPg4edh0JZQRAw3H84giKD0MqqFZY2y+0yntcy/+6cECGJIehyqAtyhBxx3hEgd66KUQGj8s+F4DpHoTXm14u/4vCL3O7fjsaOqGpSFbXL1kYts1oYdGIQQpNCMaHuBLxJfYNtj7bl+zznxWStb41pDadhw70NuPz6srhfV00XC5osQJMKTRReY0x6DEafHo0HsQ8AAE0rNEVGTgauvrkqrtrR+VBnPE98Ln6xzJRlIj4jHmW1y0IuyNHtcDc8S3imsDxlUlYS+h3rh5DEEBhpGImVS/0c+8FI0wiP4x7jVcorZORkICU7Rawsy1sGNO89UsOsBjSkGlCRqiAmPQZRqVF4k/ZGoeKgvG55dK/cHZ3tO8NY0xiCIMDzkCdCEkPyVQLEZ8Sj5d6WyJZnK9wHWwNbrGu1Drpquph2eRr8w/whlUjRzb4bPOw88POFn/Em9Q301fWRlp0m9kxSlaiipVVLeNh6wNbQFhY6FgUmn0lZSeh/rL845ArIfb93r9wdUwOnits0VDSwp8MeWOlbYfLFyTgemrvaSRvrNvCs5IklN5aIyZRreVf0ceiDoNdB2PZoGwDkWxEEyE262tm2g5WeFSJTIzElcIrYi0MCCRY3XYzW1q0BAI/jHiNTlgmnMk4KX8wFQYDfcz8suLZAXFK1ol5FtLVpi2YVmqFamWoK5WVyGTbe34jVt1ejinEVbGy9MV/FyD/R/2DA8QHi81mNZqGVVSuMOTNGrHT1tPNEVFoUrkRewcCqAzGh3oR897YgiZmJ2PFoB3YF78rXYwUA6pati1UtV0FHTQfPE55j6KmhiEmPyZdsp2WnocOBDnib/hbldMrhdeprsSdZUdKy0xCXEQczbTNxglrg//8ma6lqoaVlSxx5cSTf3DKFuRp5Fd7+3uJ7X0tVCxPqToCJlgleJr9EWk4a6pjVQS2zWoW29OYlU3pqekiXpSNHngM7Azu4W7vDSt8Kb9PfYuvDreKcPcaaxhhYbSC6Ve4mVrZly7MR9DoI6+6uw923uXMtNa/YHBNdJqK8bnl4n/JGUGRQka3kUalR6H20N96mv4WWqhZWt1xd4BCgkgqOC8bzhOdoVK6RWNFWkHcrQ3XVdOHfzb/I3lJFEQQBzfY0Q1xGHDa32YxHcY/w+7XfYahhiE1tNonzjhx9cRTTL08XexS+z0LHAgubLCywkiIuIw7aqtrFmscgU5aJjJyMDyb3giBg0sVJOBZyDC0qtsDiZosL7bHxJP4Jeh3pJf7+tNSzxK4Ou6CnrvfBeJSNCfn/MCEnKn3Z8mzsfLQTzS2bFzh2WRAE/HHrDzxPeI7fv/u9wF+aMekx8Lnvgw62HQqdlfvf7mz4WdyPvY8elXugrE5ZCIKAfU/34Y+bf6CBRYN8qwAAuffuYVxuF+rWVq2LPfFOWnYa5l+bjyPPcyfAGl1rdKHjyj9GSlYKBp8cjEdxj6ClqgVbA1tY6luicfnG6GDbAXuC92Du1blwNHbEHo89eBL/BF39ukICCeyN7MUvbG2t22Jh09wvkz+f/xknQk+gnnk9zGs8D5deXcLMoJkw0zLD8a7HFb5IvispKwnjzo7L1w1fAgl01XWRnJUMMy0zrGy5Evuf7sfu4N1imdpmtZGWkyauDgHkJmHTGk5TSIjvvb2HmUEzERwfDGNNY+zpsEdhZvGpgVNx8NlB9KzSE50qdULvo72hKlHF0S5HFWb9PxN+BtsebUPnSp3R3rY9ehzugeD4YLGSZc2dNVh9ezV01HSwv+N+hWMvvLyA709/DzWpGg55HhIrGCKSI+BxwEPs8l9RryImXpyIy68vo0mFJviz5Z8K9yUmPQajAkbhUdwj9HXsi1/q/ZLv/RSdFo0hJ4eILbDv01PXg383f9yKuoVRp0fBXMccp7qegkQiUZiw0cfdR/xiGxwXjOMhx3E85Dhep76GrpoudrbfKVbmvFtZYKRhhD+a/4E6ZevgYexD9DzSEyoSFSxosgATzk+ApoomzvQ4U+DvkNPhpzHu7DhoqWrhWJdjSM1Oxewrs3E18irKapfFjvY7cPDZQaz8p/AhBepSdfR06ImhTkMRmhiK36/9Lq5qUBAbAxu4lnNFkwpNUN+ifr77ufPxTsy7Og/W+tY41OmQuD9vJRBHY0f82fJPPI1/immXpyEqLQpltctCS1ULoUmhUJGo5Ouib61vjfWt10NLVQunw0/j4LOD+Cf6H4UyqlJVNKvQDHMazxG7NWfLs/F9wPcIigyCmbYZ5rjOwYTzE5CUlSQe16tKL0QkRyDwdSCqmlSFnYEdDr84DBWJCuSCXKE13ETTBPMaz0Oj8o0A5P6uWvnPSqy/lzs8xVDDEAOrDURIYohYsfW+KkZVYG9kjyMvjkBNqoYfav+AM+FnxJZbM20ztLFug3K65ZCYmYi7b++KvX2s9a0RlRaF9Jx08XxGGkaob1Ef1ctUh72hPTbe34hrb66J++tb1MealmsUEsWxZ8bibMRZsfeSqkQVFfUrIiQxBDpqOljefDnqW9TH2fCzGHt2LEw0TRDQPQCqUlWkZafhRtQNuJi7KCQniZmJ2PZoG7Y93CZObmqmbYaeVXpCS1ULwXHBCAgPQGp2KmqY1sCPdX7Ej+d+FJN2Qw1DHPA8IPbwWX93PVb8swLldctjtutsDD45GDpqOrjQ8wLUVdSRkpUiDg3TUdOBXJDD94kv/rj5h3h9Y01jlNUui7I6ZXEn+g7iM+PxQ+0f4G7ljnYH2kECCY51OYYKehWQnJWM8ORwlNcpr5BIRqZEoueRnojPjIe7tTviM+IV7u+7tFS14FrOFf2q9kNts9pihfWrlFfodLATMmQZmNt4Liz1LDH+3HjEpMfkO4eJpgk0VTUVKhKt9a1ha2CL229vi/dLV00Xk+tPhoeth3idvIqWwlrJ03PSMfD4QDyKeyT2Qns3Kc+WZUMqkZa4h+PVyKsYfXo0MmQZUJGooJ55PXSq1CnfCkiCIIhz1uT5pd4v6F+1f4mu+zT+Kbr4dYGWqhYCewUCEqDv0dzVWQw1DLHGbQ2exj/F9MvTIUCAhooGpBIppBIpzLXNUUGvAp4nPMfLlJdQkahgdK3RGFx9sPi7K/BVIMaeGQszbTNsb7+9wN5D7742rxNe+Cf6HzSt2BQDqw5EnbJ1Cmy8eHeeIwBws3TDwqYL8yXl2fJs8fU0KtcIIYkhiEyNRIuKLbCs+bJPHvrwuTEh/x8m5ET0tfnY9dU/Rl4XvtIUmx6LoaeG5pt5v6ZpTaTmpOJp/FNMqDsBA6sNBACMOTMG5yLOieW0VbWxq8MucaKrp/FP0eNID+TIc7vla6pqIikrqdhfShIyEvAo7hGexD+BiZYJXMu5QibIMOzUsHwx9qjcA4OqD0IFvQqQyWXwe+6HVbdXQVtVG4ubLkYV4yr5zp+ek45+x/rhSfwT1DCtgU3um6CmooZsWTaa7mmK5Kxk/O3+N+qZ1xOHb3jYemC262yoSFVw/c11DPcfLtbm2xrY4kXiC2ipasG/mz8MNAwgk8sw6OQg/BP9D+qZ18OG1hsglUiRLc9GV7+uCEkMgVc1L/xU9yeF2PJmszfWNEZiZmLupHsSVRzwPFBgN9JseTZep7wucNhDHrkgR2hiKO7H3sfdt3dxM+qmeB9HOY/CyJojkZ6TjsY7GyNLnoVDnodgomWCXkd64WXKS/Ss0hNTGkwp8NpDTw7FrehbsDOww/b223E85DgWXl+I9Jx0WOpZYo3bGljqW4rH5LVy5bVYFzWZlSAI6HesH+7G3IWBhoE4jEBLVQtb2m4RhzXkLdtmrmMOR2NH2BjYQFtNG1qqWiivW14cBgPktrAGRQYhLiMOGTkZyJHnwETLBGW1c2ev/1A38dTsVLTc2xKp2alY67YWruVdIQgCOh7siNCkUIVVDN6kvsFw/+Fi63VZ7bJY0mwJsmXZWHh9IR7FPYKDsQPWuq3NN3zicdxj7Aneg1tRtxCRHCEunelUxglr3NYgPScdEy9MxK3oW9BS1cLmNpvhaOKIW1G34O3vjUxZJlpatsSSpksQkx6DLn5dxERdRaKCJU2XoIJeBfxx8w8Evg6Ea3lXzHWdW+AwDr/nfojPiEdX+65iK9+T+CfY8mALniY8RXhSONJz0tGpUidMcpkENakaJpyfgIDwAPEcalI1aKhoiMnku1Slqvi+5vfwquaFLFkWzkacxenw0wh6HVRgeS1VLQyuPhh/3/8b6TnpaG/bHvMaz4NUIkVIYojYQ+eQ5yGsu7sOx0KOAchNYNe6rRUriLPl2XDb64a4jDj82fJPNCzXUHw/W+tbY7brbDiVccLeJ3ux8p+V4v2zN7LHMKdhcLNyU0gu7sfcx3D/4QoVIo7GjpALcgTHB6NphaZY2WIlzkWcw+RLk5GanYr5381HO5t2cNvrhrfpb7G65Wo0LNcQXie8cOftHWioaKBphaaISY8RKzUKqtQBchPb/R33Q01FTfycDXUaCgdjB8wKmiXGpa+uD3MdcxhoGCAyJRIvU17C0dgRW9pugZpUDX/f/xv7nu6DoYYhLPUsxcq5dydNrVGmBlpZtYK+hj6OhRzD1cirqFu2Lv52/xsSiQQx6TE48vwIXiS+QFhSGHLkOehs3xkedh6QSqQ4+iJ3+MO7PTvyfkZtbdpiYNWBBU7eNuzUMFyJvILG5Rujmkk1RKVFQU2qBlNtU9x9exeXXl2CkYYRNrXZhIXXFyLwdaA4kWS2PBsmmiaY/938AucRuRl1ExMvTEQlw0ro49gHjcs3Fv+GX39zHaMCRiFDlqEwgS2QO+/HtIbTxN/Beb2s1KRq8KrmhfX31qOcTjkc7XK0RH+784Z7vTspbEJGAkadHoV7MfegpaolVmJ1r9wdUxpMyffdIyUrBbOvzBY/Cy0tW2Je43l4lvAMQ08NFY+vZ14P61qtK7QlOyAsAOPPjVfY1tCiIVa1XKVQ0X4q9BR+Ov+TGNPBZweRLc9GK6tWmNd4nkJl16p/VmHd3XUw0DDAQc+DeJP6BgOOD0C2PLvQOUS+JkzI/4cJORHRp8uR5yA0MRRhyWF4GPsQWx9uFf9ISyBBQPcAcabuxMxEXHh5AYYahjDXMUd53fL5usjffXsXy24tE1u7jTWNcbzL8UK70hdHfEY8vP298TjuMcy0zTDbdTYalWuUr1xxKkQikiLQ82hPJGclo0flHvi1/q8IfB2I709/jzJaZRDQLQAqUhVci7yGIadyvxA0KtcI3jW8MebMGCRnJaO6SXU8T3wu3qd35wzIu0bXw12RnpOOn+v+jA52HTDnyhz4h/nDSMMIR7ocyTeBWmhiKDwPeYrdR6sYVYF3DW+x629picuIQ3hSOGqY1hDvU96X+HG1x+HSq0u4EXUDFjoW2N9xf6HdLd+mvUWPIz0Qkx6DMlplxFaxumXrYkmzJflaW95tdQcKnrG/sPKqElW4WLjAu4a3Uif9+f3a79j+aDuqGFXBlrZb8CjuEbxOeEFLVQtnup9RuFeJmYmYfnk6VKWq+LX+r+L9kMlluB97H1WMqnywm6hckON29G2MPTsWiZmJsNa3RlxGHJKykqCjpoNFTRbhuwrfieVvR9/G7ejb6OXQSzx33uoZUokUC75bgDY2bcTysemxMNY0LnFLVN541XdbqTNlmfjh7A+4G30X3Sp3Q7+q/WCoYYiLry7idNhpZMgyYKhhCCNNI7S1blvgHA3Z8mzcib6DW9G38CDmAR7FPUJFvYqY0mAKbAxscOnVJYw5PQY5Qg7a27bHZJfJWH5rOfY+2SvOYZEly8Kvl37Fy+SXWNBkQb6KqwXXFmDbo21obdUaZXXKKkyyKIEEFfQqiMu3VjKshFE1R6GlZctCf7cExwXD298bcRlxqFGmBta0WoOo1Cj0PNIT2fJsVDOpJg7jcDZ1xpa2WyCVSDE7aDb2PNmDrvZdUVanLFbfXp3v3FqqWvih9g/oWSX391ZUWhSiUqMQlRaF+IzcFu68Sjv/MH/8eO5HqEpVkSPPHQaho6ajMPdLHkMNQ+zqsKvIcdlyQY7HcY+x98le+D3zEyuI8qhKVbHPY584N0JxxWXE4VHsIzxLeAYbAxs0LNewyInobkXdwsATAwvdrypVxcbWG1G7bG1kyjIx/ux4XHx1UbGMRBXTGk5TmBsnMiUSvY72Uki08+Z60FPXw5nwM7mVluUbY3nz5XiT+gZHXhyBz30fZMgyoC5VRzvbdqhuUh17n+xFcHwwBlQdgDG1xqC1b2vEZ8YrDFuQC3KcDD2J9ffWIyYtBsaaxjDSNIJUIkWWLAsqUhUMrj4YTSo0wfiz4xEQHpBv5vzU7FSMPTNWbInv59gPv9T7pdDPcd6Sw3OvzkW2PBuVDCshJj0GCZkJqG1WG4/jHiMtJw19HftiYr2JCEkKQWhiKBqWawgtVS3I5DJxGFFX+66QSCTie+HdYR/vzrGT9zfxwssLGHd2HLLl2aigWwGT60+GrYEtFl1fhDMRZwAAi5osEn8v7X2yF7OCZqG+RX2sc1v3Vc/bxIT8f5iQExGVvjepb7Dg2gIEhAcojHP+GIIg4ErkFRx4egAd7DrkG2dcEilZKbj0+hIalWuUL5n9WHldx4Hcrtt6anp4nfo63yzah58fxqygWeIYZCC398D61rkTq638ZyUikiOwuOnifC2seRM3qUvVoaOmg/jMeKhIVDC38Vy0t21fYFynw08jIikCzSo2K/Gs2yWR1+06r7unrpouNrfd/MHJdW5F3cKQk0OQI+SI3ZT7V+1fYNIiCAJ6H+2NB7EPYK5jjhNdTnzwy1ZAWADSc9LRpEITpc/eDeR+Nnoe6Ym4jDg0q9AMWmpaOB5y/LPPAP00/imG+w8XJ9irZlINi5osEoc8fMiZ8DMw1jT+YhNd5X39/JxdTg89O4SpgVMhQEAZrTJIzkpGpiwTm9psKlalzaPYR+hxpIdCq/OsRrNwI+oG/J77AchtUR5dazS6V+5erBbO1ymvcenVJbS3bS8OL3h3eUEViQoGVhuIEc4jxInNLr+6jOEBw6Grpov0nHTIBBnmfzcftga2OB5yHFmyLAysNlBh6EtRsuXZaO3bGjHpMZBKpBjqNBQjnEcgW5aNiOQIxKbHIjErEUmZSahnXu+jEumY9Bjse7IPIUkhSM5KRlp2GjradVRIcD+ntXfW4l7Mvdzu+tplIRNkiE6LRmxGLDrZdVKYyE4QBLxMeQlViSo0VDWw8PpCHH1xFEBuAjuo+iDoq+tj4ImBeBj7EA7GDnAxd8GBpwfEOQ3yuJZzxfIWy6GhoiFui0iOwOyg2QiKDFIoq6umi+NdjsNQ0xB/3v4Ta++sRXWT6hhTawwikiPg+9RXYXhVQVSlqviz5Z/45cIvSMxMLHAJzkxZJtbcXgMzbTP0duhdrM/a7ejbCkMKnMo4YUPrDbmVsWfHAcj9e5g3qWWNMjWw2m01Lr26hEkXJ0FPXQ8nup6Avrq+OOwDyK1cNdY0xqCTg5CclYzG5RtjZYuV4mfm8uvLmBo4VZzXI+/vjIpEBUOchmBMrTEKP7eTYSfhZulW6j0CSxsT8v9hQk5E9Pm8Snkljvv7Fu18vBMr/1kpfvkAgM1tNueb7C04Lhg/nvsR4cnhsNK3wta2WxW6QhdGEASMPD1SnGCuslFlzHKd9dknKSyJvLGKQO6XpdVuqz+4RFyeU6GnEBAegGFOwz64tNjVyKsYf3Y8xtUZJ3bv/re58/YOhpwcojCB0vtL/X0OEUkRmHttLqqZVMOIGiM+eTmlb8Ht6NuYGjhVnCehRpka2NZuW7GSE0EQ0O1wN3HiyHdnxL786jLuxdxDjyo9ivVZL4pMLsP0y9MRmxGLcbXH5RtKky3LRtPdTcUksDgTvH3I2fCz2P90PwY7Deaa1f8jCAL+vP2nuPqGikQFFfUqIjQpVKGnQFp2Gq5GXkV8ZjySMpOgo64DD1uPAv8O5q3mcDPqJh7GPURYYhi8a3iLFRQx6TFw93XP16tAV00XA6sNRPOKzZGQmYD4jHgIEKAuVceRF0cQEB4grtihpaqFwN6BH7WMXVGiUqMwNXAqMmWZWNZ8mfj+Xn17NdbcWQMA4lj09Jx02BvZIyMnAxHJERhTawy8a3iL58rr3WGqZQqZIENcRhxqmtbEulbr8vWIS81Oxdo7a7H14VbIBBnqW9THZJfJ4sR0/0ZMyP+HCTkREX0KmVyGB7EPcCXyCvTU9dCrSq8Cv8wnZyXDP8wfTSo0KXD5tcLEpMdg3tV5cDR2hFc1r682iRIEAe0PtEdEckS+5Q4pv5OhJzHhfG43zSpGVbDXY+9XPwHRtyqvpfBMxBnMaDgjX4VaUfIm6nMxd8G6VuuU1iI36eIkHH1xFOY65vD18P0qeoN8q86Gn8WmB5sUxuWva7UO9S3qf5brrbm9BlsfbRXnqqhqUhV9HPoUOlN7liwLowJG4eqb3GVMXcu7Yq3bh2fM/1R5lQv66vpwMHZAaFIovP29xdZ0Iw0jHO96XOz5AeTOydLjcA+xQszR2BEb3DcU2YMtLCkM0WnRqFu27r/+dyYT8v9hQk5ERFQ6XqW8wtu0t0pdv/ffZPODzVh+aznmfTcPbazbfPgA+urIBTluvLkBZzNnhe7IX1pIYghW/bMKg50Gf5U9aL5FT+Kf4NiLY3AydUJLy+Kt2f6lpGSlYNDJQXgc9zjfMotfUnhSOIadGobXqa/xc92fMaDagHxlHsY+xNCTQ1FOtxzWt17/yT1K/k2YkP8PE3IiIiJSFrkg/ywrKhDRf1tKVgquRl5FkwpNlNqzKjEzEY/jHsPF3KXQFu3U7FRoqWr9534XMiH/HybkRERERERE9CUVNw/9b1VTEBEREREREX0lmJATERERERERKQETciIiIiIiIiIlYEJOREREREREpARMyImIiIiIiIiUgAk5ERERERERkRIwISciIiIiIiJSAibkRERERERERErAhJyIiIiIiIhICZiQExERERERESkBE3IiIiIiIiIiJWBCTkRERERERKQETMiJiIiIiIiIlIAJOREREREREZESMCEnIiIiIiIiUgIm5ERERERERERKwISciIiIiIiISAmYkBMREREREREpARNyIiIiIiIiIiVgQk5ERERERESkBEzIiYiIiIiIiJSACTkRERERERGREjAhJyIiIiIiIlICJuRERERERERESsCEnIiIiIiIiEgJmJATERERERERKQETciIiIiIiIiIlYEJOREREREREpARMyImIiIiIiIiUgAk5ERERERERkRIwISciIiIiIiJSAibkRERERERERErAhJyIiIiIiIhICZiQExERERERESkBE3IiIiIiIiIiJWBCTkRERERERKQETMiJiIiIiIiIlIAJOREREREREZESMCEnIiIiIiIiUgIm5ERERERERERKwISciIiIiIiISAmYkBMREREREREpARNyIiIiIiIiIiVgQk5ERERERESkBEzIiYiIiIiIiJSACTkRERERERGREjAhJyIiIiIiIlICJuRERERERERESsCEnIiIiIiIiEgJmJATERERERERKQETciIiIiIiIiIlYEJOREREREREpARMyImIiIiIiIiUgAk5ERERERERkRIwISciIiIiIiJSAibkRERERERERErAhJyIiIiIiIhICZiQExERERERESkBE3IiIiIiIiIiJWBCTkRERERERKQETMiJiIiIiIiIlIAJOREREREREZESMCEnIiIiIiIiUgIm5ERERERERERKwISciIiIiIiISAm+6oRcJpNh6tSpsLGxgZaWFuzs7DB79mwIgqDs0IiIiIiIiIg+iaqyAyjKggULsGbNGmzevBnVqlXDjRs3MGjQIBgYGGDs2LHKDo+IiIiIiIioxL7qhPzy5cvw9PRE+/btAQDW1tbYuXMnrl27puTIiIiIiIiIiD7NV91lvVGjRjh9+jSePHkCALhz5w4uXbqEtm3bFnpMZmYmkpKSFB5EREREREREX5uvuoV80qRJSEpKgoODA1RUVCCTyTB37lz07du30GPmz5+PmTNnfsEoiYiIiIiIiD7eV91CvmfPHmzfvh07duzArVu3sHnzZixevBibN28u9JjJkycjMTFRfERERHzBiImIiIiIiIiKRyJ8xVOWV6xYEZMmTcL3338vbpszZw62bduGx48fF+scSUlJMDAwQGJiIvT19T9XqEREREREREQAip+HftUt5GlpaZBKFUNUUVGBXC5XUkREREREREREpeOrHkPu4eGBuXPnwtLSEtWqVcM///yDpUuXYvDgwcoOjYiIiIiIiOiTfNVd1pOTkzF16lQcOHAA0dHRKFeuHHr37o1p06ZBXV29WOdgl3UiIiIiIiL6koqbh37VCXlpYEJOREREREREX9I3MYaciIiIiIiI6FvFhJyIiIiIiIhICZiQExERERERESkBE3IiIiIiIiIiJWBCTkRERERERKQETMiJiIiIiIiIlIAJOREREREREZESMCEnIiIiIiIiUgIm5ERERERERERKwISciIiIiIiISAmYkBMREREREREpARNyIiIiIiIiIiVgQk5ERERERESkBEzIiYiIiIiIiJSACTkRERERERGREjAhJyIiIiIiIlICJuRERERERERESsCEnIiIiIiIiEgJmJATERERERERKQETciIiIiIiIiIlYEJOREREREREpARMyImIiIiIiIiUgAk5ERERERERkRIwISciIiIiIiJSAibkRERERERERErAhJyIiIiIiIhICZiQExERERERESkBE3IiIiIiIiIiJWBCTkRERERERKQETMiJiIiIiIiIlIAJOREREREREZESMCEnIiIiIiIiUgIm5ERERERERERKwISciIiIiIiISAmYkBMREREREREpARNyIiIiIiIiIiVgQk5ERERERESkBEzIiYiIiIiIiJSACTkRERERERGREjAhJyIiIiIiIlICJuRERERERERESsCEnIiIiIiIiEgJmJATERERERERKQETciIiIiIiIiIlYEJOREREREREpARMyImIiIiIiIiUgAk5ERERERERkRIwISciIiIiIiJSAibkREREREREREqgquwAiIiIiIiIvgSZTIbs7Gxlh0HfADU1NaioqHzyeZiQExERERHRN00QBLx58wYJCQnKDoW+IYaGhjA3N4dEIinxOZiQExERERHRNy0vGTczM4O2tvYnJVBEgiAgLS0N0dHRAAALC4sSn4sJORERERERfbNkMpmYjJuYmCg7HPpGaGlpAQCio6NhZmZW4u7rnNSNiIiIiIi+WXljxrW1tZUcCX1r8t5TnzIvARNyIiIiIiL65rGbOpW20nhPMSEnIiIiIiIiUgIm5ERERERERP8R1tbWWLZsmbLD+CjNmjXDuHHjCt0/Y8YM1KxZ84vFU5qYkBMREREREX1lJBJJkY8ZM2aU6LzXr1+Ht7f3J8VW0qT+31gZ8LlxlnUiIiIiIqKvTGRkpPj/3bt3Y9q0aQgODha36erqiv8XBAEymQyqqh9O70xNTUs3UPokbCEnIiIiIqL/FEEQkJaV88UfgiAUO0Zzc3PxYWBgAIlEIj5//Pgx9PT0cPz4cdSpUwcaGhq4dOkSnj9/Dk9PT5QtWxa6urqoV68eAgICFM77fiu1RCLBhg0b0LlzZ2hra8Pe3h5+fn6FxtWsWTOEhYVh/PjxYmt9nn379qFatWrQ0NCAtbU1lixZ8sHjYmNj0bt3b5QvXx7a2tpwcnLCzp07i32fCiKXyzFr1ixUqFABGhoaqFmzJk6cOCHuz8rKwujRo2FhYQFNTU1YWVlh/vz5AHLfGzNmzIClpSU0NDRQrlw5jB079pPiKQpbyImIiIiI6D8lPVuGqtNOfvHrPpzlDm310kvBJk2ahMWLF8PW1hZGRkaIiIhAu3btMHfuXGhoaGDLli3w8PBAcHAwLC0tCz3PzJkzsXDhQixatAgrV65E3759ERYWBmNj43xl9+/fD2dnZ3h7e2PYsGHi9ps3b6JHjx6YMWMGevbsicuXL2PUqFEwMTGBl5dXocdlZGSgTp06mDhxIvT19XH06FH0798fdnZ2cHFxKdF9Wb58OZYsWYJ169ahVq1a+Pvvv9GxY0c8ePAA9vb2WLFiBfz8/LBnzx5YWloiIiICERERAHIrFf744w/s2rUL1apVw5s3b3Dnzp0SxVEcTMiJiIiIiIj+hWbNmoVWrVqJz42NjeHs7Cw+nz17Ng4cOAA/Pz+MHj260PN4eXmhd+/eAIB58+ZhxYoVuHbtGtq0aZOvrLGxMVRUVKCnpwdzc3Nx+9KlS9GyZUtMnToVAFC5cmU8fPgQixYtgpeXV6HHlS9fHhMmTBCfjxkzBidPnsSePXtKnJAvXrwYEydORK9evQAACxYswNmzZ7Fs2TL8+eefCA8Ph729PRo3bgyJRAIrKyvx2PDwcJibm8PNzQ1qamqwtLQscRzFwYSciIiIiIj+U7TUVPBwlrtSrlua6tatq/A8JSUFM2bMwNGjRxEZGYmcnBykp6cjPDy8yPPUqFFD/L+Ojg709fURHR39UbE8evQInp6eCttcXV2xbNkyyGQyqKgU/NplMhnmzZuHPXv24NWrV8jKykJmZia0tbU/6vp5kpKS8Pr1a7i6uuaLJa+l28vLC61atUKVKlXQpk0bdOjQAa1btwYAdO/eHcuWLYOtrS3atGmDdu3awcPDo1jj80uCCTkREREREf2nSCSSUu06riw6OjoKzydMmAB/f38sXrwYlSpVgpaWFrp164asrKwiz6OmpqbwXCKRQC6Xl3q8BVm0aBGWL1+OZcuWwcnJCTo6Ohg3btwHY/4UtWvXRkhICI4fP46AgAD06NEDbm5u8PX1RcWKFREcHIyAgAD4+/tj1KhRWLRoEc6fP5/vPpUGTupGRERERET0DQgMDISXlxc6d+4MJycnmJubIzQ0tNSvo66uDplMprDN0dERgYGB+eKpXLmy2Dpe0HGBgYHw9PREv3794OzsDFtbWzx58qTEsenr66NcuXIFxlK1alWFcj179sT69euxe/du7Nu3D3FxcQAALS0teHh4YMWKFTh37hyCgoJw7969EsdUlH9/tRARERERERHB3t4e+/fvh4eHByQSCaZOnfpZWrqtra1x4cIF9OrVCxoaGihTpgx++ukn1KtXD7Nnz0bPnj0RFBSEVatWYfXq1UUeZ29vD19fX1y+fBlGRkZYunQpoqKiFJLnj/Xzzz9j+vTpsLOzQ82aNeHj44Pbt29j+/btAHLHu1tYWKBWrVqQSqXYu3cvzM3NYWhoiE2bNkEmk6F+/frQ1tbGtm3boKWlpTDOvDSxhZyIiIiIiOgbsHTpUhgZGaFRo0bw8PCAu7s7ateuXerXmTVrFkJDQ2FnZyeua167dm3s2bMHu3btQvXq1TFt2jTMmjULXl5eRR43ZcoU1K5dG+7u7mjWrBnMzc3RqVOnT4pv7Nix+PHHH/HTTz/ByckJJ06cgJ+fH+zt7QEAenp6WLhwIerWrYt69eohNDQUx44dg1QqhaGhIdavXw9XV1fUqFEDAQEBOHz4MExMTD4ppsJIhI9ZDO9fKCkpCQYGBkhMTIS+vr6ywyEiIiIioi8oIyMDISEhsLGxgaamprLDoW9IUe+t4uahbCEnIiIiIiIiUgIm5ERERERERERKwISciIiIiIiISAmYkBMREREREREpARNyIiIiIiIiIiVgQk5ERERERESkBEzIiYiIiIiIiJSACTkRERERERGREjAhJyIiIiIiIlICJuRERERERETfqGbNmmHcuHHKDkO0adMmGBoaFro/NDQUEokEt2/f/mIxKRMTciIiIiIioq+Mh4cH2rRpU+C+ixcvQiKR4O7du598nRkzZqBmzZpf7DhSxISciIiIiIjoKzNkyBD4+/vj5cuX+fb5+Pigbt26qFGjhhIio9L01Sfkr169Qr9+/WBiYgItLS04OTnhxo0byg6LiIiIiIj+rQQByEr98g9BKHaIHTp0gKmpKTZt2qSwPSUlBXv37sWQIUMQGxuL3r17o3z58tDW1oaTkxN27txZ7Gts2rQJM2fOxJ07dyCRSCCRSMTrhYeHw9PTE7q6utDX10ePHj0QFRX1weOWLl0KJycn6OjooGLFihg1ahRSUlKKHVNBzp8/DxcXF2hoaMDCwgKTJk1CTk6OuN/X1xdOTk7Q0tKCiYkJ3NzckJqaCgA4d+4cXFxcoKOjA0NDQ7i6uiIsLOyT4ilNqsoOoCjx8fFwdXVF8+bNcfz4cZiamuLp06cwMjJSdmhERERERPRvlZ0GzCv35a/762tAXadYRVVVVTFgwABs2rQJv/32GyQSCQBg7969kMlk6N27N1JSUlCnTh1MnDgR+vr6OHr0KPr37w87Ozu4uLh88Bo9e/bE/fv3ceLECQQEBAAADAwMIJfLxWT8/PnzyMnJwffff4+ePXvi3LlzhR4HAFKpFCtWrICNjQ1evHiBUaNG4ZdffsHq1atLcsfw6tUrtGvXDl5eXtiyZQseP36MYcOGQVNTEzNmzEBkZCR69+6NhQsXonPnzkhOTsbFixchCAJycnLQqVMnDBs2DDt37kRWVhauXbsm3suvwVedkC9YsAAVK1aEj4+PuM3GxkaJEREREREREX0ZgwcPxqJFi3D+/Hk0a9YMQG539a5du8LAwAAGBgaYMGGCWH7MmDE4efIk9uzZU6yEXEtLC7q6ulBVVYW5ubm43d/fH/fu3UNISAgqVqwIANiyZQuqVauG69evo169egUeB0BhAjlra2vMmTMHI0aMKHFCvnr1alSsWBGrVq2CRCKBg4MDXr9+jYkTJ2LatGmIjIxETk4OunTpAisrKwCAk5MTACAuLg6JiYno0KED7OzsAACOjo4liuNz+aoTcj8/P7i7u6N79+44f/48ypcvj1GjRmHYsGGFHpOZmYnMzEzxeVJS0pcIlYiIiIiI/i3UtHNbq5Vx3Y/g4OCARo0a4e+//0azZs3w7NkzXLx4EbNmzQIAyGQyzJs3D3v27MGrV6+QlZWFzMxMaGt/3HXe9+jRI1SsWFFMxgGgatWqMDQ0xKNHj1CvXr1Cjw0ICMD8+fPx+PFjJCUlIScnBxkZGUhLSytRXI8ePULDhg0VWrVdXV2RkpKCly9fwtnZGS1btoSTkxPc3d3RunVrdOvWDUZGRjA2NoaXlxfc3d3RqlUruLm5oUePHrCwsPjoOD6Xr3oM+YsXL7BmzRrY29vj5MmTGDlyJMaOHYvNmzcXesz8+fPF2iIDAwOFNxEREREREREkktyu41/6UYKu0kOGDMG+ffuQnJwMHx8f2NnZoWnTpgCARYsWYfny5Zg4cSLOnj2L27dvw93dHVlZWaV9x4olNDQUHTp0QI0aNbBv3z7cvHkTf/75JwB8tphUVFTg7++P48ePo2rVqli5ciWqVKmCkJAQALk9CoKCgtCoUSPs3r0blStXxpUrVz5LLCXxVSfkcrkctWvXxrx581CrVi14e3tj2LBhWLt2baHHTJ48GYmJieIjIiLiC0ZMRERERERUenr06AGpVIodO3Zgy5YtGDx4sNhaHBgYCE9PT/Tr1w/Ozs6wtbXFkydPPur86urqkMlkCtscHR0RERGhkEs9fPgQCQkJqFq1aqHH3bx5E3K5HEuWLEGDBg1QuXJlvH79aT0RHB0dERQUBOGdCfECAwOhp6eHChUqAAAkEglcXV0xc+ZM/PPPP1BXV8eBAwfE8rVq1cLkyZNx+fJlVK9eHTt27PikmErTV52QW1hYiD/wPI6OjggPDy/0GA0NDejr6ys8iIiIiIiI/o10dXXRs2dPTJ48GZGRkfDy8hL32dvbw9/fH5cvX8ajR48wfPhwcSb04rK2tkZISAhu376NmJgYZGZmws3NDU5OTujbty9u3bqFa9euYcCAAWjatCnq1q1b6HGVKlVCdnY2Vq5ciRcvXmDr1q1FNqYWx6hRoxAREYExY8bg8ePHOHToEKZPn44ff/wRUqkUV69exbx583Djxg2Eh4dj//79ePv2LRwdHRESEoLJkycjKCgIYWFhOHXqFJ4+ffpVjSP/qhNyV1dXBAcHK2x78uSJOFifiIiIiIjoWzdkyBDEx8fD3d0d5cr9/+zwU6ZMQe3ateHu7o5mzZrB3NwcnTp1+qhzd+3aFW3atEHz5s1hamqKnTt3QiKR4NChQzAyMkKTJk3g5uYGW1tb7N69u8jjnJ2dsXTpUixYsADVq1fH9u3bMX/+/E967eXLl8exY8dw7do1ODs7Y8SIERgyZAimTJkCANDX18eFCxfQrl07VK5cGVOmTMGSJUvQtm1baGtr4/Hjx+jatSsqV64Mb29vfP/99xg+fPgnxVSaJILwEYvhfWHXr19Ho0aNMHPmTPTo0QPXrl3DsGHD8Ndff6Fv377FOkdSUhIMDAyQmJjI1nIiIiIiov+YjIwMhISEwMbGBpqamsoOh74hRb23ipuHftUt5PXq1cOBAwewc+dOVK9eHbNnz8ayZcuKnYwTERERERERfa2+6mXPAKBDhw7o0KGDssMgIiIiIiIiKlVfdQs5ERERERER0beKCTkRERERERGREjAhJyIiIiIiIlICJuRERERERERESsCEnIiIiIiIiEgJmJATERERERERKQETciIiIiIiIiIlYEJORERERET0H2FtbY1ly5YpO4xSIZFIcPDgQWWH8UmYkBMREREREX1lJBJJkY8ZM2aU6LzXr1+Ht7f3J8XWrFkzSCQS/P777/n2tW/fPl98ISEh6NOnD8qVKwdNTU1UqFABnp6eePz4sVimsNe5a9euT4r1a6eq7ACIiIiIiIhIUWRkpPj/3bt3Y9q0aQgODha36erqiv8XBAEymQyqqh9O70xNTUslvooVK2LTpk2YNGmSuO3Vq1c4ffo0LCwsxG3Z2dlo1aoVqlSpgv3798PCwgIvX77E8ePHkZCQoHBOHx8ftGnTRmGboaFhqcT7tWILORERERER/acIgoC07LQv/hAEodgxmpubiw8DAwNIJBLx+ePHj6Gnp4fjx4+jTp060NDQwKVLl/D8+XN4enqibNmy0NXVRb169RAQEKBw3ve7rEskEmzYsAGdO3eGtrY27O3t4efn98H4OnTogJiYGAQGBorbNm/ejNatW8PMzEzc9uDBAzx//hyrV69GgwYNYGVlBVdXV8yZMwcNGjRQOKehoaHC6zY3N4empmax79m9e/fQokULaGlpwcTEBN7e3khJSRH3nzt3Di4uLtDR0YGhoSFcXV0RFhYGALhz5w6aN28OPT096Ovro06dOrhx40axr11SbCEnIiIiIqL/lPScdNTfUf+LX/dqn6vQVtMutfNNmjQJixcvhq2tLYyMjBAREYF27dph7ty50NDQwJYtW+Dh4YHg4GBYWloWep6ZM2di4cKFWLRoEVauXIm+ffsiLCwMxsbGhR6jrq6Ovn37wsfHB66urgCATZs2YeHChQrd1U1NTSGVSuHr64tx48ZBRUWl1F7/u1JTU+Hu7o6GDRvi+vXriI6OxtChQzF69Ghs2rQJOTk56NSpE4YNG4adO3ciKysL165dg0QiAQD07dsXtWrVwpo1a6CiooLbt29DTU3ts8T6rhK1kEdERODly5fi82vXrmHcuHH466+/Si0wIiIiIiIiKtysWbPQqlUr2NnZwdjYGM7Ozhg+fDiqV68Oe3t7zJ49G3Z2dh9s8fby8kLv3r1RqVIlzJs3DykpKbh27doHrz948GDs2bMHqampuHDhAhITE9GhQweFMuXLl8eKFSswbdo0GBkZoUWLFpg9ezZevHiR73y9e/eGrq6uwiM8PLxY92LHjh3IyMjAli1bUL16dbRo0QKrVq3C1q1bERUVhaSkJDE+Ozs7ODo6YuDAgWJFRXh4ONzc3ODg4AB7e3t0794dzs7Oxbr2pyhRC3mfPn3g7e2N/v37482bN2jVqhWqVauG7du3482bN5g2bVppx0lERERERFQqtFS1cLXPVaVctzTVrVtX4XlKSgpmzJiBo0ePIjIyEjk5OUhPT/9gUlujRg3x/zo6OtDX10d0dPQHr+/s7Ax7e3v4+vri7Nmz6N+/f4Hj2L///nsMGDAA586dw5UrV7B3717MmzcPfn5+aNWqlVjujz/+gJubm8Kx5cqV+2AcAPDo0SM4OztDR0dH3Obq6gq5XI7g4GA0adIEXl5ecHd3R6tWreDm5oYePXqI491//PFHDB06FFu3boWbmxu6d+8OOzu7Yl37U5Sohfz+/ftwcXEBAOzZswfVq1fH5cuXsX37dmzatKk04yMiIiIiIipVEokE2mraX/yR1z26tLybfALAhAkTcODAAcybNw8XL17E7du34eTkhKysrCLP837XbIlEArlcXqwYBg8ejD///BO+vr4YPHhwoeX09PTg4eGBuXPn4s6dO/juu+8wZ84chTLm5uaoVKmSwqM4E9UVl4+PD4KCgtCoUSPs3r0blStXxpUrVwAAM2bMwIMHD9C+fXucOXMGVatWxYEDB0rt2oUpUUKenZ0NDQ0NAEBAQAA6duwIAHBwcFCYDZCIiIiIiIi+jMDAQHh5eaFz585wcnKCubk5QkNDP+s1+/Tpg3v37qF69eqoWrVqsY6RSCRwcHBAampqqcXh6OiIO3fuKJwzMDAQUqkUVapUEbfVqlULkydPxuXLl1G9enXs2LFD3Fe5cmWMHz8ep06dQpcuXeDj41Nq8RWmRAl5tWrVsHbtWly8eBH+/v7i1PSvX7+GiYlJqQZIREREREREH2Zvb4/9+/fj9u3buHPnDvr06VPslu6SMjIyQmRkJE6fPl3g/tu3b8PT0xO+vr54+PAhnj17ho0bN+Lvv/+Gp6enQtmEhAS8efNG4VHcpL1v377Q1NTEwIEDcf/+fZw9exZjxoxB//79UbZsWYSEhGDy5MkICgpCWFgYTp06hadPn8LR0RHp6ekYPXo0zp07h7CwMAQGBuL69etwdHT85PvzISVq/1+wYAE6d+6MRYsWYeDAgeJgdz8/P7ErOxEREREREX05S5cuxeDBg9GoUSOUKVMGEydORFJS0me/blFrhVeoUAHW1taYOXMmQkNDIZFIxOfjx49XKDto0KB8x8+fP19hrfPCaGtr4+TJk/jhhx9Qr149aGtro2vXrli6dKm4//Hjx9i8eTNiY2NhYWGB77//HsOHD0dOTg5iY2MxYMAAREVFoUyZMujSpQtmzpz5cTeiBCTCxyyG9w6ZTIakpCQYGRmJ20JDQ6Gtra2w7pyyJSUlwcDAAImJidDX11d2OERERERE9AVlZGQgJCQENjY2H7WmNdGHFPXeKm4eWqIu6+np6cjMzBST8bCwMCxbtgzBwcFfVTJORERERERE9LUqUULu6emJLVu2AMjt51+/fn0sWbIEnTp1wpo1a0o1QCIiIiIiIqJvUYkS8lu3buG7774DAPj6+qJs2bIICwvDli1bsGLFilINkIiIiIiIiOhbVKKEPC0tDXp6egAgTgkvlUrRoEEDhIWFlWqARERERERERN+iEiXklSpVwsGDBxEREYGTJ0+idevWAIDo6GhOnEZERERERERUDCVKyKdNm4YJEybA2toaLi4uaNiwIYDc1vJatWqVaoBERERERERE36ISrUPerVs3NG7cGJGRkeIa5ADQsmVLdO7cudSCIyIiIiIiIvpWlSghBwBzc3OYm5vj5cuXAHIXfHdxcSm1wIiIiIiIiIi+ZSXqsi6XyzFr1iwYGBjAysoKVlZWMDQ0xOzZsyGXy0s7RiIiIiIiIqJvTolayH/77Tds3LgRv//+O1xdXQEAly5dwowZM5CRkYG5c+eWapBERERERET08Zo1a4aaNWti2bJlyg6lSF5eXkhISMDBgweVHcoXVaIW8s2bN2PDhg0YOXIkatSogRo1amDUqFFYv349Nm3aVMohEhERERER/bd4eHigTZs2Be67ePEiJBIJ7t69+8nX2bRpEyQSCRwdHfPt27t3LyQSCaytrcVtMpkMv//+OxwcHKClpQVjY2PUr18fGzZsEMt4eXlBIpHkexT2ev7LStRCHhcXBwcHh3zbHRwcEBcX98lBERERERER/ZcNGTIEXbt2xcuXL1GhQgWFfT4+Pqhbty5q1KhRKtfS0dFBdHQ0goKCxBW0AGDjxo2wtLRUKDtz5kysW7cOq1atQt26dZGUlIQbN24gPj5eoVybNm3g4+OjsE1DQ6NU4v2WlKiF3NnZGatWrcq3fdWqVaX2piAiIiIiIvocBEGAPC3tiz8EQSh2jB06dICpqWm+HsgpKSnYu3cvhgwZgtjYWPTu3Rvly5eHtrY2nJycsHPnzo++H6qqqujTpw/+/vtvcdvLly9x7tw59OnTR6Gsn58fRo0ahe7du8PGxgbOzs4YMmQIJkyYoFBOQ0NDnAg872FkZFTsmDIzMzF27FiYmZlBU1MTjRs3xvXr18X98fHx6Nu3L0xNTaGlpQV7e3uxAiArKwujR4+GhYUFNDU1YWVlhfnz53/0ffkSStRCvnDhQrRv3x4BAQFiDUpQUBAiIiJw7NixUg2QiIiIiIioNAnp6QiuXeeLX7fKrZuQaGsXq6yqqioGDBiATZs24bfffoNEIgGQ241cJpOhd+/eSElJQZ06dTBx4kTo6+vj6NGj6N+/P+zs7D56BazBgwejWbNmWL58ObS1tbFp0ya0adMGZcuWVShnbm6OM2fOYNSoUTA1Nf2oa3yMX375Bfv27cPmzZthZWWFhQsXwt3dHc+ePYOxsTGmTp2Khw8f4vjx4yhTpgyePXuG9PR0AMCKFSvg5+eHPXv2wNLSEhEREYiIiPhssX6KErWQN23aFE+ePEHnzp2RkJCAhIQEdOnSBQ8ePMDWrVtLO0YiIiIiIqL/nMGDB+P58+c4f/68uM3Hxwddu3aFgYEBypcvjwkTJqBmzZqwtbXFmDFj0KZNG+zZs+ejr1WrVi3Y2trC19cXgiBg06ZNGDx4cL5yS5cuxdu3b2Fubo4aNWpgxIgROH78eL5yR44cga6ursJj3rx5xYolNTUVa9aswaJFi9C2bVtUrVoV69evh5aWFjZu3AgACA8PR61atVC3bl1YW1vDzc0NHh4e4j57e3s0btwYVlZWaNy4MXr37v3R9+RLKPE65OXKlcs3m/qdO3ewceNG/PXXX58cGBERERER0ecg0dJClVs3lXLdj+Hg4IBGjRrh77//RrNmzfDs2TNcvHgRs2bNApA7wdq8efOwZ88evHr1CllZWcjMzIR2MVvh3zd48GD4+PjA0tISqampaNeuXb6hylWrVsX9+/dx8+ZNBAYG4sKFC/Dw8ICXl5fCxG7NmzfHmjVrFI41NjYuVhzPnz9Hdna2uKIXAKipqcHFxQWPHj0CAIwcORJdu3bFrVu30Lp1a3Tq1AmNGjUCkDupXKtWrVClShW0adMGHTp0QOvWrUt0Tz63ErWQExERERER/VtJJBJItbW/+COv2/nHGDJkCPbt24fk5GT4+PjAzs4OTZs2BQAsWrQIy5cvx8SJE3H27Fncvn0b7u7uyMrKKtF96du3L65cuYIZM2agf//+UFUtuP1WKpWiXr16GDduHPbv349NmzZh48aNCAkJEcvo6OigUqVKCo/iJuTF0bZtW4SFhWH8+PF4/fo1WrZsKY5jr127NkJCQjB79mykp6ejR48e6NatW6lduzQxISciIiIiIvpK9ejRA1KpFDt27MCWLVswePBgMbEPDAyEp6cn+vXrB2dnZ9ja2uLJkyclvpaxsTE6duyI8+fPF9hdvTBVq1YFkNvVvDTY2dlBXV0dgYGB4rbs7Gxcv35dvBYAmJqaYuDAgdi2bRuWLVum0FNbX18fPXv2xPr167F7927s27fvq1wRrMRd1omIiIiIiOjz0tXVRc+ePTF58mQkJSXBy8tL3Gdvbw9fX19cvnwZRkZGWLp0KaKiohSS1o+1adMmrF69GiYmJgXu79atG1xdXdGoUSOYm5sjJCQEkydPRuXKlRWWxs7MzMSbN28UjlVVVUWZMmU+GIOOjg5GjhyJn3/+GcbGxrC0tMTChQuRlpaGIUOGAACmTZuGOnXqoFq1asjMzMSRI0fEtdSXLl0KCwsL1KpVC1KpFHv37oW5uTkMDQ1LeFc+n49KyLt06VLk/oSEhE+JhYiIiIiIiN4zZMgQbNy4Ee3atUO5cuXE7VOmTMGLFy/g7u4ObW1teHt7o1OnTkhMTCzxtbS0tKBVxFh3d3d37Ny5E/Pnz0diYiLMzc3RokULzJgxQ6GL+4kTJ2BhYaFwbJUqVfD48eNixfH7779DLpejf//+SE5ORt26dXHy5Elx6TR1dXVMnjwZoaGh0NLSwnfffYddu3YBAPT09LBw4UI8ffoUKioqqFevHo4dOwap9OvrIC4RPmIxvEGDBhWr3PsLwCtTUlISDAwMkJiYCH19fWWHQ0REREREX1BGRgZCQkJgY2MDTU1NZYdD35Ci3lvFzUM/qoX8a0q0iYiIiIiIiP7Nvr42eyIiIiIiIqL/ACbkRERERERERErAhJyIiIiIiIhICZiQExERERERESkBE3IiIiIiIiIiJWBCTkRERERERKQETMiJiIiIiIiIlIAJOREREREREZESMCEnIiIiIiL6j7C2tsayZcuUHUahJBIJDh48qOwwvhgm5ERERERERF8ZiURS5GPGjBklOu/169fh7e39yfE9e/YMgwYNQoUKFaChoQEbGxv07t0bN27c+OBr2LVr1ydf/1uhquwAiIiIiIiISFFkZKT4/927d2PatGkIDg4Wt+nq6or/FwQBMpkMqqofTu9MTU0/ObYbN26gZcuWqF69OtatWwcHBwckJyfj0KFD+Omnn3D+/HmxrI+PD9q0aaNwvKGh4SfH8K1gCzkREREREf2nCIKA7EzZF38IglDsGM3NzcWHgYEBJBKJ+Pzx48fQ09PD8ePHUadOHWhoaODSpUt4/vw5PD09UbZsWejq6qJevXoICAhQOO/7XdYlEgk2bNiAzp07Q1tbG/b29vDz8yvy3nl5ecHe3h4XL15E+/btYWdnh5o1a2L69Ok4dOiQQnlDQ0OF12Jubg5NTc1i34d79+6hRYsW0NLSgomJCby9vZGSkiLuP3fuHFxcXKCjowNDQ0O4uroiLCwMAHDnzh00b94cenp60NfXR506dRRa8L8GbCEnIiIiIqL/lJwsOf764fyHC5Yy7+VNoaahUmrnmzRpEhYvXgxbW1sYGRkhIiIC7dq1w9y5c6GhoYEtW7bAw8MDwcHBsLS0LPQ8M2fOxMKFC7Fo0SKsXLkSffv2RVhYGIyNjfOVvX37Nh48eIAdO3ZAKs3fvluard+pqalwd3dHw4YNcf36dURHR2Po0KEYPXo0Nm3ahJycHHTq1AnDhg3Dzp07kZWVhWvXrkEikQAA+vbti1q1amHNmjVQUVHB7du3oaamVmrxlQYm5ERERERERP9Cs2bNQqtWrcTnxsbGcHZ2Fp/Pnj0bBw4cgJ+fH0aPHl3oeby8vNC7d28AwLx587BixQpcu3YtX1dzAHj69CkAwMHBoVgx9u7dGyoqipUQDx8+LLKCIM+OHTuQkZGBLVu2QEdHBwCwatUqeHh4YMGCBVBTU0NiYiI6dOgAOzs7AICjo6N4fHh4OH7++WcxVnt7+2LF/CUxISciIiIiov8UVXUpvJc3Vcp1S1PdunUVnqekpGDGjBk4evQoIiMjkZOTg/T0dISHhxd5nho1aoj/19HRgb6+PqKjowss+zHd7gHgjz/+r717j5OjrvP9/66qvvdMzyWTyUwu5E64hXDHEESUSMKqP1BcXMyyIBw5YkDZVRdx1/sqLrqsuutyznHd4K4XFBR1WVC5CAgEBCSAEAK5kPttZjLTM33vqu/vj+rumUkmyUwSpibJ6/l49KO7q6qrvlX17ar6fD/f6v5nLVy4cNCwiRMnDuuzK1eu1Lx582rBuCQtWLBAnudp1apVOu+883TVVVdp0aJFeuc736mFCxfqsssuU3t7uyTpb/7mb/S//tf/0n/9139p4cKF+vM///Na4D5WcA85AAAAgKOKZVkKR51Rf1S7Uh8qAwNVSfrkJz+pe+65R1/96lf1+9//XitWrNDcuXNVLBb3OZ/du3FbliXP84ac9thjj5Ukvfrqq8MqY1tbm2bNmjXoMZwfnxuuZcuWafny5TrnnHP0k5/8RMcee6yeeuopSdIXvvAFvfzyy3rXu96lhx9+WCeccILuueeeQ7bsQ4GAHAAAAACOAE888YSuuuoqvfe979XcuXPV1tamN95445Au45RTTtEJJ5ygf/qnfxoyaO/u7j5kyzr++OP1wgsvKJPJ1IY98cQTsm1bc+bMqQ079dRTdfPNN+vJJ5/USSedpB/96Ee1cccee6z++q//Wr/97W/1vve9T8uWLTtk5TsUCMgBAAAA4Agwe/Zs/fznP9eKFSv0wgsv6IMf/OBeM90HyrIsLVu2TK+99pre+ta36r777tPatWv14osv6itf+YouvvjiQdN3d3dr27Ztgx4DA+x9WbJkiWKxmK688kr96U9/0u9+9zvdcMMNuuKKKzRhwgStW7dON998s5YvX67169frt7/9rV5//XUdf/zxyuVyuv766/XII49o/fr1euKJJ/TMM88Musd8LOAecgAAAAA4Atx22226+uqrdc4556ilpUU33XST0un0IV/OWWedpWeffVZf+cpX9OEPf1gdHR1qb2/XOeecM+gv1STpQx/60B6fv+WWW/TpT396v8tJJBL6zW9+o49//OM688wzlUgkdOmll+q2226rjX/11Vf1/e9/X52dnWpvb9fSpUv1v//3/1a5XFZnZ6f+6q/+Stu3b1dLS4ve97736Ytf/OIh2QaHimVGelf+YSadTquhoUE9PT1KpVJBFwcAAADAKMrn81q3bp2mT58+ov+/BvZnX3VruHEoXdYBAAAAAAgAATkAAAAAAAEgIAcAAAAAIAAE5AAAAAAABICAHAAAAACAABCQAwAAAAAQAAJyAAAAAAACQEAOAAAAAEAACMgBAAAAAAgAATkAAAAAHKHOP/983XjjjYEs+6qrrtIll1wSyLIPFwTkAAAAADDGvOc979HixYuHHPf73/9elmXpxRdfPCTLKhaLuvXWWzVv3jwlEgm1tLRowYIFWrZsmUqlkiQ/uLYsa4/H3sqI4QkFXQAAAAAAwGDXXHONLr30Um3atEmTJ08eNG7ZsmU644wzdPLJJx/0corFohYtWqQXXnhBX/7yl7VgwQKlUik99dRT+sY3vqFTTz1Vp5xyiiRp8eLFWrZs2aDPR6PRgy7D0YwMOQAAAICjijFGpXx+1B/GmGGX8d3vfrfGjx+vO+64Y9Dwvr4+3XXXXbrmmmvU2dmpyy+/XJMmTVIikdDcuXP14x//eETb4pvf/KYee+wxPfTQQ1q6dKlOOeUUzZgxQx/84Af19NNPa/bs2bVpo9Go2traBj2ampqGvaxCoaCPfexjam1tVSwW07nnnqtnnnmmNn7Xrl1asmSJxo8fr3g8rtmzZ9caAIrFoq6//nq1t7crFotp6tSpuuWWW0a0rmMRGXIAAAAAR5VyoaBvX/n+UV/ux75/t8Kx2LCmDYVC+qu/+ivdcccd+ru/+ztZliVJuuuuu+S6ri6//HL19fXp9NNP10033aRUKqX/+Z//0RVXXKGZM2fqrLPOGtZyfvjDH2rhwoU69dRT9xgXDocVDoeHv4L78bd/+7f62c9+pu9///uaOnWqbr31Vi1atEirV69Wc3OzPvvZz+qVV17R/fffr5aWFq1evVq5XE6S9O1vf1u/+tWv9NOf/lTHHHOMNm7cqI0bNx6ysgWFDDkAAAAAjEFXX3211qxZo0cffbQ2bNmyZbr00kvV0NCgSZMm6ZOf/GQtq33DDTdo8eLF+ulPfzrsZbz++us67rjjhjXtvffeq7q6ukGPr371q8P6bCaT0e23366vf/3ruuiii3TCCSfou9/9ruLxuL73ve9JkjZs2KBTTz1VZ5xxhqZNm6aFCxfqPe95T23c7Nmzde6552rq1Kk699xzdfnllw97PccqMuQAAAAAjiqhaFQf+/7dgSx3JI477jidc845+o//+A+df/75Wr16tX7/+9/rS1/6kiTJdV199atf1U9/+lNt3rxZxWJRhUJBiURi2MsYSTf6t7/97br99tsHDWtubh7WZ9esWaNSqaQFCxbUhoXDYZ111llauXKlJOm6667TpZdeqj/+8Y+68MILdckll+icc86R5P+o3Dvf+U7NmTNHixcv1rvf/W5deOGFwy77WEVADgAAAOCoYlnWsLuOB+2aa67RDTfcoO985ztatmyZZs6cqbe97W2SpK9//ev61re+pW9+85uaO3euksmkbrzxRhWLxWHP/9hjj9Wrr746rGmTyaRmzZp1QOsxHBdddJHWr1+v++67Tw888IAuuOACLV26VN/4xjd02mmnad26dbr//vv14IMP6rLLLtPChQt1992j37ByKNFlHQAAAADGqMsuu0y2betHP/qR/vM//1NXX3117X7yJ554QhdffLH+8i//UvPmzdOMGTP02muvjWj+H/zgB/Xggw/q+eef32NcqVRSJpM5JOsxc+ZMRSIRPfHEE4Pm/8wzz+iEE06oDRs/fryuvPJK/eAHP9A3v/lN/b//9/9q41KplD7wgQ/ou9/9rn7yk5/oZz/7mbq6ug5J+YJChhwAAAAAxqi6ujp94AMf0M0336x0Oq2rrrqqNm727Nm6++679eSTT6qpqUm33Xabtm/fPijA3Z8bb7xR//M//6MLLrhAX/7yl3Xuueeqvr5ezz77rP7xH/9R3/ve92p/e1YoFLRt27ZBnw+FQmppadnvcpLJpK677jp96lOfUnNzs4455hjdeuutymazuuaaayRJn/vc53T66afrxBNPVKFQ0L333qvjjz9eknTbbbepvb1dp556qmzb1l133aW2tjY1NjYOe13HosMqQ/61r31NlmXpxhtvDLooAAAAADAqrrnmGu3atUuLFi3SxIkTa8P//u//XqeddpoWLVqk888/X21tbbrkkktGNO9oNKoHHnhAf/u3f6v/+3//r97ylrfozDPP1Le//W197GMf00knnVSb9te//rXa29sHPc4999xhL+trX/uaLr30Ul1xxRU67bTTtHr1av3mN7+p/XVaJBLRzTffrJNPPlnnnXeeHMfRnXfeKUmqr6/XrbfeqjPOOENnnnmm3njjDd13332y7cMqpN2DZUZyF3+AnnnmGV122WVKpVJ6+9vfrm9+85vD+lw6nVZDQ4N6enqUSqXe3EICAAAAGFPy+bzWrVun6dOnK3aY3DeOw8O+6tZw49DDojmhr69PS5Ys0Xe/+939/vF8oVBQOp0e9AAAAAAAYKw5LALypUuX6l3vepcWLly432lvueUWNTQ01B5TpkwZhRICAAAAADAyYz4gv/POO/XHP/5Rt9xyy7Cmv/nmm9XT01N7bNy48U0uIQAAAAAAIzemf2V948aN+vjHP64HHnhg2Pd7RKNRRaPRN7lkAAAAAAAcnDEdkD/33HPasWOHTjvttNow13X12GOP6V//9V9VKBTkOE6AJQQAAABwODhMfssah5FDUafGdEB+wQUX6KWXXho07EMf+pCOO+443XTTTQTjAAAAAPYpHA5LkrLZrOLxeMClwZEkm81K6q9jB2JMB+T19fWD/vdO8v9Qfty4cXsMBwAAAIDdOY6jxsZG7dixQ5KUSCRkWVbApcLhzBijbDarHTt2qLGx8aASxWM6IAcAAACAg9XW1iZJtaAcOBQaGxtrdetAHXYB+SOPPBJ0EQAAAAAcRizLUnt7u1pbW1UqlYIuDo4A4XD4kNxCfdgF5AAAAABwIBzH4XeoMKaM+f8hBwAAAADgSERADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgAAQkAMAAAAAEAACcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgAAQkAMAAAAAEAACcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgAAQkAMAAAAAEAACcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgAAQkAMAAAAAEAACcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgAAQkAMAAAAAEAACcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgAAQkAMAAAAAEAACcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgAAQkAMAAAAAEAACcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgAAQkAMAAAAAEAACcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgAAQkAMAAAAAEAACcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAARgTAfkt9xyi84880zV19ertbVVl1xyiVatWhV0sQAAAAAAOGhjOiB/9NFHtXTpUj311FN64IEHVCqVdOGFFyqTyQRdNAAAAAAADopljDFBF2K4du7cqdbWVj366KM677zzhvWZdDqthoYG9fT0KJVKvcklBAAAAAAc7YYbh4ZGsUwHraenR5LU3Ny812kKhYIKhULtfTqdftPLBQAAAADASI3pLusDeZ6nG2+8UQsWLNBJJ5201+luueUWNTQ01B5TpkwZxVICAAAAADA8h02X9euuu07333+/Hn/8cU2ePHmv0w2VIZ8yZQpd1gEAAAAAo+KI6rJ+/fXX695779Vjjz22z2BckqLRqKLR6CiVDAAAAACAAzOmA3JjjG644Qbdc889euSRRzR9+vSgiwQAAAAAwCExpgPypUuX6kc/+pF++ctfqr6+Xtu2bZMkNTQ0KB6PB1w6AAAAAAAO3Ji+h9yyrCGHL1u2TFddddWw5sHfngEAAAAARtMRcQ/5GG4rAAAAAADgoBw2f3sGAAAAAMCRhIAcAAAAAIAAEJADAAAAABAAAnIAAAAAAAJAQA4AAAAAQAAIyAEAAAAACMCY/tuzo8nvVu1QvujKsvz/X7ctS5Yk297tvWXJtiRZkj/k4O3l795HPh9Jtu2XzxpUXkuW5b82MjJG8kz/s2f8v7gzkoxRZdqh52FZ/jSqTFudX1V1uup61T6n6nNlvgNfD/E5q7Kdh/M5VcplSfI8KZ0vKZ0vyRipKRlRcyKiSMhWyfXkekZlz6hceV3dj/aAsmaLrt7ozGh9Z1bFsqeJjXFNaoyrPrbn19UYqeR5KrtGRkbJSEipWFh1sZAce/COLZRdlVxT+Vz/Rhv454KWpHjYUcjpb6szxsj1jEquUdH1FAvbioackVWO/cgVXe3ozas+FlZDPCzHtlQou+rJlmRZllrqIrJGUFFNpV7tvg0OVNn1ZCSFndFvwzzU6wIAAICxg4B8jPjcL/+kjV25oIuBI0hdNFQL4ruzJeVK7rA/G3FshR1LJc+o5HqDGj0kP2hPxUMqu0bZoqui6ykRdlQfCykecfyGin1wbEvRkC3HtrSlO69t6XxtnGVJ0ZCtfMmrDUtGHE0dl1Q4ZGtXpqhd2aLqoiFNSMU0vj6qQtlTT66k3lxJPTm/QaTkGiUjjupjYUXDtt/440muZyoNQX6g63r+yjUnI5qQiqoxHlFfoazuXFHd2ZJ6siX1FsqSpPpYSM3JiGL7aZCwbUuJiKNExFE0ZKvkGpUrDSfVBhn/2ajkVRpqXL9MiYijulhYYdvSjt6CtqfzKpS92rqEQ5Y8zw/UE9GQmhJhNcQjSkQcxcK2YmF/mbGwI9cz2tqT1+bunLLFsuqiIdVF/caaalncAfu4NRXVtHFJTUhFlc6X1dFXULbg+vONOHIsSyXXU8k1ioZspeJhJSOOtvcWtHZnn7Z05zWxMaY5E+o1pTlRm0dnX0GdfUV19BUUdmzNaq3TrNY6NSUism3J9aQ3OjJauTWtdZ0ZRRxbyWhIsbDtbyPXU8ixNb4+qtb6qBIRR4WSp0LZUyRkqyEeHvRIRkPqzZe0K1tSd9bfj7uyRZVdo6ZkROOSERXKrjbtymlzd07Fsqdwpc4noiGlYqHK9yes+t1eZ4uuXtveq9e29yoWcjR3coPmTW5UUzJca2TszpbUlSkqnS/JtiyFbP+71NFb0M6+gnJFV45tybEtJSMhNSfDakxElIw6ioUc2bal7em8Nu3KaWdvQYWyq2LZKGRbmtIc1zHjkkpGnNr67coWtSvr1/2o4++X+lio8ggrFrbVV3DVmy+pq6+oLT05be7Oy7Gk6S11mjE+qVQsVPluqPb9KLlG6Zy/7Uqu0czxSR07oV7j66PqzZdr37WeXEnpXP/7vnxZDfGwJqSiGl8fVX3M3ych21JXpqjOvqKKrqv6WFh10ZDyJVfbevLa0VvQ5Ka4zp3dotOOadKmXTk9v2GX1nZkVBcNqSkRkSSt6+jTuo6MMgVXqXiott9TsbBS8bCyRVedfQV150oKO3bte1htWLStSmNpMqJoyFax7KnoekrnSurKlNSdKyoedtScjKgp4U/XlIwoZFtata1XK7emtStb9MfVRRQNOf73ouypKRnRjJakpo9Pqr0hrlTlGLx6R5+eWN2h1Tv7FA35x4aGeFiTm+Ka3JSQY1va2VtQR19B6VxJmaKrTKEsI8mx+hu6q/UmHnYUrxxjYmFH8fCA1xFHmUJZnZmierIlecbUjsm5kqtc0ZXrGdVV6nYi4sixLYVsW4Wyq958Wb2FsrzKsbHketq0K6e1HRntSOcVCzuqi4aUioc0Lunv40TEkTGSWzmGVbedMVLRdWvf12LZU6HsqlDuf1+tbyHbVn0spGQ0JM8Y5Yp+WbOVMhddTy3JiFpTMTUmwsqXPGWL/rG5Ie5/h/zvX1G7MiVZlmrbqbU+pkmNcaXiIa3c2qsXN3Vra09e45IRv/zRkF8Pyv55J+RYiji2WuujmtaSVFsqpqLraWdvQT05/3sddiyFHFsh21LYsVX2/PNQT66kQsmvUyXX0/Z0QVu6c+roK6itIaaZ4+s0sSGuzkxBW3vyyhVdtTXE1N4QUzTkVL5Ppdq8evMlTWtJ6qzpzTquLaXOvoJWbuvV9p585bwVk21Lb3Rk9UZnRoWSq9Rux8REJKR1nRm9siWtDV0ZTWyIa1ZrnY4Zl1A0ZCtk++fjsGPXGn+r54bdEwklz8j1PHmeaseJalk9Y2rLDNmWiq6nQsnTzr7+bdBaH9OM8Ukd05xQJGTLtiyVPaPefP+2c2xLtm3JsSw5dn9iZvfhnlHt+J4rukpGQ6qLhRR17ErCo3pd4b/qf9//bIxfx4tlT7ZtaVwyqpa6iBriYUVDjiIhW5liWV2ZoroyReVLfn0ueZ4ijn+ujYT6z1WWpdqxt/pcFw2pVDnGdOdK2rwrp427surqK2pqS1LHt9Vrxvi62rEqX/K0aVdWG7qy8ow0vt4vU8j265nnSdGwf2wrlj09va5LT67p0Laego5vr9fcSQ2aOi4pp5LUy5dcpXNl9eZLSuf950yh7B+Loo6SEf/aLRkJybak7lxJ3dmSyp6nZCSkZNRRMup/N5ORkF9nHEvRkKNxyYjsSp0xxmhHb0HbevK16wvHVu1zkZBdq1udfUVt7cmpM1NUMuofx2MhWz2VZWeKZf985PnXmd25otK5shIRRxNSUbWm/O/S6VOb9nk9driwjNn9UvvIkk6n1dDQoJ6eHqVSqaCLs1fX/+iP2pEu+Jlj9WeOZQZeIFUyyZX3h8Kh3Pmen7KulbWaBa9m+Fxj+jPBqmShbQ3K/lfLNDCDbgZcIFaSykNmvc2AbVcpSm35/uv+LHy1rAOHD/ycKtn33bP3wxEL20rFwpJUu5AdSjXh6e022r/oTmhq5US5pTuvLd25vQbUIduqHeT6CuVBgezhJBa29yi7bWlE2x4AxpJoyFY84qg7Wwq6KDgIIdsPGoMWdqy9XlPgyDCwJ+jhIOLYmtQUVyoe1rqdfUrny6O27IXHt+rfrzxz1JZ3IIYbh5IhHyP+9YOnBV0EDIMZFOwPDuSlanbXGTR9X6FcaSXsb312LKvWolidrhr8V7MgB6pY9mqtoOmcfxHYmPBbrGPhoTO71dZiz/MzKNliWWXXKBzys4Z+xtxvEc0X+7MA4ZClRNhv9cwWy+orlJUp7DsTb+S3mhbLfqZ1QiUr25SMqOR66s6WlK+08NdHQyp5fnZmfWdGZddoXF1EDZUs9raevHb2FRQPO5UMWUgNlXWNOLb6CmWlc2UVyu6AVnW/Iae6H/yWc6OOvqK2p/PqyZVUHwupMR5RQyKsxkrmxRijXZWW+GoWZW/cSotuplCuZXdDtqWQ42ehwo5Vqw/+cP/Ztiz1FfzW65Jr1JqKakJ9TImoo758Wb35skqeV7t1pa9QHpQdKJQ9FUr92SdjjNob45rYGFd9NFTZP2W5xs+2hmy7ViYjo63dea3vymh7uqDGeFjj6qKqizoqlD3lS67KnlGkUg8KJU/pfEm9+bJa6qKaUckIbunO6bXtvdrcnVNjIlLLQI1LRjSuLqpcydXqHX1avaNXvQNO3JOb4jquLaVZrXXyjFGm0rgUcvztVCz72antvXkVSp6ilVsnCiV3UKa2J1dSpuD6+zARUWM8rKaEvw9DtqWurJ/lqF5ETGqMKxZ2VPY8lcpGmaK/nav7oTc/+H3YsXXshHrNnlCnbMHVC5u69afNPcpWbjmyLUsN8bCak36GxRipXNlnLXV+NjEZdeRWsku9+bJ2ZYrqyg7IvLiextdHNakprrZUTPGwo3DIVr7kamNXThu6MsqXPDUmwmpKRGrr1xAPq+R66s2Xa/smnSspX3aVjPiZmqZEWJOa/DpRdo3W7uzT2o6M8iW3csuRJaeyHo7tr0tT0s9Mr9nRp1e39ao7W1QqHh6UhUvFqq/9LEpPrqTt6bx29hbUV/C3X9k1aq7Ug2jIrmzbkiIhWxMb4xqXjGjVtl49vrpDO3oLioZszZvcqOPa65UvuerK+BmbaeOSmjk+qVQ8XDvO9eT83izpfEmJSKhynAir7BrlSq4KZbf2XXM942/zTFFF11M0ZCsSslUf9de1IR5WruSqu1JXdmWL6sqUVCi5mtlap+PbU5qQiqo7W1JnX1El1++pEapkudd2ZPRGZ0bd2VLtuxgN2TpzWrPmTWlQ2fOzv52ZojbvymnTrpyMMZUsWLTSy8NRIhKSbVkDGqT980TZ9ZQreX4GuVT2s8hFV/mS/5wr+fu7KenXDce2aueoagbdsVTrNZEvuXKN30snGrJrmfOQY/snO0ua3BjX9Jak2hvjKpRcZYpl9WRL6swUtbO3oFzJlVNpIa+uW3e2KNvye0NVM43RkK1o2FbE8YeFQ/3H4ZLr+XUlX671MhrYEyBk2+rMFLQ97Wepq70CpP4sqW1Z/ncvEZYlKV9y1Vcoa0e6oM3dORXKniY1xnXyZD+D2J31y58plhUJOYo41Qyen+He2p3Xhq5sLRiPhGw1JcK1/VDt5VR2jezK9yUVCykRCdWOXeProprYGFNLXVRbunNaszOjbem8Wuoiam+IKxa2a1n0ouvVvlONled4JKSVW9N6bv0u9RXKsi1pWktSk5sS2pUpakdvXmXX6JhxCU0fl1Qi6tR6rFSz7b2FsiY1xnXixJSmjUtqS49fjs27srXeWuVKb62Sa2RZfiOEM+A84diWwpVMesixauehkGPXvv+OLfXk/N5JnmcUqXy3xiWjmtQYU0t9VFt78lq3M6PN3TmVPSPP85fXUDmmxMKOvEp21TX+eLdy65xXe1atB0djIqLmpP+5TME/7xZdr3Zr3sBG/Wqcu3susnqNU/Y8dWb83lx9+XLt+5sIO2qu83t9JKNO5Tzon5fyJbd2no9Uji+9hVLtnN2b98sj+ZnzVCys9oaYpjQn1JSIaF2Hf1zd2pOvlE21aY9pTtSOKx19xdq527b82/qqbURzJtRr/sxxmjouoZVb03pxU4929BZq15bRkF3L1ld7UCUr2fVs5fiRLZaVrfSeaaz0vAvZljKV4ZlCWZmif41XqvQqqj7WdWRq29KxLbXWR2vXN2XPP59Xr4Ul/5qzKRFRe0NM4+qiyhbLteNlqnLOrvaqsi1LsbCtxkREqVhYuWJZ29J5bU8XNG9y4z6vxQ4nZMgBAMCYYYypBCzRQH634VDJl1ztSBeUzpc0q7Vurw2iGB3G+I0zicjIclFl19OO3oKSldtZRvJ7JodS2fW0uTunCakYdekwky+5g24HGErJ9YP7XMlVxPFvxdpXXTPGqFC55WOkdfpQKbuetvbktXFXVumcf2vF9JbkIf+docPZcONQAnIAAAAAAA6h4cahh2/TMwAAAAAAhzECcgAAAAAAAkBADgAAAABAAAjIAQAAAAAIAAE5AAAAAAABICAHAAAAACAABOQAAAAAAASAgBwAAAAAgACEgi4AAADYU8+vfqXun98jy7ZkxeKy65IKt7UrPLFd8VNPVWzOnKCLCAAADhIBOQAgMPlXXlH33T+T8Vw5DY0KjWtW3TsuUGTypIOarzFG5W3bVNqyRU5Tk0ItLbLr62VZliTJy+fV9+hj6n3gAZU2blS5q0teOq34Kaeo4b2XqO7tb5cdjcoYIxkjy95/hzJjjEqbtyj71HJlnlyu/Gur5DQ2Ktw6QeFJExWdc5xiJxyvyNSpshxnr/PxMhlt+9KX1PPLX+19YZalluuXquUjH9nnvAAAwNhmGWNM0IV4M6XTaTU0NKinp0epVCro4gDAHkyppK7//E/1/Oq/VX/hO9V85ZVy6uqCLtabxpRKyvzhD+padocyjz++5wS2rbp3vF2N73+/Qi3jZYXDCo1rVqilZch55VasUObpP8jt6pKXyai8q0v5V1bK7egYNK0VichpGafQuBYV166Vl8nstYxWLCbLtuXl85IkZ1yzwuNbFZkxQ6k/u0h1554rOY5yL76ozONPKPfSi8q//Irczs79rr+VSCg2Z45ixx+vxBmnK3ne2+TUJeUVCup98EF1/Mu/qvjGG5Jta9y1H1Z05kx5uZy8dFqlrdtUWLtG2eVPSZKSCxao/Sv/oHBb236Xa4yR19Oj4qbNKm3erNC4ZsVPP73WSDFSxhjJ82RcVyqXZcXjBzyvN4vb26vSlq3y+noVO/542YnEqCzXy2ZV3LhRxfXrVd6xU6GWFoWnTFZ4wgSp0oBiR6NDlscYo8Jrryv73LMq79ght7NLbl+vnLo62amUnIZGOQ0NchpSclIp2Sn/teU48goFmXzeX/aaNSpu2Cjjlv39YtmSbUuW5KQaVPe2tylxxumyQnvmZtyeHvU+9LDS99+v7HPPKTZ7turefr7qzjtP0VmzZEUib/YmfFNU66xc13/tujKekTzXr8ueV6nTnmQ8f/vuo84YY+T19cndtUumWJQVjcqKRmUKBbm7dsnt7pZsR3YiLjsWkxWPy47HZdfVHdHH+KFUt5Upl/19YFmyo1H/uFFp8DSuK1nWsBpAR5MxRqZUkslmJceRFQ7LikTGXDkx9gw3DiUgB4BRUNy0SaVNm/wLDteVFYnKrquT29OtHf94qwqvvVab1mloUONlfy7Jktu9S1YkquQ585U4+y1y6pIHVQ4vm5XxvGFdDLrptHIvvSSvp0duOi1TKstOxGXFYrLjCdnxmOx43L/IjMVkRaP+xWl3t9yeHv+5u2dA4GtUeP11ZZ5c3j/MtpW66CJFpk2T29OjwuuvK/v000OWJ3byyap/50KF29pVWL1ahddeU/aZZ+T19Q29Ao6jcHu73J4eeb29e4wOTWxXw5/9mWInn6xQS4uscFi9Dzyonl/+UuXt2/e5beyGBlnyA5fdlxmfO1fJc85R/JR5cnt7Vd6+Q8UN65VfuVKFVa/JVIL8KiscVvyM01V4ZWVtfqEJEzTpn76hxBlnDLn87l/8Qtu+8MXavELt7YqdeIIikyYrNL5Fdl29ihs3qLB6tUobNvr7I52WyuVB84nOnqWmJX+puvPeKqe5WXYsJq9QUHn7drldXZITkh2LystmlXvxJeVeeEHFN95QuatTbmeXTKHQv02SSUVmzVR0+gw545rl1NXJisbk9fX69SCX8y9ioxFZliUvm5WXzckKhWQ3pPwgM9Ugp7FBTiolp6lJTnOzrEhEpfXrVVizVuXOjlpgE2psVPiYqYpMmyrLtlXu6FB5+3ZlV6xQ9tlnlV/xwuD9Ew4rMW+eYieeIFMqy8vnZfI5ebm8vHxOVjgspz4lu77OL6cTkuXYUuW52qBRrgRabrdfx+V5suvqZCeTfoPQjh1D1rehOA0NCk2aqFBjoxQKSZblN+zs1pj0ZnGamhSfN0+yLMkYlXd1qbRp874blkIhRaZOVXjyJFmhsCzblp1I1Bq7rHDYD14GPopFmUJBXrEglcpSOCQrHJY8Iy+T8QM148mOVQPXmOxY3A9u83m5fb3yslnJSLIkeUYmn/f34cA6bQ186b8xxvj7q7PT318jZNfVKTR+vOxEwm+IcGx5PWmVu3fJ3dW9x3dq2PNtaFBk8mSFJ7bLrqv361A0UumVI3nZjF/HerqlsisZIyN/XGXF/IckOx5XdPYsRY89VqHWCZJt+d+xfF5uOi2vt8//HqZ7/efePnnptLxczg+Opf55V8OCAfOvLdufUCqX5fb2+ueFQqH/+9rY6D+aGqWyq3Jnp3+s6OhUuatr79sqHPbHVZbnn2PishMJ/5wTT/iv43HZ8ZgkS8Z4flk8zy+f8SSvUmbP88s78L3xKnWxUh+rj3LZ31624/c2chz/+14q+w2h2axf91x3z3IPDM4HPYdlhQe+DvvTl10Z15Vxy7XXe7Wv8OxAx2nv4/YZDu5zlvsJIw+4rPK3r11pTHQcv2HRcSr121biLWer9cYb9z2PgBGQVxCQAwfPuJUT686dCjU2KjRx4pjIhHnFoorr1smpq1OotbX/pFcdt3q18qteU3nnTplySSq78jIZub298np7FZ4yRYkzzlD81FMkY/zALZ2Wm07L7UmrvGOHiuvWqrBunVQqKzxlisKTJ8nkciq88YZKmzcrNuc4pd7zbtUtWKByV5dyL7yg0oYNfgYklVJpyxal7/+18i+/vM91cRob1bRkidL336/i2rVDTxQOKzJ5sp/RCof89SnkZUolhdsnKjprlsKTJ8nt2qXStq3ysllFJk1WZNpUedms+h5/XLkVL0jlsuz6eoUnTpTT2Cg7mZRdl1RsznGKn3qqnMYG7frhj9T985/L5HKHcI8NWN+mJtUvXqRxV1+tyJQpg8YVVq9W1w9+oMzy5TKFokyp5AeHezldOU1NSi5YoPCUyXKSSdl19YrOOdbPiMZikvwu6uWOTrmdHSp3dCjU0qLY3LlDZjiM66q4fr2sUEhWLCYZqdyxU+XtO5R9+mml77tP5Z07JUl2KqXkgnP8enTiiYoed1xtmUMx5bKKb7yh/MqVyv/pZfU98oiK69fXxofa2tT4vveq6YorFGpq2uc2zL/2mrb+/WeVf/HFfU63x/Ya36LwxIkqvL7az/gMYMViezQYHO6cxkZZkYjKO3aM6nLthgY/cJ3QqvLODhU3bRp2oG3FYkqcfroi06bJaW6SU58aEKANfHTLS/uBkYypNHhEFW5vV3TGDEWmT5MVjVWCFq+SDfZU3LBBfQ89tGeD0gDR2bNUf5HfGyS/8lX1PfKIsn/4wz57lhwxbNtvpNhXwDSAlUjIDofllUoy+bysaFROU6OchkbJ8+TlczLZnB/g5XLDni+AvatftEiTv/XNoIuxTwTkFYdLQJ556mmZYkGybL9F3vYflm3LlF2Zkn9RKlmyQtUWvGrrvdPfkmmMf8I1RtKAYWZgy+fAaSstjHtMu9v0ntc/D8vPhDiplOxk0v9M2ZW8Smuf58nL5StZsm6ZQnFAq2fUv/iPxeR296jc2SG3u8dfv3JZlmVXsiJNfnYlHPazBq7rt4QXS7LjcTmNDbITCbnd3Srv3Fnrxllcu04mn1d0zhzFTjhB4UmT/IvqkKNyZ5dKmzaqtHWbQi3jFJkxU+H2dpU2b1Jh9RqVuzoVOWaqojNnKNTW5l+oG6NyR4cKa9eq+MZ6mVLRb/WMhBWe0KbI1GMUnjhRckKS8fyL/p07Vd6xQ6ZQlNPcpFBzs+xknaxIWLJtFdeuU+6lF1VcvVpOS4uiM2YqcowfkJhSScYYP0uQiMtN96rw+usqrF4tWVJ44kSFJ06U5YRkCgWZUrFSg6xKnfFbxc2A7IGM8bd7JOp3w41GZEejftfGbFZevtK63dwsOx5XubOjsg7+epR37FC5o6PWii75QUj02Nm1/b/XVvWQU8s4OfX1sutTsuuS8tK9Km3dqvK2bfIKBalclvG8Stkq2ZHKszxTy7hWu1rayaSKa9cq//LLle+FJMuS09joF8F1/RbtA8xcHAgrEpEpFvc+geMoMm2a32Ju2/KKBXm9fTKFguoWXqDWT3xCoaYmGddV+t57lXnqadn1dQo1Nam8s0N9Tzyu0voNo7Y+VeHJkxVub5fdkJIVCsvkcvLyeXm5nP+68t5ks/KKRTnJpB/kNzb4mZIGf39VG3BC48cree65ip144oi6+5V37lTvQw+r96GH5GWzis6cqeisWYqfMs+f1yjeR21cV7kVKyTLVvzkuUN2+R32vIxRcfVqZZY/pci0qUouWDDidXH7+pR/5RUVXn1Vpe3b5XZ0yO1JKzxpkqKzZioyfbqcpma/i3NjY63BwO3tVc/Pf65dd92l4voNUvW7JD8gDI0bJ+N5fhbcthU78QTF581T7LjjFGppkdM8TnYy4a+/Zau8basKq9eo+MY6uT1peX198vJ5OfV1shsaZMcT/rG+ck7wM19xP9vWk65l8d2eSg+LXd1yu/wsfHjSJEVmzVR4Qpufac3lVO7oUHH9+lo2t1rm6PHHKXHGGUqcfoaiM6bLTib9+/s3blTmySdV3LBRdizq/1BeLOaXIxqTKZXk9ablpnv944rn+udfz/UzlJZV24YDH7IdP8ub6ZOdSCg0frxCra1y6uv33N8DjqNeJqPSlq0qbdksr7dXplSWccuKHDNV8dNOlf0mdws3pZKyzz6r4oaNfmbZsuTUsrYTa8fTQZ+p/DZDYfVqlbdvl3E9GbfsN3R2dKrc2el3kR+YLaw87GhUViQqKxyqXNf49c2uS/o9dizbb2Cs9Fgw+by8XN7viZP0eyCo2hBsyc+kx2KyQmG//GZQQQeV22lskNPcrFBTk39dYduSXcm+Va61ZA/IwlXW1e/xsNNv1C3k/fOV6/q9OQZkhPfVCDcUL5NRcfNmlTZuVGn7dnl9lV4Cle+aLKtyrdPo344QDvevuyz/dWWfWZYlt7tb+ddeU+G11+Xu2lU7D1vxeP+tDvV1susHP1uJhL/ug+ZdfWlVbnWwau+rIy3H9ufR2CArEun/vnZ393fVd2yFxrUo1DJOTvM4/7m6/Ss9Mmq9HIpF/zjiOJVrSD8zbaoZ6lxOXjYnL5eVyeX7y2NbA7q4D/G+Mo1lVfZzrV76PXXsSEQKhQddv1afZdv+9WoiITuRrN12IGPkFUv+saw4oAfIUM+1aYoadP0eCvnrazuDenUMZb/Jj4Maf7Dz3t/HD2L+tXhm8G0k1dtK5HkKjR/v9/AZwwjIKw6XgHz1wneqtGlT0MUA9s625YxrltvdM+jCPWh2fb1MPt8fmA8c19Cg2Jw5fiNGOCwrHPIvUBoaZcfjfpfnZ59Vcd06f/pkstJ1tlFOKqXQuGZFpk1XZPp0WeGwSps2qrhpk+xYXJFp0xRqbVVm+XKl77vPDwpsW9Fjj1V09mx5uay8nrSsaFT1Cy9Q/YUXKtTcfFDrWty4UeVt22TKZb8By3Fq9zoXN27yuydv3aJQ8ziF2ibIjif8Mr+xXrJtJee/RckFC+Q0Nau8dYtKW7f6XRgzGbm7din3p5eU++Pzcru6lDzvrRp31VVKzJ8/JnpD4M1ljJHX2yu3u9tvbG1oGBP7vXqf774aPdy+PkmW3zgwBsoMAIBEQF5zuATkGz+61G9trrX8VH5oxHWlkFO7F0XVHyFx3UGt91atJdCutZxa1dZMa/eHZFn2nsOGO22l1djtTcvry/gtkY4z6NmKxWrZMSsWra2nyRf81s58Tk6qQaFxzXIam2ot6cYt17IiXiYzOPCIRmVFIvJyWb9rcSYjp7FRoZbxCk9oVWT6dEWmz5AdjSj/6qvKv/yKyl1dflflUtn/teMpUxRum6Dyzp0qrFmr0tat/q8fz5rl/9DT+vUqrF0jt6Ny/5wxchobFJkxU5EZ0/szPPm8Spu3qLhhg0rbtvnTWv69oOHWVoXGt/qtxt3dKu/qkslk5VV6OUQmTVJs7smKzTlW5Y5OFdasUWnLFn8dK12uvXxeXjYjOxZXdPZsRWfPlmxLpS1bVN661c+iR2MDWpqrrYaV7LRlyY7H/K6KluVnlAp5v/tvPi+vWJAd8XsrWNGo3HSP3M4uedmsQi0ttQxP/2O8QuPGyXIcmWJRhbVr/e6uA7LT/XVOtbpiiqX++9V605XnXr+7dHt7LWi0Qo5k2TLFgt9ansv75c0Xalkbp7Gh0q08La+vV+GJExU/5RSFjznGH75rl8qdnZV7jEKykwm/G/swLtC9bNavgweY6TTlsgpr1yoyaZKfxTmMGWNkCoURZ3wAAADQj4C84nAJyAEAAAAAR4bhxqH8Xj8AAAAAAAEgIAcAAAAAIAAE5AAAAAAABICAHAAAAACAABCQAwAAAAAQAAJyAAAAAAACQEAOAAAAAEAACMgBAAAAAAgAATkAAAAAAAEgIAcAAAAAIAAE5AAAAAAABICAHAAAAACAABCQAwAAAAAQAAJyAAAAAAACQEAOAAAAAEAACMgBAAAAAAhAKOgCoGLna5JX2m2gted01hDDApvuTVrmoPHW3ocNe/gAxuw+YGTjA5vHEA7bdXkT5jHk5hrpPIaaySjMw3hSMSMV0v5zOC5F6qRwQrJtybIly6k8Vx5SpY5bA56lwd8Xa8/nfY2rfWcqz15JcouSW30uSl55iPXb+6oOPY3bPz+3PHje1edQXIokpFBst21mKq/NgOH7GGa8Aa8r46vb1Hj9y/dK/np6ZcmJ+MsNxfxtMnC7WJZkh/zyhWP+sNq6FPu3V7lQmV9pwH6zBu/D3ffpwPGSXx9yu6RC74Dl2/37KxyT4k1SvNmvM7bjl82qPNu25Hn+9vbcAc9e/3tZUijir7MT7X8t+dPt8TB7GV55DNxOu5d3b/Vsn8d7a49Rw/qc5/r7svpwS375qtvXdirbyRkw44H1RyMYNmDc7sP2O6+9fO6QzX8fZZUq9aT6qNSb2vTVfT3w+1T5/MC6au9ej50RlukA1vuQDdO+h+1uONdBw5p+L8PH2vTAWFffLk0+PehSHBIE5GPFD98vda8PuhQAAAAAMLYd//9JH/ivoEtxSBCQjxWJZqmUHTxsONm2vU43kmn38vnhZB1HPM99TDvi1v79DKsO36NVeLf3b/b40VjGkA3cR+J6DqMXxh6TBFGGEc4jkpSiKf+5nJcKff7xoJp5rGY2jbtnVthowHvtNs4Mc5ypja69dsKVzGnYz546ET/rerDZFMuWQlHJDu+2jMpry/G3QSkrlXL7yO5rP6+rWefK6+o8jOtnji1r8LLtsJ8hdIv+8sv5/qzvwOOLW+ofL+22jarrE/GzzXZoiKyyGbAv95aFNlIs5WfAo/V+2WvZ/so0pZyfQc91+WXx3P7McDUbvns22HIqPQQq743pz+xXs/puYcD220sGf8iHNWBbDaxfu2VZ+yvaQWSeB1aoIT5XzfZW96ldXV9vt14DlW0+ZEZ+JJn8kQzbff5vxjKGMcyYynao9iTwBvTSG6qXw+6f3b1eD+h9cTDlGzT8UA/TyKerGsl11j6nf7Pnf4imBw4HLbODLsEhQ0A+Vlz7SNAlAAAAAACMIn7UDQAAAACAABCQAwAAAAAQALqsjxE//LsvK9fXM+CHkvvvZbJ2v6+p8t7a4xdorT0+3/9Ra7fh+7gPtva02/vKvYKWZct4ZXluWZ7nVucoWVb/fC3L/3zlPjTL6p9mcJH3fz/qoPUc1mcGj9/zdt4DWOaQE+1lmn3M/yDvvj1wI/112H3Z271oB7CIN3t7VEs6att9NHfwgP0wqncBDrGwEVevkUy/t5Xb49i252zNSO/j1P7Xpf/2WdM/vWVpwKFv0LRDz2T3hfoDrQHrUp3OVH4nwAz1uT0Kv5/xB/KxQA5a1j7eDTVgCNVtNuC+8+HWBmuvbw6xfdwSP2LDLOdeJzuo9dzzw/uandnHu6EG7zHFwRzwBh4rhiykNejQciQxxlR+0sGo/7pMu10eWsM7nu9nH+w5eqzfq24N8WpvA0Zgj+/4/g/iR1z9G+nPKYxg2tbpM3X+X146sgWMUQTkY8SON1bJK3cHXQwAAAAAGNN6tncRkOPQOu7c9yufyfanVAZkRipvVWviN9V3Q7T8Dxw3IKUyKFNk9ngx6LODxwz8nCdjPBljZNmOLDsky3YGfNb0L6f6vvK53efvTzvUlthfs+u+x+8/I7Zb2/xIMmgjmNbsdT32kQ2w9j56/8sbRebAGm5HvX28lh4faWnfvJ2w70mGUc4h/zVgiE+PxZZ1s/dDzyAHlD3f+zdu0AwPUSW0bav2d+H9PyBuKtlzM2RPp92T33tkwgce6yv72Rr4uUPZy2U0HejX6UDOD3sYnAUc8DT8OR9sr4Rh1vU9s3IH8ovfI1v8wU28l1nsc0MO7Pm3j5kM9aPoewzYR1a++mLIa6MBxdm9hIekq8LYZVmWLNvasxePqV6rVYcdqnXfT2/FMeKA9vvul88aov4Nsnuv1X2VZc+Zj/zId6gnPJSGt9Dh1JfW6VMPsixjBwH5GHHRdZcEXQQAAAAAwCjiR90AAAAAAAgAATkAAAAAAAEgIAcAAAAAIAAE5AAAAAAABICAHAAAAACAABCQAwAAAAAQAAJyAAAAAAACQEAOAAAAAEAACMgBAAAAAAgAATkAAAAAAAEgIAcAAAAAIAAE5AAAAAAABICAHAAAAACAABCQAwAAAAAQAAJyAAAAAAACQEAOAAAAAEAACMgBAAAAAAgAATkAAAAAAAEIBV2AN5sxRpKUTqcDLgkAAAAA4GhQjT+r8ejeHPEBeW9vryRpypQpAZcEAAAAAHA06e3tVUNDw17HW2Z/IfthzvM8bdmyRfX19bIsK+ji7FU6ndaUKVO0ceNGpVKpoIuDwwT1BiNFncFIUWdwIKg3GCnqDEZqrNcZY4x6e3s1ceJE2fbe7xQ/4jPktm1r8uTJQRdj2FKp1JisUBjbqDcYKeoMRoo6gwNBvcFIUWcwUmO5zuwrM17Fj7oBAAAAABAAAnIAAAAAAAJAQD5GRKNRff7zn1c0Gg26KDiMUG8wUtQZjBR1BgeCeoORos5gpI6UOnPE/6gbAAAAAABjERlyAAAAAAACQEAOAAAAAEAACMgBAAAAAAgAATkAAAAAAAEgIB8jvvOd72jatGmKxWI6++yz9Yc//CHoImGM+MIXviDLsgY9jjvuuNr4fD6vpUuXaty4caqrq9Oll16q7du3B1hijLbHHntM73nPezRx4kRZlqVf/OIXg8YbY/S5z31O7e3tisfjWrhwoV5//fVB03R1dWnJkiVKpVJqbGzUNddco76+vlFcC4y2/dWbq666ao9jz+LFiwdNQ705utxyyy0688wzVV9fr9bWVl1yySVatWrVoGmGc07asGGD3vWudymRSKi1tVWf+tSnVC6XR3NVMEqGU2fOP//8PY41H/nIRwZNQ505etx+++06+eSTlUqllEqlNH/+fN1///218UfiMYaAfAz4yU9+or/5m7/R5z//ef3xj3/UvHnztGjRIu3YsSPoomGMOPHEE7V169ba4/HHH6+N++u//mv993//t+666y49+uij2rJli973vvcFWFqMtkwmo3nz5uk73/nOkONvvfVWffvb39b/+T//R08//bSSyaQWLVqkfD5fm2bJkiV6+eWX9cADD+jee+/VY489pmuvvXa0VgEB2F+9kaTFixcPOvb8+Mc/HjSeenN0efTRR7V06VI99dRTeuCBB1QqlXThhRcqk8nUptnfOcl1Xb3rXe9SsVjUk08+qe9///u644479LnPfS6IVcKbbDh1RpI+/OEPDzrW3HrrrbVx1Jmjy+TJk/W1r31Nzz33nJ599lm94x3v0MUXX6yXX35Z0hF6jDEI3FlnnWWWLl1ae++6rpk4caK55ZZbAiwVxorPf/7zZt68eUOO6+7uNuFw2Nx11121YStXrjSSzPLly0ephBhLJJl77rmn9t7zPNPW1ma+/vWv14Z1d3ebaDRqfvzjHxtjjHnllVeMJPPMM8/Uprn//vuNZVlm8+bNo1Z2BGf3emOMMVdeeaW5+OKL9/oZ6g127NhhJJlHH33UGDO8c9J9991nbNs227Ztq01z++23m1QqZQqFwuiuAEbd7nXGGGPe9ra3mY9//ON7/Qx1Bk1NTebf//3fj9hjDBnygBWLRT333HNauHBhbZht21q4cKGWL18eYMkwlrz++uuaOHGiZsyYoSVLlmjDhg2SpOeee06lUmlQ/TnuuON0zDHHUH8gSVq3bp22bds2qI40NDTo7LPPrtWR5cuXq7GxUWeccUZtmoULF8q2bT399NOjXmaMHY888ohaW1s1Z84cXXfdders7KyNo96gp6dHktTc3CxpeOek5cuXa+7cuZowYUJtmkWLFimdTtcyYDhy7V5nqn74wx+qpaVFJ510km6++WZls9naOOrM0ct1Xd15553KZDKaP3/+EXuMCQVdgKNdR0eHXNcdVGkkacKECXr11VcDKhXGkrPPPlt33HGH5syZo61bt+qLX/yi3vrWt+pPf/qTtm3bpkgkosbGxkGfmTBhgrZt2xZMgTGmVOvBUMeY6rht27aptbV10PhQKKTm5mbq0VFs8eLFet/73qfp06drzZo1+sxnPqOLLrpIy5cvl+M41JujnOd5uvHGG7VgwQKddNJJkjSsc9K2bduGPB5Vx+HINVSdkaQPfvCDmjp1qiZOnKgXX3xRN910k1atWqWf//znkqgzR6OXXnpJ8+fPVz6fV11dne655x6dcMIJWrFixRF5jCEgB8a4iy66qPb65JNP1tlnn62pU6fqpz/9qeLxeIAlA3Ak+4u/+Iva67lz5+rkk0/WzJkz9cgjj+iCCy4IsGQYC5YuXao//elPg37TBNiXvdWZgb87MXfuXLW3t+uCCy7QmjVrNHPmzNEuJsaAOXPmaMWKFerp6dHdd9+tK6+8Uo8++mjQxXrT0GU9YC0tLXIcZ49fB9y+fbva2toCKhXGssbGRh177LFavXq12traVCwW1d3dPWga6g+qqvVgX8eYtra2PX5Eslwuq6uri3qEmhkzZqilpUWrV6+WRL05ml1//fW699579bvf/U6TJ0+uDR/OOamtrW3I41F1HI5Me6szQzn77LMladCxhjpzdIlEIpo1a5ZOP/103XLLLZo3b56+9a1vHbHHGALygEUiEZ1++ul66KGHasM8z9NDDz2k+fPnB1gyjFV9fX1as2aN2tvbdfrppyscDg+qP6tWrdKGDRuoP5AkTZ8+XW1tbYPqSDqd1tNPP12rI/Pnz1d3d7eee+652jQPP/ywPM+rXRgBmzZtUmdnp9rb2yVRb45Gxhhdf/31uueee/Twww9r+vTpg8YP55w0f/58vfTSS4Macx544AGlUimdcMIJo7MiGDX7qzNDWbFihSQNOtZQZ45unuepUCgcuceYoH9VDsbceeedJhqNmjvuuMO88sor5tprrzWNjY2Dfh0QR69PfOIT5pFHHjHr1q0zTzzxhFm4cKFpaWkxO3bsMMYY85GPfMQcc8wx5uGHHzbPPvusmT9/vpk/f37ApcZo6u3tNc8//7x5/vnnjSRz2223meeff96sX7/eGGPM1772NdPY2Gh++ctfmhdffNFcfPHFZvr06SaXy9XmsXjxYnPqqaeap59+2jz++ONm9uzZ5vLLLw9qlTAK9lVvent7zSc/+UmzfPlys27dOvPggw+a0047zcyePdvk8/naPKg3R5frrrvONDQ0mEceecRs3bq19shms7Vp9ndOKpfL5qSTTjIXXnihWbFihfn1r39txo8fb26++eYgVglvsv3VmdWrV5svfelL5tlnnzXr1q0zv/zlL82MGTPMeeedV5sHdebo8ulPf9o8+uijZt26debFF180n/70p41lWea3v/2tMebIPMYQkI8R//Iv/2KOOeYYE4lEzFlnnWWeeuqpoIuEMeIDH/iAaW9vN5FIxEyaNMl84AMfMKtXr66Nz+Vy5qMf/ahpamoyiUTCvPe97zVbt24NsMQYbb/73e+MpD0eV155pTHG/+uzz372s2bChAkmGo2aCy64wKxatWrQPDo7O83ll19u6urqTCqVMh/60IdMb29vAGuD0bKvepPNZs2FF15oxo8fb8LhsJk6dar58Ic/vEdDMfXm6DJUfZFkli1bVptmOOekN954w1x00UUmHo+blpYW84lPfMKUSqVRXhuMhv3VmQ0bNpjzzjvPNDc3m2g0ambNmmU+9alPmZ6enkHzoc4cPa6++mozdepUE4lEzPjx480FF1xQC8aNOTKPMZYxxoxePh4AAAAAAEjcQw4AAAAAQCAIyAEAAAAACAABOQAAAAAAASAgBwAAAAAgAATkAAAAAAAEgIAcAAAAAIAAEJADAAAAABAAAnIAAAAAAAJAQA4AAA4py7L0i1/8IuhiAAAw5hGQAwBwBLnqqqtkWdYej8WLFwddNAAAsJtQ0AUAAACH1uLFi7Vs2bJBw6LRaEClAQAAe0OGHACAI0w0GlVbW9ugR1NTkyS/O/ntt9+uiy66SPF4XDNmzNDdd9896PMvvfSS3vGOdygej2vcuHG69tpr1dfXN2ia//iP/9CJJ56oaDSq9vZ2XX/99YPGd3R06L3vfa8SiYRmz56tX/3qV2/uSgMAcBgiIAcA4Cjz2c9+VpdeeqleeOEFLVmyRH/xF3+hlStXSpIymYwWLVqkpqYmPfPMM7rrrrv04IMPDgq4b7/9di1dulTXXnutXnrpJf3qV7/SrFmzBi3ji1/8oi677DK9+OKL+rM/+zMtWbJEXV1do7qeAACMdZYxxgRdCAAAcGhcddVV+sEPfqBYLDZo+Gc+8xl95jOfkWVZ+shHPqLbb7+9Nu4tb3mLTjvtNP3bv/2bvvvd7+qmm27Sxo0blUwmJUn33Xef3vOe92jLli2aMGGCJk2apA996EP6h3/4hyHLYFmW/v7v/15f/vKXJflBfl1dne6//37uZQcAYADuIQcA4Ajz9re/fVDALUnNzc211/Pnzx80bv78+VqxYoUkaeXKlZo3b14tGJekBQsWyPM8rVq1SpZlacuWLbrgggv2WYaTTz659jqZTCqVSmnHjh0HukoAAByRCMgBADjCJJPJPbqQHyrxeHxY04XD4UHvLcuS53lvRpEAADhscQ85AABHmaeeemqP98cff7wk6fjjj9cLL7ygTCZTG//EE0/Itm3NmTNH9fX1mjZtmh566KFRLTMAAEciMuQAABxhCoWCtm3bNmhYKBRSS0uLJOmuu+7SGWecoXPPPVc//OEP9Yc//EHf+973JElLlizR5z//eV155ZX6whe+oJ07d+qGG27QFVdcoQkTJkiSvvCFL+gjH/mIWltbddFFF6m3t1dPPPGEbrjhhtFdUQAADnME5AAAHGF+/etfq729fdCwOXPm6NVXX5Xk/wL6nXfeqY9+9KNqb2/Xj3/8Y51wwgmSpEQiod/85jf6+Mc/rjPPPFOJREKXXnqpbrvtttq8rrzySuXzef3zP/+zPvnJT6qlpUXvf//7R28FAQA4QvAr6wAAHEUsy9I999yjSy65JOiiAABw1OMecgAAAAAAAkBADgAAAABAALiHHACAowh3qgEAMHaQIQcAAAAAIAAE5AAAAAAABICAHAAAAACAABCQAwAAAAAQAAJyAAAAAAACQEAOAAAAAEAACMgBAAAAAAgAATkAAAAAAAH4/wEiFSeMqXp/LQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1) 加载并清洗数据\n",
        "data = np.load(\"SP500_preprocessed.npz\")\n",
        "X, y_reg, y_clf = data['X'], data['y_reg'], data['y_clf']\n",
        "mask = (~np.isnan(y_reg)) & (~np.isinf(y_reg))\n",
        "X, y_reg, y_clf = X[mask], y_reg[mask], y_clf[mask]\n",
        "y_reg = np.clip(y_reg, -10, 10)\n",
        "\n",
        "# 2) 切分数据：70% 训练 / 10% 验证 / 20% 测试（不打乱）\n",
        "X_temp, X_test, y_reg_temp, y_reg_test, y_clf_temp, y_clf_test = train_test_split(\n",
        "    X, y_reg, y_clf, test_size=0.2, shuffle=False)\n",
        "X_train, X_val, y_reg_train, y_reg_val, y_clf_train, y_clf_val = train_test_split(\n",
        "    X_temp, y_reg_temp, y_clf_temp, test_size=0.125, shuffle=False)\n",
        "\n",
        "print(\"Shapes → Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)\n",
        "\n",
        "# 3) 定义模型构造函数\n",
        "def build_model(input_dim):\n",
        "    inp = Input(shape=(50, input_dim))\n",
        "    # 共享层\n",
        "    x = LSTM(15)(inp)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(30, activation=\"relu\")(x)\n",
        "    # 回归分支\n",
        "    r = Dense(30, activation=\"relu\")(x)\n",
        "    r = Dense(30, activation=\"relu\")(r)\n",
        "    r = Dense(30, activation=\"relu\")(r)\n",
        "    out_reg = Dense(1, name=\"regression\")(r)\n",
        "    # 分类分支\n",
        "    c = Dense(30, activation=\"relu\")(x)\n",
        "    c = Dense(30, activation=\"relu\")(c)\n",
        "    c = Dense(30, activation=\"relu\")(c)\n",
        "    out_clf = Dense(2, activation=\"softmax\", name=\"classification\")(c)\n",
        "    model = Model(inp, [out_reg, out_clf])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "        loss={\"regression\":\"mse\", \"classification\":\"sparse_categorical_crossentropy\"},\n",
        "        loss_weights={\"regression\":0.1, \"classification\":1.0}\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# 4) 学习率调度（每 50 轮减半）\n",
        "def lr_schedule(epoch, lr):\n",
        "    return 1e-3 * (0.5 ** (epoch // 50))\n",
        "lr_cb = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# 5) 取固定的 13 维特征子集（示例取前 13 个）\n",
        "feature_subset = 13\n",
        "feature_idx = list(range(feature_subset))\n",
        "X_train_sub = X_train[:, :, feature_idx]\n",
        "X_val_sub   = X_val[:, :, feature_idx]\n",
        "\n",
        "# 6) 训练 300 轮，不使用 EarlyStopping\n",
        "model = build_model(input_dim=feature_subset)\n",
        "history = model.fit(\n",
        "    X_train_sub,\n",
        "    {\"regression\": y_reg_train, \"classification\": y_clf_train},\n",
        "    validation_data=(X_val_sub, {\"regression\": y_reg_val, \"classification\": y_clf_val}),\n",
        "    epochs=300,\n",
        "    batch_size=256,\n",
        "    callbacks=[lr_cb],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 7) 绘制损失曲线\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Train total loss')\n",
        "plt.plot(history.history['val_loss'], label='Val total loss')\n",
        "plt.plot(history.history['regression_loss'], label='Train MSE loss')\n",
        "plt.plot(history.history['val_regression_loss'], label='Val MSE loss')\n",
        "plt.plot(history.history['classification_loss'], label='Train CE loss')\n",
        "plt.plot(history.history['val_classification_loss'], label='Val CE loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Multi-Task LFM Training Curves (no EarlyStopping)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KTYy5LJJ8W1-",
        "outputId": "f530c1ed-f4c0-49b6-a8ee-6d0ab3160206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes → Train: (2751, 50, 48) Val: (393, 50, 48) Test: (786, 50, 48)\n",
            "Epoch 1/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 82ms/step - classification_loss: 0.6897 - loss: 1.6857 - regression_loss: 9.9634 - val_classification_loss: 0.6909 - val_loss: 0.8119 - val_regression_loss: 1.2569 - learning_rate: 0.0010\n",
            "Epoch 2/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6894 - loss: 1.7077 - regression_loss: 10.1784 - val_classification_loss: 0.6908 - val_loss: 0.8120 - val_regression_loss: 1.2575 - learning_rate: 0.0010\n",
            "Epoch 3/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6866 - loss: 1.6942 - regression_loss: 10.0720 - val_classification_loss: 0.6911 - val_loss: 0.8120 - val_regression_loss: 1.2565 - learning_rate: 0.0010\n",
            "Epoch 4/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6899 - loss: 1.6918 - regression_loss: 10.0171 - val_classification_loss: 0.6905 - val_loss: 0.8120 - val_regression_loss: 1.2585 - learning_rate: 0.0010\n",
            "Epoch 5/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6879 - loss: 1.6759 - regression_loss: 9.8793 - val_classification_loss: 0.6906 - val_loss: 0.8121 - val_regression_loss: 1.2587 - learning_rate: 0.0010\n",
            "Epoch 6/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6884 - loss: 1.6741 - regression_loss: 9.8609 - val_classification_loss: 0.6908 - val_loss: 0.8120 - val_regression_loss: 1.2576 - learning_rate: 0.0010\n",
            "Epoch 7/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6878 - loss: 1.6795 - regression_loss: 9.9140 - val_classification_loss: 0.6908 - val_loss: 0.8121 - val_regression_loss: 1.2580 - learning_rate: 0.0010\n",
            "Epoch 8/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6912 - loss: 1.6694 - regression_loss: 9.7844 - val_classification_loss: 0.6906 - val_loss: 0.8125 - val_regression_loss: 1.2614 - learning_rate: 0.0010\n",
            "Epoch 9/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6876 - loss: 1.6621 - regression_loss: 9.7484 - val_classification_loss: 0.6916 - val_loss: 0.8126 - val_regression_loss: 1.2585 - learning_rate: 0.0010\n",
            "Epoch 10/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6896 - loss: 1.6747 - regression_loss: 9.8532 - val_classification_loss: 0.6912 - val_loss: 0.8126 - val_regression_loss: 1.2608 - learning_rate: 0.0010\n",
            "Epoch 11/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6907 - loss: 1.6469 - regression_loss: 9.5677 - val_classification_loss: 0.6910 - val_loss: 0.8125 - val_regression_loss: 1.2606 - learning_rate: 0.0010\n",
            "Epoch 12/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6878 - loss: 1.6431 - regression_loss: 9.5580 - val_classification_loss: 0.6916 - val_loss: 0.8125 - val_regression_loss: 1.2579 - learning_rate: 0.0010\n",
            "Epoch 13/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - classification_loss: 0.6867 - loss: 1.6930 - regression_loss: 10.0680 - val_classification_loss: 0.6913 - val_loss: 0.8122 - val_regression_loss: 1.2568 - learning_rate: 0.0010\n",
            "Epoch 14/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - classification_loss: 0.6894 - loss: 1.7129 - regression_loss: 10.2290 - val_classification_loss: 0.6908 - val_loss: 0.8123 - val_regression_loss: 1.2591 - learning_rate: 0.0010\n",
            "Epoch 15/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - classification_loss: 0.6903 - loss: 1.6437 - regression_loss: 9.5299 - val_classification_loss: 0.6912 - val_loss: 0.8125 - val_regression_loss: 1.2597 - learning_rate: 0.0010\n",
            "Epoch 16/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6890 - loss: 1.6890 - regression_loss: 9.9920 - val_classification_loss: 0.6913 - val_loss: 0.8126 - val_regression_loss: 1.2598 - learning_rate: 0.0010\n",
            "Epoch 17/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6843 - loss: 1.6987 - regression_loss: 10.1419 - val_classification_loss: 0.6920 - val_loss: 0.8127 - val_regression_loss: 1.2567 - learning_rate: 0.0010\n",
            "Epoch 18/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6884 - loss: 1.6172 - regression_loss: 9.2836 - val_classification_loss: 0.6909 - val_loss: 0.8123 - val_regression_loss: 1.2585 - learning_rate: 0.0010\n",
            "Epoch 19/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6885 - loss: 1.6876 - regression_loss: 9.9865 - val_classification_loss: 0.6912 - val_loss: 0.8125 - val_regression_loss: 1.2593 - learning_rate: 0.0010\n",
            "Epoch 20/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6875 - loss: 1.6436 - regression_loss: 9.5717 - val_classification_loss: 0.6916 - val_loss: 0.8126 - val_regression_loss: 1.2580 - learning_rate: 0.0010\n",
            "Epoch 21/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6883 - loss: 1.6771 - regression_loss: 9.8840 - val_classification_loss: 0.6913 - val_loss: 0.8125 - val_regression_loss: 1.2587 - learning_rate: 0.0010\n",
            "Epoch 22/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6896 - loss: 1.7288 - regression_loss: 10.3922 - val_classification_loss: 0.6912 - val_loss: 0.8126 - val_regression_loss: 1.2603 - learning_rate: 0.0010\n",
            "Epoch 23/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6902 - loss: 1.6567 - regression_loss: 9.6588 - val_classification_loss: 0.6915 - val_loss: 0.8125 - val_regression_loss: 1.2573 - learning_rate: 0.0010\n",
            "Epoch 24/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6873 - loss: 1.6494 - regression_loss: 9.6251 - val_classification_loss: 0.6916 - val_loss: 0.8124 - val_regression_loss: 1.2565 - learning_rate: 0.0010\n",
            "Epoch 25/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6861 - loss: 1.6875 - regression_loss: 10.0218 - val_classification_loss: 0.6909 - val_loss: 0.8120 - val_regression_loss: 1.2563 - learning_rate: 0.0010\n",
            "Epoch 26/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6883 - loss: 1.6735 - regression_loss: 9.8536 - val_classification_loss: 0.6909 - val_loss: 0.8122 - val_regression_loss: 1.2579 - learning_rate: 0.0010\n",
            "Epoch 27/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6870 - loss: 1.6246 - regression_loss: 9.3722 - val_classification_loss: 0.6912 - val_loss: 0.8125 - val_regression_loss: 1.2603 - learning_rate: 0.0010\n",
            "Epoch 28/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6865 - loss: 1.6504 - regression_loss: 9.6366 - val_classification_loss: 0.6913 - val_loss: 0.8129 - val_regression_loss: 1.2636 - learning_rate: 0.0010\n",
            "Epoch 29/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6872 - loss: 1.6240 - regression_loss: 9.3687 - val_classification_loss: 0.6910 - val_loss: 0.8126 - val_regression_loss: 1.2612 - learning_rate: 0.0010\n",
            "Epoch 30/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6891 - loss: 1.7150 - regression_loss: 10.2625 - val_classification_loss: 0.6911 - val_loss: 0.8124 - val_regression_loss: 1.2590 - learning_rate: 0.0010\n",
            "Epoch 31/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6872 - loss: 1.7129 - regression_loss: 10.2631 - val_classification_loss: 0.6915 - val_loss: 0.8125 - val_regression_loss: 1.2583 - learning_rate: 0.0010\n",
            "Epoch 32/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6888 - loss: 1.6394 - regression_loss: 9.5119 - val_classification_loss: 0.6916 - val_loss: 0.8127 - val_regression_loss: 1.2589 - learning_rate: 0.0010\n",
            "Epoch 33/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6878 - loss: 1.6544 - regression_loss: 9.6687 - val_classification_loss: 0.6912 - val_loss: 0.8127 - val_regression_loss: 1.2613 - learning_rate: 0.0010\n",
            "Epoch 34/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6918 - loss: 1.6541 - regression_loss: 9.6282 - val_classification_loss: 0.6910 - val_loss: 0.8126 - val_regression_loss: 1.2613 - learning_rate: 0.0010\n",
            "Epoch 35/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - classification_loss: 0.6875 - loss: 1.6348 - regression_loss: 9.4769 - val_classification_loss: 0.6914 - val_loss: 0.8124 - val_regression_loss: 1.2570 - learning_rate: 0.0010\n",
            "Epoch 36/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - classification_loss: 0.6875 - loss: 1.6498 - regression_loss: 9.6285 - val_classification_loss: 0.6914 - val_loss: 0.8124 - val_regression_loss: 1.2572 - learning_rate: 0.0010\n",
            "Epoch 37/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - classification_loss: 0.6891 - loss: 1.6673 - regression_loss: 9.7891 - val_classification_loss: 0.6913 - val_loss: 0.8124 - val_regression_loss: 1.2576 - learning_rate: 0.0010\n",
            "Epoch 38/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6901 - loss: 1.6173 - regression_loss: 9.2665 - val_classification_loss: 0.6912 - val_loss: 0.8126 - val_regression_loss: 1.2605 - learning_rate: 0.0010\n",
            "Epoch 39/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6877 - loss: 1.6355 - regression_loss: 9.4730 - val_classification_loss: 0.6915 - val_loss: 0.8128 - val_regression_loss: 1.2608 - learning_rate: 0.0010\n",
            "Epoch 40/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6887 - loss: 1.7235 - regression_loss: 10.3440 - val_classification_loss: 0.6916 - val_loss: 0.8127 - val_regression_loss: 1.2596 - learning_rate: 0.0010\n",
            "Epoch 41/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6886 - loss: 1.6823 - regression_loss: 9.9275 - val_classification_loss: 0.6913 - val_loss: 0.8126 - val_regression_loss: 1.2596 - learning_rate: 0.0010\n",
            "Epoch 42/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6858 - loss: 1.6705 - regression_loss: 9.8441 - val_classification_loss: 0.6916 - val_loss: 0.8128 - val_regression_loss: 1.2599 - learning_rate: 0.0010\n",
            "Epoch 43/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6901 - loss: 1.6753 - regression_loss: 9.8608 - val_classification_loss: 0.6912 - val_loss: 0.8129 - val_regression_loss: 1.2638 - learning_rate: 0.0010\n",
            "Epoch 44/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6866 - loss: 1.6694 - regression_loss: 9.8321 - val_classification_loss: 0.6916 - val_loss: 0.8126 - val_regression_loss: 1.2583 - learning_rate: 0.0010\n",
            "Epoch 45/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6895 - loss: 1.6451 - regression_loss: 9.5508 - val_classification_loss: 0.6911 - val_loss: 0.8125 - val_regression_loss: 1.2603 - learning_rate: 0.0010\n",
            "Epoch 46/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6876 - loss: 1.7132 - regression_loss: 10.2522 - val_classification_loss: 0.6913 - val_loss: 0.8125 - val_regression_loss: 1.2596 - learning_rate: 0.0010\n",
            "Epoch 47/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6895 - loss: 1.6469 - regression_loss: 9.5747 - val_classification_loss: 0.6913 - val_loss: 0.8127 - val_regression_loss: 1.2615 - learning_rate: 0.0010\n",
            "Epoch 48/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6892 - loss: 1.7033 - regression_loss: 10.1436 - val_classification_loss: 0.6913 - val_loss: 0.8130 - val_regression_loss: 1.2646 - learning_rate: 0.0010\n",
            "Epoch 49/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6898 - loss: 1.7451 - regression_loss: 10.5461 - val_classification_loss: 0.6913 - val_loss: 0.8131 - val_regression_loss: 1.2657 - learning_rate: 0.0010\n",
            "Epoch 50/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - classification_loss: 0.6880 - loss: 1.6636 - regression_loss: 9.7568 - val_classification_loss: 0.6917 - val_loss: 0.8127 - val_regression_loss: 1.2588 - learning_rate: 0.0010\n",
            "Epoch 51/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6872 - loss: 1.6588 - regression_loss: 9.7140 - val_classification_loss: 0.6915 - val_loss: 0.8128 - val_regression_loss: 1.2615 - learning_rate: 5.0000e-04\n",
            "Epoch 52/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6887 - loss: 1.6493 - regression_loss: 9.6156 - val_classification_loss: 0.6914 - val_loss: 0.8127 - val_regression_loss: 1.2617 - learning_rate: 5.0000e-04\n",
            "Epoch 53/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6882 - loss: 1.6563 - regression_loss: 9.6824 - val_classification_loss: 0.6913 - val_loss: 0.8127 - val_regression_loss: 1.2620 - learning_rate: 5.0000e-04\n",
            "Epoch 54/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6883 - loss: 1.6427 - regression_loss: 9.5455 - val_classification_loss: 0.6912 - val_loss: 0.8127 - val_regression_loss: 1.2618 - learning_rate: 5.0000e-04\n",
            "Epoch 55/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6879 - loss: 1.6369 - regression_loss: 9.4920 - val_classification_loss: 0.6914 - val_loss: 0.8126 - val_regression_loss: 1.2599 - learning_rate: 5.0000e-04\n",
            "Epoch 56/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - classification_loss: 0.6895 - loss: 1.6450 - regression_loss: 9.5549 - val_classification_loss: 0.6912 - val_loss: 0.8124 - val_regression_loss: 1.2590 - learning_rate: 5.0000e-04\n",
            "Epoch 57/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6862 - loss: 1.6057 - regression_loss: 9.2027 - val_classification_loss: 0.6913 - val_loss: 0.8125 - val_regression_loss: 1.2591 - learning_rate: 5.0000e-04\n",
            "Epoch 58/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - classification_loss: 0.6881 - loss: 1.6737 - regression_loss: 9.8512 - val_classification_loss: 0.6911 - val_loss: 0.8127 - val_regression_loss: 1.2626 - learning_rate: 5.0000e-04\n",
            "Epoch 59/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6885 - loss: 1.6510 - regression_loss: 9.6212 - val_classification_loss: 0.6910 - val_loss: 0.8128 - val_regression_loss: 1.2640 - learning_rate: 5.0000e-04\n",
            "Epoch 60/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6874 - loss: 1.7197 - regression_loss: 10.3132 - val_classification_loss: 0.6910 - val_loss: 0.8127 - val_regression_loss: 1.2630 - learning_rate: 5.0000e-04\n",
            "Epoch 61/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6874 - loss: 1.7253 - regression_loss: 10.3777 - val_classification_loss: 0.6910 - val_loss: 0.8127 - val_regression_loss: 1.2629 - learning_rate: 5.0000e-04\n",
            "Epoch 62/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6886 - loss: 1.6891 - regression_loss: 10.0057 - val_classification_loss: 0.6910 - val_loss: 0.8130 - val_regression_loss: 1.2654 - learning_rate: 5.0000e-04\n",
            "Epoch 63/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6877 - loss: 1.6611 - regression_loss: 9.7261 - val_classification_loss: 0.6911 - val_loss: 0.8128 - val_regression_loss: 1.2637 - learning_rate: 5.0000e-04\n",
            "Epoch 64/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6877 - loss: 1.7493 - regression_loss: 10.6212 - val_classification_loss: 0.6910 - val_loss: 0.8127 - val_regression_loss: 1.2625 - learning_rate: 5.0000e-04\n",
            "Epoch 65/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6876 - loss: 1.6176 - regression_loss: 9.3016 - val_classification_loss: 0.6910 - val_loss: 0.8128 - val_regression_loss: 1.2635 - learning_rate: 5.0000e-04\n",
            "Epoch 66/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6890 - loss: 1.6813 - regression_loss: 9.9239 - val_classification_loss: 0.6912 - val_loss: 0.8128 - val_regression_loss: 1.2625 - learning_rate: 5.0000e-04\n",
            "Epoch 67/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6885 - loss: 1.7162 - regression_loss: 10.2782 - val_classification_loss: 0.6912 - val_loss: 0.8129 - val_regression_loss: 1.2641 - learning_rate: 5.0000e-04\n",
            "Epoch 68/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6879 - loss: 1.7091 - regression_loss: 10.2110 - val_classification_loss: 0.6912 - val_loss: 0.8129 - val_regression_loss: 1.2636 - learning_rate: 5.0000e-04\n",
            "Epoch 69/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6864 - loss: 1.6837 - regression_loss: 9.9710 - val_classification_loss: 0.6910 - val_loss: 0.8128 - val_regression_loss: 1.2636 - learning_rate: 5.0000e-04\n",
            "Epoch 70/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6872 - loss: 1.7037 - regression_loss: 10.1593 - val_classification_loss: 0.6909 - val_loss: 0.8128 - val_regression_loss: 1.2641 - learning_rate: 5.0000e-04\n",
            "Epoch 71/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6867 - loss: 1.6887 - regression_loss: 10.0136 - val_classification_loss: 0.6910 - val_loss: 0.8127 - val_regression_loss: 1.2626 - learning_rate: 5.0000e-04\n",
            "Epoch 72/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6879 - loss: 1.6994 - regression_loss: 10.1202 - val_classification_loss: 0.6910 - val_loss: 0.8129 - val_regression_loss: 1.2645 - learning_rate: 5.0000e-04\n",
            "Epoch 73/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6882 - loss: 1.6632 - regression_loss: 9.7515 - val_classification_loss: 0.6910 - val_loss: 0.8127 - val_regression_loss: 1.2628 - learning_rate: 5.0000e-04\n",
            "Epoch 74/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6907 - loss: 1.6905 - regression_loss: 10.0029 - val_classification_loss: 0.6910 - val_loss: 0.8130 - val_regression_loss: 1.2659 - learning_rate: 5.0000e-04\n",
            "Epoch 75/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - classification_loss: 0.6884 - loss: 1.6638 - regression_loss: 9.7419 - val_classification_loss: 0.6911 - val_loss: 0.8128 - val_regression_loss: 1.2637 - learning_rate: 5.0000e-04\n",
            "Epoch 76/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6884 - loss: 1.6722 - regression_loss: 9.8349 - val_classification_loss: 0.6910 - val_loss: 0.8128 - val_regression_loss: 1.2637 - learning_rate: 5.0000e-04\n",
            "Epoch 77/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6875 - loss: 1.7189 - regression_loss: 10.3176 - val_classification_loss: 0.6909 - val_loss: 0.8129 - val_regression_loss: 1.2653 - learning_rate: 5.0000e-04\n",
            "Epoch 78/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6855 - loss: 1.6417 - regression_loss: 9.5671 - val_classification_loss: 0.6911 - val_loss: 0.8130 - val_regression_loss: 1.2659 - learning_rate: 5.0000e-04\n",
            "Epoch 79/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - classification_loss: 0.6890 - loss: 1.6545 - regression_loss: 9.6606 - val_classification_loss: 0.6910 - val_loss: 0.8133 - val_regression_loss: 1.2690 - learning_rate: 5.0000e-04\n",
            "Epoch 80/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - classification_loss: 0.6872 - loss: 1.6277 - regression_loss: 9.3995 - val_classification_loss: 0.6912 - val_loss: 0.8127 - val_regression_loss: 1.2619 - learning_rate: 5.0000e-04\n",
            "Epoch 81/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6867 - loss: 1.6576 - regression_loss: 9.7126 - val_classification_loss: 0.6911 - val_loss: 0.8128 - val_regression_loss: 1.2633 - learning_rate: 5.0000e-04\n",
            "Epoch 82/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6852 - loss: 1.6801 - regression_loss: 9.9452 - val_classification_loss: 0.6912 - val_loss: 0.8128 - val_regression_loss: 1.2625 - learning_rate: 5.0000e-04\n",
            "Epoch 83/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6869 - loss: 1.6535 - regression_loss: 9.6621 - val_classification_loss: 0.6910 - val_loss: 0.8131 - val_regression_loss: 1.2665 - learning_rate: 5.0000e-04\n",
            "Epoch 84/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6897 - loss: 1.6635 - regression_loss: 9.7353 - val_classification_loss: 0.6910 - val_loss: 0.8131 - val_regression_loss: 1.2666 - learning_rate: 5.0000e-04\n",
            "Epoch 85/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6891 - loss: 1.6797 - regression_loss: 9.9112 - val_classification_loss: 0.6910 - val_loss: 0.8129 - val_regression_loss: 1.2645 - learning_rate: 5.0000e-04\n",
            "Epoch 86/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6899 - loss: 1.6760 - regression_loss: 9.8547 - val_classification_loss: 0.6911 - val_loss: 0.8126 - val_regression_loss: 1.2618 - learning_rate: 5.0000e-04\n",
            "Epoch 87/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6861 - loss: 1.6585 - regression_loss: 9.7308 - val_classification_loss: 0.6913 - val_loss: 0.8129 - val_regression_loss: 1.2632 - learning_rate: 5.0000e-04\n",
            "Epoch 88/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6872 - loss: 1.7114 - regression_loss: 10.2393 - val_classification_loss: 0.6912 - val_loss: 0.8131 - val_regression_loss: 1.2657 - learning_rate: 5.0000e-04\n",
            "Epoch 89/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - classification_loss: 0.6859 - loss: 1.6453 - regression_loss: 9.5992 - val_classification_loss: 0.6910 - val_loss: 0.8130 - val_regression_loss: 1.2659 - learning_rate: 5.0000e-04\n",
            "Epoch 90/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6891 - loss: 1.7040 - regression_loss: 10.1508 - val_classification_loss: 0.6909 - val_loss: 0.8132 - val_regression_loss: 1.2682 - learning_rate: 5.0000e-04\n",
            "Epoch 91/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6904 - loss: 1.6895 - regression_loss: 9.9828 - val_classification_loss: 0.6911 - val_loss: 0.8129 - val_regression_loss: 1.2645 - learning_rate: 5.0000e-04\n",
            "Epoch 92/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6878 - loss: 1.6493 - regression_loss: 9.6166 - val_classification_loss: 0.6914 - val_loss: 0.8129 - val_regression_loss: 1.2621 - learning_rate: 5.0000e-04\n",
            "Epoch 93/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6858 - loss: 1.6733 - regression_loss: 9.8769 - val_classification_loss: 0.6915 - val_loss: 0.8127 - val_regression_loss: 1.2604 - learning_rate: 5.0000e-04\n",
            "Epoch 94/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6864 - loss: 1.6917 - regression_loss: 10.0609 - val_classification_loss: 0.6911 - val_loss: 0.8130 - val_regression_loss: 1.2651 - learning_rate: 5.0000e-04\n",
            "Epoch 95/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6871 - loss: 1.6836 - regression_loss: 9.9707 - val_classification_loss: 0.6909 - val_loss: 0.8129 - val_regression_loss: 1.2648 - learning_rate: 5.0000e-04\n",
            "Epoch 96/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6860 - loss: 1.6881 - regression_loss: 10.0138 - val_classification_loss: 0.6911 - val_loss: 0.8128 - val_regression_loss: 1.2633 - learning_rate: 5.0000e-04\n",
            "Epoch 97/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6897 - loss: 1.7034 - regression_loss: 10.1249 - val_classification_loss: 0.6910 - val_loss: 0.8131 - val_regression_loss: 1.2665 - learning_rate: 5.0000e-04\n",
            "Epoch 98/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6865 - loss: 1.6968 - regression_loss: 10.0961 - val_classification_loss: 0.6912 - val_loss: 0.8128 - val_regression_loss: 1.2630 - learning_rate: 5.0000e-04\n",
            "Epoch 99/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6852 - loss: 1.6062 - regression_loss: 9.2156 - val_classification_loss: 0.6913 - val_loss: 0.8131 - val_regression_loss: 1.2645 - learning_rate: 5.0000e-04\n",
            "Epoch 100/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - classification_loss: 0.6877 - loss: 1.7059 - regression_loss: 10.1836 - val_classification_loss: 0.6911 - val_loss: 0.8132 - val_regression_loss: 1.2675 - learning_rate: 5.0000e-04\n",
            "Epoch 101/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - classification_loss: 0.6867 - loss: 1.6966 - regression_loss: 10.0965 - val_classification_loss: 0.6911 - val_loss: 0.8132 - val_regression_loss: 1.2675 - learning_rate: 2.5000e-04\n",
            "Epoch 102/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6872 - loss: 1.6962 - regression_loss: 10.0859 - val_classification_loss: 0.6911 - val_loss: 0.8130 - val_regression_loss: 1.2647 - learning_rate: 2.5000e-04\n",
            "Epoch 103/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - classification_loss: 0.6888 - loss: 1.6901 - regression_loss: 10.0057 - val_classification_loss: 0.6911 - val_loss: 0.8130 - val_regression_loss: 1.2656 - learning_rate: 2.5000e-04\n",
            "Epoch 104/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6879 - loss: 1.7056 - regression_loss: 10.1756 - val_classification_loss: 0.6912 - val_loss: 0.8130 - val_regression_loss: 1.2647 - learning_rate: 2.5000e-04\n",
            "Epoch 105/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6855 - loss: 1.6673 - regression_loss: 9.8207 - val_classification_loss: 0.6911 - val_loss: 0.8129 - val_regression_loss: 1.2646 - learning_rate: 2.5000e-04\n",
            "Epoch 106/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6870 - loss: 1.6946 - regression_loss: 10.0866 - val_classification_loss: 0.6911 - val_loss: 0.8129 - val_regression_loss: 1.2649 - learning_rate: 2.5000e-04\n",
            "Epoch 107/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6892 - loss: 1.6251 - regression_loss: 9.3590 - val_classification_loss: 0.6910 - val_loss: 0.8131 - val_regression_loss: 1.2670 - learning_rate: 2.5000e-04\n",
            "Epoch 108/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6890 - loss: 1.6420 - regression_loss: 9.5330 - val_classification_loss: 0.6910 - val_loss: 0.8131 - val_regression_loss: 1.2661 - learning_rate: 2.5000e-04\n",
            "Epoch 109/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6880 - loss: 1.6491 - regression_loss: 9.6071 - val_classification_loss: 0.6910 - val_loss: 0.8131 - val_regression_loss: 1.2670 - learning_rate: 2.5000e-04\n",
            "Epoch 110/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6862 - loss: 1.6518 - regression_loss: 9.6551 - val_classification_loss: 0.6911 - val_loss: 0.8132 - val_regression_loss: 1.2671 - learning_rate: 2.5000e-04\n",
            "Epoch 111/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6889 - loss: 1.6599 - regression_loss: 9.7133 - val_classification_loss: 0.6910 - val_loss: 0.8132 - val_regression_loss: 1.2678 - learning_rate: 2.5000e-04\n",
            "Epoch 112/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6877 - loss: 1.7363 - regression_loss: 10.4778 - val_classification_loss: 0.6909 - val_loss: 0.8130 - val_regression_loss: 1.2662 - learning_rate: 2.5000e-04\n",
            "Epoch 113/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6860 - loss: 1.6489 - regression_loss: 9.6299 - val_classification_loss: 0.6909 - val_loss: 0.8132 - val_regression_loss: 1.2679 - learning_rate: 2.5000e-04\n",
            "Epoch 114/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6880 - loss: 1.6849 - regression_loss: 9.9737 - val_classification_loss: 0.6910 - val_loss: 0.8130 - val_regression_loss: 1.2660 - learning_rate: 2.5000e-04\n",
            "Epoch 115/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6887 - loss: 1.6501 - regression_loss: 9.6104 - val_classification_loss: 0.6909 - val_loss: 0.8131 - val_regression_loss: 1.2671 - learning_rate: 2.5000e-04\n",
            "Epoch 116/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6862 - loss: 1.6639 - regression_loss: 9.7681 - val_classification_loss: 0.6910 - val_loss: 0.8132 - val_regression_loss: 1.2682 - learning_rate: 2.5000e-04\n",
            "Epoch 117/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6898 - loss: 1.7222 - regression_loss: 10.3214 - val_classification_loss: 0.6909 - val_loss: 0.8134 - val_regression_loss: 1.2707 - learning_rate: 2.5000e-04\n",
            "Epoch 118/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6877 - loss: 1.5777 - regression_loss: 8.9126 - val_classification_loss: 0.6909 - val_loss: 0.8132 - val_regression_loss: 1.2684 - learning_rate: 2.5000e-04\n",
            "Epoch 119/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6885 - loss: 1.6669 - regression_loss: 9.7755 - val_classification_loss: 0.6909 - val_loss: 0.8132 - val_regression_loss: 1.2682 - learning_rate: 2.5000e-04\n",
            "Epoch 120/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6876 - loss: 1.6955 - regression_loss: 10.0740 - val_classification_loss: 0.6909 - val_loss: 0.8130 - val_regression_loss: 1.2664 - learning_rate: 2.5000e-04\n",
            "Epoch 121/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6894 - loss: 1.6810 - regression_loss: 9.9215 - val_classification_loss: 0.6909 - val_loss: 0.8132 - val_regression_loss: 1.2682 - learning_rate: 2.5000e-04\n",
            "Epoch 122/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6901 - loss: 1.6562 - regression_loss: 9.6660 - val_classification_loss: 0.6908 - val_loss: 0.8134 - val_regression_loss: 1.2704 - learning_rate: 2.5000e-04\n",
            "Epoch 123/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6861 - loss: 1.6911 - regression_loss: 10.0585 - val_classification_loss: 0.6909 - val_loss: 0.8134 - val_regression_loss: 1.2703 - learning_rate: 2.5000e-04\n",
            "Epoch 124/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - classification_loss: 0.6875 - loss: 1.6853 - regression_loss: 9.9775 - val_classification_loss: 0.6909 - val_loss: 0.8134 - val_regression_loss: 1.2705 - learning_rate: 2.5000e-04\n",
            "Epoch 125/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - classification_loss: 0.6864 - loss: 1.6935 - regression_loss: 10.0839 - val_classification_loss: 0.6909 - val_loss: 0.8132 - val_regression_loss: 1.2683 - learning_rate: 2.5000e-04\n",
            "Epoch 126/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - classification_loss: 0.6888 - loss: 1.7216 - regression_loss: 10.3172 - val_classification_loss: 0.6908 - val_loss: 0.8133 - val_regression_loss: 1.2689 - learning_rate: 2.5000e-04\n",
            "Epoch 127/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - classification_loss: 0.6860 - loss: 1.6646 - regression_loss: 9.7898 - val_classification_loss: 0.6908 - val_loss: 0.8132 - val_regression_loss: 1.2681 - learning_rate: 2.5000e-04\n",
            "Epoch 128/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6848 - loss: 1.6728 - regression_loss: 9.8773 - val_classification_loss: 0.6909 - val_loss: 0.8133 - val_regression_loss: 1.2696 - learning_rate: 2.5000e-04\n",
            "Epoch 129/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6875 - loss: 1.6788 - regression_loss: 9.9110 - val_classification_loss: 0.6909 - val_loss: 0.8133 - val_regression_loss: 1.2690 - learning_rate: 2.5000e-04\n",
            "Epoch 130/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6867 - loss: 1.6435 - regression_loss: 9.5688 - val_classification_loss: 0.6909 - val_loss: 0.8133 - val_regression_loss: 1.2695 - learning_rate: 2.5000e-04\n",
            "Epoch 131/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6867 - loss: 1.6618 - regression_loss: 9.7486 - val_classification_loss: 0.6908 - val_loss: 0.8135 - val_regression_loss: 1.2718 - learning_rate: 2.5000e-04\n",
            "Epoch 132/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6840 - loss: 1.6220 - regression_loss: 9.3841 - val_classification_loss: 0.6908 - val_loss: 0.8136 - val_regression_loss: 1.2723 - learning_rate: 2.5000e-04\n",
            "Epoch 133/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6855 - loss: 1.6949 - regression_loss: 10.0879 - val_classification_loss: 0.6908 - val_loss: 0.8136 - val_regression_loss: 1.2724 - learning_rate: 2.5000e-04\n",
            "Epoch 134/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6882 - loss: 1.6924 - regression_loss: 10.0334 - val_classification_loss: 0.6908 - val_loss: 0.8132 - val_regression_loss: 1.2681 - learning_rate: 2.5000e-04\n",
            "Epoch 135/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6878 - loss: 1.7291 - regression_loss: 10.4135 - val_classification_loss: 0.6908 - val_loss: 0.8133 - val_regression_loss: 1.2693 - learning_rate: 2.5000e-04\n",
            "Epoch 136/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6833 - loss: 1.6573 - regression_loss: 9.7495 - val_classification_loss: 0.6908 - val_loss: 0.8129 - val_regression_loss: 1.2657 - learning_rate: 2.5000e-04\n",
            "Epoch 137/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6864 - loss: 1.6602 - regression_loss: 9.7275 - val_classification_loss: 0.6908 - val_loss: 0.8134 - val_regression_loss: 1.2704 - learning_rate: 2.5000e-04\n",
            "Epoch 138/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6848 - loss: 1.6817 - regression_loss: 9.9750 - val_classification_loss: 0.6908 - val_loss: 0.8137 - val_regression_loss: 1.2736 - learning_rate: 2.5000e-04\n",
            "Epoch 139/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6880 - loss: 1.6734 - regression_loss: 9.8512 - val_classification_loss: 0.6908 - val_loss: 0.8137 - val_regression_loss: 1.2734 - learning_rate: 2.5000e-04\n",
            "Epoch 140/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6887 - loss: 1.5980 - regression_loss: 9.1050 - val_classification_loss: 0.6908 - val_loss: 0.8137 - val_regression_loss: 1.2731 - learning_rate: 2.5000e-04\n",
            "Epoch 141/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6867 - loss: 1.6217 - regression_loss: 9.3513 - val_classification_loss: 0.6908 - val_loss: 0.8133 - val_regression_loss: 1.2689 - learning_rate: 2.5000e-04\n",
            "Epoch 142/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6875 - loss: 1.6974 - regression_loss: 10.1010 - val_classification_loss: 0.6908 - val_loss: 0.8132 - val_regression_loss: 1.2681 - learning_rate: 2.5000e-04\n",
            "Epoch 143/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6856 - loss: 1.6974 - regression_loss: 10.1041 - val_classification_loss: 0.6909 - val_loss: 0.8135 - val_regression_loss: 1.2710 - learning_rate: 2.5000e-04\n",
            "Epoch 144/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6866 - loss: 1.7024 - regression_loss: 10.1584 - val_classification_loss: 0.6909 - val_loss: 0.8138 - val_regression_loss: 1.2745 - learning_rate: 2.5000e-04\n",
            "Epoch 145/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - classification_loss: 0.6877 - loss: 1.6633 - regression_loss: 9.7486 - val_classification_loss: 0.6909 - val_loss: 0.8139 - val_regression_loss: 1.2757 - learning_rate: 2.5000e-04\n",
            "Epoch 146/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - classification_loss: 0.6863 - loss: 1.5825 - regression_loss: 8.9736 - val_classification_loss: 0.6909 - val_loss: 0.8140 - val_regression_loss: 1.2761 - learning_rate: 2.5000e-04\n",
            "Epoch 147/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - classification_loss: 0.6866 - loss: 1.6849 - regression_loss: 9.9882 - val_classification_loss: 0.6909 - val_loss: 0.8137 - val_regression_loss: 1.2735 - learning_rate: 2.5000e-04\n",
            "Epoch 148/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - classification_loss: 0.6884 - loss: 1.6843 - regression_loss: 9.9641 - val_classification_loss: 0.6909 - val_loss: 0.8139 - val_regression_loss: 1.2753 - learning_rate: 2.5000e-04\n",
            "Epoch 149/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6827 - loss: 1.6674 - regression_loss: 9.8457 - val_classification_loss: 0.6909 - val_loss: 0.8139 - val_regression_loss: 1.2756 - learning_rate: 2.5000e-04\n",
            "Epoch 150/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6882 - loss: 1.6480 - regression_loss: 9.5940 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2772 - learning_rate: 2.5000e-04\n",
            "Epoch 151/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6867 - loss: 1.6871 - regression_loss: 9.9990 - val_classification_loss: 0.6909 - val_loss: 0.8140 - val_regression_loss: 1.2761 - learning_rate: 1.2500e-04\n",
            "Epoch 152/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6875 - loss: 1.6303 - regression_loss: 9.4375 - val_classification_loss: 0.6909 - val_loss: 0.8141 - val_regression_loss: 1.2770 - learning_rate: 1.2500e-04\n",
            "Epoch 153/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6867 - loss: 1.6388 - regression_loss: 9.5227 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2778 - learning_rate: 1.2500e-04\n",
            "Epoch 154/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6869 - loss: 1.6472 - regression_loss: 9.6097 - val_classification_loss: 0.6909 - val_loss: 0.8141 - val_regression_loss: 1.2777 - learning_rate: 1.2500e-04\n",
            "Epoch 155/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6854 - loss: 1.7195 - regression_loss: 10.3371 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2775 - learning_rate: 1.2500e-04\n",
            "Epoch 156/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6850 - loss: 1.6762 - regression_loss: 9.9141 - val_classification_loss: 0.6908 - val_loss: 0.8139 - val_regression_loss: 1.2757 - learning_rate: 1.2500e-04\n",
            "Epoch 157/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6866 - loss: 1.6468 - regression_loss: 9.6012 - val_classification_loss: 0.6908 - val_loss: 0.8138 - val_regression_loss: 1.2750 - learning_rate: 1.2500e-04\n",
            "Epoch 158/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - classification_loss: 0.6858 - loss: 1.6770 - regression_loss: 9.9183 - val_classification_loss: 0.6908 - val_loss: 0.8138 - val_regression_loss: 1.2748 - learning_rate: 1.2500e-04\n",
            "Epoch 159/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6876 - loss: 1.6771 - regression_loss: 9.8866 - val_classification_loss: 0.6908 - val_loss: 0.8138 - val_regression_loss: 1.2746 - learning_rate: 1.2500e-04\n",
            "Epoch 160/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6894 - loss: 1.6576 - regression_loss: 9.6897 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2761 - learning_rate: 1.2500e-04\n",
            "Epoch 161/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6868 - loss: 1.6610 - regression_loss: 9.7377 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2768 - learning_rate: 1.2500e-04\n",
            "Epoch 162/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6862 - loss: 1.6583 - regression_loss: 9.7246 - val_classification_loss: 0.6908 - val_loss: 0.8139 - val_regression_loss: 1.2751 - learning_rate: 1.2500e-04\n",
            "Epoch 163/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6866 - loss: 1.6190 - regression_loss: 9.3320 - val_classification_loss: 0.6908 - val_loss: 0.8139 - val_regression_loss: 1.2757 - learning_rate: 1.2500e-04\n",
            "Epoch 164/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6860 - loss: 1.6751 - regression_loss: 9.8867 - val_classification_loss: 0.6908 - val_loss: 0.8139 - val_regression_loss: 1.2757 - learning_rate: 1.2500e-04\n",
            "Epoch 165/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6880 - loss: 1.6861 - regression_loss: 9.9874 - val_classification_loss: 0.6908 - val_loss: 0.8139 - val_regression_loss: 1.2756 - learning_rate: 1.2500e-04\n",
            "Epoch 166/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6859 - loss: 1.6463 - regression_loss: 9.6133 - val_classification_loss: 0.6908 - val_loss: 0.8138 - val_regression_loss: 1.2747 - learning_rate: 1.2500e-04\n",
            "Epoch 167/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6882 - loss: 1.6222 - regression_loss: 9.3424 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2764 - learning_rate: 1.2500e-04\n",
            "Epoch 168/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6874 - loss: 1.6798 - regression_loss: 9.9314 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2764 - learning_rate: 1.2500e-04\n",
            "Epoch 169/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6851 - loss: 1.6686 - regression_loss: 9.8345 - val_classification_loss: 0.6908 - val_loss: 0.8138 - val_regression_loss: 1.2749 - learning_rate: 1.2500e-04\n",
            "Epoch 170/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - classification_loss: 0.6899 - loss: 1.7122 - regression_loss: 10.2236 - val_classification_loss: 0.6908 - val_loss: 0.8139 - val_regression_loss: 1.2755 - learning_rate: 1.2500e-04\n",
            "Epoch 171/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - classification_loss: 0.6854 - loss: 1.6048 - regression_loss: 9.1974 - val_classification_loss: 0.6908 - val_loss: 0.8138 - val_regression_loss: 1.2746 - learning_rate: 1.2500e-04\n",
            "Epoch 172/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6860 - loss: 1.6586 - regression_loss: 9.7136 - val_classification_loss: 0.6908 - val_loss: 0.8136 - val_regression_loss: 1.2733 - learning_rate: 1.2500e-04\n",
            "Epoch 173/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6853 - loss: 1.6369 - regression_loss: 9.5198 - val_classification_loss: 0.6908 - val_loss: 0.8138 - val_regression_loss: 1.2751 - learning_rate: 1.2500e-04\n",
            "Epoch 174/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - classification_loss: 0.6877 - loss: 1.6614 - regression_loss: 9.7338 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2765 - learning_rate: 1.2500e-04\n",
            "Epoch 175/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6899 - loss: 1.6784 - regression_loss: 9.8845 - val_classification_loss: 0.6908 - val_loss: 0.8144 - val_regression_loss: 1.2807 - learning_rate: 1.2500e-04\n",
            "Epoch 176/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6877 - loss: 1.6554 - regression_loss: 9.6742 - val_classification_loss: 0.6908 - val_loss: 0.8145 - val_regression_loss: 1.2816 - learning_rate: 1.2500e-04\n",
            "Epoch 177/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6840 - loss: 1.6024 - regression_loss: 9.1800 - val_classification_loss: 0.6908 - val_loss: 0.8143 - val_regression_loss: 1.2800 - learning_rate: 1.2500e-04\n",
            "Epoch 178/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 44ms/step - classification_loss: 0.6886 - loss: 1.6362 - regression_loss: 9.4663 - val_classification_loss: 0.6908 - val_loss: 0.8144 - val_regression_loss: 1.2804 - learning_rate: 1.2500e-04\n",
            "Epoch 179/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6866 - loss: 1.6117 - regression_loss: 9.2522 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2786 - learning_rate: 1.2500e-04\n",
            "Epoch 180/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6891 - loss: 1.7114 - regression_loss: 10.2312 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2821 - learning_rate: 1.2500e-04\n",
            "Epoch 181/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6848 - loss: 1.6989 - regression_loss: 10.1392 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2779 - learning_rate: 1.2500e-04\n",
            "Epoch 182/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6855 - loss: 1.6789 - regression_loss: 9.9295 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2795 - learning_rate: 1.2500e-04\n",
            "Epoch 183/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6862 - loss: 1.6585 - regression_loss: 9.7246 - val_classification_loss: 0.6908 - val_loss: 0.8145 - val_regression_loss: 1.2818 - learning_rate: 1.2500e-04\n",
            "Epoch 184/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6861 - loss: 1.6740 - regression_loss: 9.8756 - val_classification_loss: 0.6908 - val_loss: 0.8143 - val_regression_loss: 1.2798 - learning_rate: 1.2500e-04\n",
            "Epoch 185/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - classification_loss: 0.6893 - loss: 1.6650 - regression_loss: 9.7599 - val_classification_loss: 0.6908 - val_loss: 0.8145 - val_regression_loss: 1.2820 - learning_rate: 1.2500e-04\n",
            "Epoch 186/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6880 - loss: 1.6869 - regression_loss: 9.9866 - val_classification_loss: 0.6908 - val_loss: 0.8144 - val_regression_loss: 1.2807 - learning_rate: 1.2500e-04\n",
            "Epoch 187/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6866 - loss: 1.6946 - regression_loss: 10.0806 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2781 - learning_rate: 1.2500e-04\n",
            "Epoch 188/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6877 - loss: 1.7176 - regression_loss: 10.2939 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2776 - learning_rate: 1.2500e-04\n",
            "Epoch 189/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6891 - loss: 1.6624 - regression_loss: 9.7363 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2786 - learning_rate: 1.2500e-04\n",
            "Epoch 190/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - classification_loss: 0.6861 - loss: 1.6969 - regression_loss: 10.1117 - val_classification_loss: 0.6908 - val_loss: 0.8143 - val_regression_loss: 1.2798 - learning_rate: 1.2500e-04\n",
            "Epoch 191/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - classification_loss: 0.6873 - loss: 1.6338 - regression_loss: 9.4683 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2791 - learning_rate: 1.2500e-04\n",
            "Epoch 192/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - classification_loss: 0.6850 - loss: 1.6733 - regression_loss: 9.8756 - val_classification_loss: 0.6908 - val_loss: 0.8146 - val_regression_loss: 1.2832 - learning_rate: 1.2500e-04\n",
            "Epoch 193/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6890 - loss: 1.6806 - regression_loss: 9.9121 - val_classification_loss: 0.6908 - val_loss: 0.8149 - val_regression_loss: 1.2863 - learning_rate: 1.2500e-04\n",
            "Epoch 194/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6881 - loss: 1.6167 - regression_loss: 9.2866 - val_classification_loss: 0.6908 - val_loss: 0.8147 - val_regression_loss: 1.2840 - learning_rate: 1.2500e-04\n",
            "Epoch 195/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6857 - loss: 1.6306 - regression_loss: 9.4470 - val_classification_loss: 0.6908 - val_loss: 0.8145 - val_regression_loss: 1.2816 - learning_rate: 1.2500e-04\n",
            "Epoch 196/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6850 - loss: 1.6736 - regression_loss: 9.8855 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2792 - learning_rate: 1.2500e-04\n",
            "Epoch 197/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6862 - loss: 1.6795 - regression_loss: 9.9325 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2770 - learning_rate: 1.2500e-04\n",
            "Epoch 198/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6856 - loss: 1.6525 - regression_loss: 9.6652 - val_classification_loss: 0.6909 - val_loss: 0.8137 - val_regression_loss: 1.2741 - learning_rate: 1.2500e-04\n",
            "Epoch 199/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6879 - loss: 1.6455 - regression_loss: 9.5773 - val_classification_loss: 0.6908 - val_loss: 0.8138 - val_regression_loss: 1.2755 - learning_rate: 1.2500e-04\n",
            "Epoch 200/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6859 - loss: 1.6398 - regression_loss: 9.5477 - val_classification_loss: 0.6908 - val_loss: 0.8138 - val_regression_loss: 1.2753 - learning_rate: 1.2500e-04\n",
            "Epoch 201/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6827 - loss: 1.6591 - regression_loss: 9.7557 - val_classification_loss: 0.6908 - val_loss: 0.8139 - val_regression_loss: 1.2757 - learning_rate: 6.2500e-05\n",
            "Epoch 202/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6894 - loss: 1.6398 - regression_loss: 9.5146 - val_classification_loss: 0.6908 - val_loss: 0.8139 - val_regression_loss: 1.2765 - learning_rate: 6.2500e-05\n",
            "Epoch 203/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6902 - loss: 1.6499 - regression_loss: 9.5979 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2785 - learning_rate: 6.2500e-05\n",
            "Epoch 204/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6866 - loss: 1.6114 - regression_loss: 9.2506 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2778 - learning_rate: 6.2500e-05\n",
            "Epoch 205/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6880 - loss: 1.6228 - regression_loss: 9.3518 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2771 - learning_rate: 6.2500e-05\n",
            "Epoch 206/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6876 - loss: 1.6274 - regression_loss: 9.4022 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2781 - learning_rate: 6.2500e-05\n",
            "Epoch 207/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6888 - loss: 1.6129 - regression_loss: 9.2436 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2773 - learning_rate: 6.2500e-05\n",
            "Epoch 208/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6862 - loss: 1.7064 - regression_loss: 10.1901 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2770 - learning_rate: 6.2500e-05\n",
            "Epoch 209/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6892 - loss: 1.6666 - regression_loss: 9.7606 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2783 - learning_rate: 6.2500e-05\n",
            "Epoch 210/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - classification_loss: 0.6849 - loss: 1.6450 - regression_loss: 9.6055 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2774 - learning_rate: 6.2500e-05\n",
            "Epoch 211/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - classification_loss: 0.6870 - loss: 1.6075 - regression_loss: 9.2161 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2780 - learning_rate: 6.2500e-05\n",
            "Epoch 212/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6877 - loss: 1.7049 - regression_loss: 10.1729 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2780 - learning_rate: 6.2500e-05\n",
            "Epoch 213/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6849 - loss: 1.6187 - regression_loss: 9.3349 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2775 - learning_rate: 6.2500e-05\n",
            "Epoch 214/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6875 - loss: 1.6672 - regression_loss: 9.7979 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2776 - learning_rate: 6.2500e-05\n",
            "Epoch 215/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6893 - loss: 1.6758 - regression_loss: 9.8626 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2791 - learning_rate: 6.2500e-05\n",
            "Epoch 216/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6875 - loss: 1.7128 - regression_loss: 10.2495 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2778 - learning_rate: 6.2500e-05\n",
            "Epoch 217/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6865 - loss: 1.6625 - regression_loss: 9.7520 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2782 - learning_rate: 6.2500e-05\n",
            "Epoch 218/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6841 - loss: 1.6401 - regression_loss: 9.5640 - val_classification_loss: 0.6908 - val_loss: 0.8139 - val_regression_loss: 1.2768 - learning_rate: 6.2500e-05\n",
            "Epoch 219/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6881 - loss: 1.6605 - regression_loss: 9.7228 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2784 - learning_rate: 6.2500e-05\n",
            "Epoch 220/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - classification_loss: 0.6866 - loss: 1.6272 - regression_loss: 9.4060 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2782 - learning_rate: 6.2500e-05\n",
            "Epoch 221/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6858 - loss: 1.6769 - regression_loss: 9.9067 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2772 - learning_rate: 6.2500e-05\n",
            "Epoch 222/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6890 - loss: 1.6558 - regression_loss: 9.6659 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2785 - learning_rate: 6.2500e-05\n",
            "Epoch 223/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6881 - loss: 1.6680 - regression_loss: 9.7864 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2789 - learning_rate: 6.2500e-05\n",
            "Epoch 224/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6909 - loss: 1.6887 - regression_loss: 9.9730 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2796 - learning_rate: 6.2500e-05\n",
            "Epoch 225/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6858 - loss: 1.6523 - regression_loss: 9.6657 - val_classification_loss: 0.6908 - val_loss: 0.8140 - val_regression_loss: 1.2775 - learning_rate: 6.2500e-05\n",
            "Epoch 226/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6870 - loss: 1.6764 - regression_loss: 9.8897 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2783 - learning_rate: 6.2500e-05\n",
            "Epoch 227/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6879 - loss: 1.6622 - regression_loss: 9.7403 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2789 - learning_rate: 6.2500e-05\n",
            "Epoch 228/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6859 - loss: 1.7044 - regression_loss: 10.1854 - val_classification_loss: 0.6908 - val_loss: 0.8141 - val_regression_loss: 1.2787 - learning_rate: 6.2500e-05\n",
            "Epoch 229/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6875 - loss: 1.6395 - regression_loss: 9.5204 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2797 - learning_rate: 6.2500e-05\n",
            "Epoch 230/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - classification_loss: 0.6854 - loss: 1.6257 - regression_loss: 9.4117 - val_classification_loss: 0.6908 - val_loss: 0.8144 - val_regression_loss: 1.2814 - learning_rate: 6.2500e-05\n",
            "Epoch 231/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6839 - loss: 1.7008 - regression_loss: 10.1733 - val_classification_loss: 0.6908 - val_loss: 0.8146 - val_regression_loss: 1.2835 - learning_rate: 6.2500e-05\n",
            "Epoch 232/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6884 - loss: 1.6458 - regression_loss: 9.5781 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2835 - learning_rate: 6.2500e-05\n",
            "Epoch 233/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6855 - loss: 1.6538 - regression_loss: 9.6777 - val_classification_loss: 0.6907 - val_loss: 0.8147 - val_regression_loss: 1.2840 - learning_rate: 6.2500e-05\n",
            "Epoch 234/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - classification_loss: 0.6896 - loss: 1.6388 - regression_loss: 9.4969 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2837 - learning_rate: 6.2500e-05\n",
            "Epoch 235/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 63ms/step - classification_loss: 0.6864 - loss: 1.7236 - regression_loss: 10.3606 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2825 - learning_rate: 6.2500e-05\n",
            "Epoch 236/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6901 - loss: 1.6497 - regression_loss: 9.5900 - val_classification_loss: 0.6907 - val_loss: 0.8147 - val_regression_loss: 1.2841 - learning_rate: 6.2500e-05\n",
            "Epoch 237/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - classification_loss: 0.6879 - loss: 1.6546 - regression_loss: 9.6590 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2822 - learning_rate: 6.2500e-05\n",
            "Epoch 238/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6866 - loss: 1.6756 - regression_loss: 9.8976 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2793 - learning_rate: 6.2500e-05\n",
            "Epoch 239/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6861 - loss: 1.6698 - regression_loss: 9.8330 - val_classification_loss: 0.6908 - val_loss: 0.8144 - val_regression_loss: 1.2812 - learning_rate: 6.2500e-05\n",
            "Epoch 240/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6862 - loss: 1.6413 - regression_loss: 9.5551 - val_classification_loss: 0.6908 - val_loss: 0.8145 - val_regression_loss: 1.2822 - learning_rate: 6.2500e-05\n",
            "Epoch 241/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6862 - loss: 1.6377 - regression_loss: 9.5262 - val_classification_loss: 0.6908 - val_loss: 0.8144 - val_regression_loss: 1.2818 - learning_rate: 6.2500e-05\n",
            "Epoch 242/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6854 - loss: 1.6868 - regression_loss: 10.0173 - val_classification_loss: 0.6908 - val_loss: 0.8146 - val_regression_loss: 1.2834 - learning_rate: 6.2500e-05\n",
            "Epoch 243/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6863 - loss: 1.6733 - regression_loss: 9.8669 - val_classification_loss: 0.6908 - val_loss: 0.8146 - val_regression_loss: 1.2833 - learning_rate: 6.2500e-05\n",
            "Epoch 244/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6864 - loss: 1.6631 - regression_loss: 9.7646 - val_classification_loss: 0.6908 - val_loss: 0.8145 - val_regression_loss: 1.2827 - learning_rate: 6.2500e-05\n",
            "Epoch 245/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6864 - loss: 1.6606 - regression_loss: 9.7457 - val_classification_loss: 0.6908 - val_loss: 0.8145 - val_regression_loss: 1.2822 - learning_rate: 6.2500e-05\n",
            "Epoch 246/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6892 - loss: 1.7348 - regression_loss: 10.4516 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2831 - learning_rate: 6.2500e-05\n",
            "Epoch 247/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6862 - loss: 1.6715 - regression_loss: 9.8582 - val_classification_loss: 0.6908 - val_loss: 0.8144 - val_regression_loss: 1.2811 - learning_rate: 6.2500e-05\n",
            "Epoch 248/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6859 - loss: 1.6944 - regression_loss: 10.0870 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2793 - learning_rate: 6.2500e-05\n",
            "Epoch 249/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6895 - loss: 1.5724 - regression_loss: 8.8372 - val_classification_loss: 0.6908 - val_loss: 0.8143 - val_regression_loss: 1.2803 - learning_rate: 6.2500e-05\n",
            "Epoch 250/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6887 - loss: 1.6306 - regression_loss: 9.4119 - val_classification_loss: 0.6908 - val_loss: 0.8143 - val_regression_loss: 1.2810 - learning_rate: 6.2500e-05\n",
            "Epoch 251/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6883 - loss: 1.6963 - regression_loss: 10.0814 - val_classification_loss: 0.6908 - val_loss: 0.8144 - val_regression_loss: 1.2813 - learning_rate: 3.1250e-05\n",
            "Epoch 252/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6878 - loss: 1.6137 - regression_loss: 9.2632 - val_classification_loss: 0.6908 - val_loss: 0.8143 - val_regression_loss: 1.2803 - learning_rate: 3.1250e-05\n",
            "Epoch 253/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6843 - loss: 1.6825 - regression_loss: 9.9757 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2796 - learning_rate: 3.1250e-05\n",
            "Epoch 254/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - classification_loss: 0.6846 - loss: 1.6389 - regression_loss: 9.5423 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2792 - learning_rate: 3.1250e-05\n",
            "Epoch 255/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - classification_loss: 0.6823 - loss: 1.6778 - regression_loss: 9.9475 - val_classification_loss: 0.6908 - val_loss: 0.8142 - val_regression_loss: 1.2794 - learning_rate: 3.1250e-05\n",
            "Epoch 256/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - classification_loss: 0.6880 - loss: 1.6871 - regression_loss: 9.9776 - val_classification_loss: 0.6908 - val_loss: 0.8143 - val_regression_loss: 1.2808 - learning_rate: 3.1250e-05\n",
            "Epoch 257/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6871 - loss: 1.6508 - regression_loss: 9.6433 - val_classification_loss: 0.6908 - val_loss: 0.8143 - val_regression_loss: 1.2807 - learning_rate: 3.1250e-05\n",
            "Epoch 258/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6869 - loss: 1.6786 - regression_loss: 9.9178 - val_classification_loss: 0.6908 - val_loss: 0.8143 - val_regression_loss: 1.2807 - learning_rate: 3.1250e-05\n",
            "Epoch 259/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6862 - loss: 1.6935 - regression_loss: 10.0807 - val_classification_loss: 0.6907 - val_loss: 0.8143 - val_regression_loss: 1.2806 - learning_rate: 3.1250e-05\n",
            "Epoch 260/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6874 - loss: 1.6486 - regression_loss: 9.6138 - val_classification_loss: 0.6907 - val_loss: 0.8144 - val_regression_loss: 1.2812 - learning_rate: 3.1250e-05\n",
            "Epoch 261/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6871 - loss: 1.7225 - regression_loss: 10.3537 - val_classification_loss: 0.6907 - val_loss: 0.8143 - val_regression_loss: 1.2807 - learning_rate: 3.1250e-05\n",
            "Epoch 262/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6907 - loss: 1.6829 - regression_loss: 9.9244 - val_classification_loss: 0.6907 - val_loss: 0.8143 - val_regression_loss: 1.2806 - learning_rate: 3.1250e-05\n",
            "Epoch 263/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6860 - loss: 1.6659 - regression_loss: 9.7952 - val_classification_loss: 0.6907 - val_loss: 0.8143 - val_regression_loss: 1.2802 - learning_rate: 3.1250e-05\n",
            "Epoch 264/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6881 - loss: 1.6796 - regression_loss: 9.9161 - val_classification_loss: 0.6907 - val_loss: 0.8143 - val_regression_loss: 1.2802 - learning_rate: 3.1250e-05\n",
            "Epoch 265/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - classification_loss: 0.6859 - loss: 1.5847 - regression_loss: 8.9856 - val_classification_loss: 0.6907 - val_loss: 0.8143 - val_regression_loss: 1.2809 - learning_rate: 3.1250e-05\n",
            "Epoch 266/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6890 - loss: 1.6308 - regression_loss: 9.4107 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2821 - learning_rate: 3.1250e-05\n",
            "Epoch 267/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6851 - loss: 1.6434 - regression_loss: 9.5785 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2827 - learning_rate: 3.1250e-05\n",
            "Epoch 268/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6855 - loss: 1.6471 - regression_loss: 9.6167 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2832 - learning_rate: 3.1250e-05\n",
            "Epoch 269/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6844 - loss: 1.6357 - regression_loss: 9.5181 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2833 - learning_rate: 3.1250e-05\n",
            "Epoch 270/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6886 - loss: 1.6721 - regression_loss: 9.8309 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2828 - learning_rate: 3.1250e-05\n",
            "Epoch 271/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6886 - loss: 1.6162 - regression_loss: 9.2766 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2827 - learning_rate: 3.1250e-05\n",
            "Epoch 272/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - classification_loss: 0.6878 - loss: 1.6822 - regression_loss: 9.9347 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2825 - learning_rate: 3.1250e-05\n",
            "Epoch 273/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - classification_loss: 0.6886 - loss: 1.6681 - regression_loss: 9.7939 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2830 - learning_rate: 3.1250e-05\n",
            "Epoch 274/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6828 - loss: 1.6874 - regression_loss: 10.0477 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2822 - learning_rate: 3.1250e-05\n",
            "Epoch 275/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - classification_loss: 0.6864 - loss: 1.6527 - regression_loss: 9.6654 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2830 - learning_rate: 3.1250e-05\n",
            "Epoch 276/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - classification_loss: 0.6850 - loss: 1.6456 - regression_loss: 9.5977 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2829 - learning_rate: 3.1250e-05\n",
            "Epoch 277/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - classification_loss: 0.6883 - loss: 1.6373 - regression_loss: 9.4873 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2823 - learning_rate: 3.1250e-05\n",
            "Epoch 278/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 62ms/step - classification_loss: 0.6854 - loss: 1.6397 - regression_loss: 9.5565 - val_classification_loss: 0.6907 - val_loss: 0.8144 - val_regression_loss: 1.2819 - learning_rate: 3.1250e-05\n",
            "Epoch 279/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 38ms/step - classification_loss: 0.6855 - loss: 1.6553 - regression_loss: 9.6965 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2823 - learning_rate: 3.1250e-05\n",
            "Epoch 280/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6843 - loss: 1.6893 - regression_loss: 10.0428 - val_classification_loss: 0.6907 - val_loss: 0.8144 - val_regression_loss: 1.2815 - learning_rate: 3.1250e-05\n",
            "Epoch 281/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6846 - loss: 1.6270 - regression_loss: 9.4276 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2823 - learning_rate: 3.1250e-05\n",
            "Epoch 282/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6889 - loss: 1.6650 - regression_loss: 9.7695 - val_classification_loss: 0.6907 - val_loss: 0.8144 - val_regression_loss: 1.2820 - learning_rate: 3.1250e-05\n",
            "Epoch 283/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - classification_loss: 0.6838 - loss: 1.6735 - regression_loss: 9.8960 - val_classification_loss: 0.6907 - val_loss: 0.8143 - val_regression_loss: 1.2811 - learning_rate: 3.1250e-05\n",
            "Epoch 284/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6874 - loss: 1.6639 - regression_loss: 9.7648 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2822 - learning_rate: 3.1250e-05\n",
            "Epoch 285/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6879 - loss: 1.6766 - regression_loss: 9.8804 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2833 - learning_rate: 3.1250e-05\n",
            "Epoch 286/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - classification_loss: 0.6880 - loss: 1.6733 - regression_loss: 9.8498 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2826 - learning_rate: 3.1250e-05\n",
            "Epoch 287/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6869 - loss: 1.7111 - regression_loss: 10.2420 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2822 - learning_rate: 3.1250e-05\n",
            "Epoch 288/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6845 - loss: 1.6969 - regression_loss: 10.1252 - val_classification_loss: 0.6907 - val_loss: 0.8143 - val_regression_loss: 1.2810 - learning_rate: 3.1250e-05\n",
            "Epoch 289/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6870 - loss: 1.6632 - regression_loss: 9.7594 - val_classification_loss: 0.6907 - val_loss: 0.8144 - val_regression_loss: 1.2820 - learning_rate: 3.1250e-05\n",
            "Epoch 290/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6847 - loss: 1.6919 - regression_loss: 10.0714 - val_classification_loss: 0.6907 - val_loss: 0.8144 - val_regression_loss: 1.2819 - learning_rate: 3.1250e-05\n",
            "Epoch 291/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - classification_loss: 0.6867 - loss: 1.6427 - regression_loss: 9.5634 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2828 - learning_rate: 3.1250e-05\n",
            "Epoch 292/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - classification_loss: 0.6847 - loss: 1.5954 - regression_loss: 9.1154 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2827 - learning_rate: 3.1250e-05\n",
            "Epoch 293/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6867 - loss: 1.6904 - regression_loss: 10.0431 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2834 - learning_rate: 3.1250e-05\n",
            "Epoch 294/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - classification_loss: 0.6855 - loss: 1.6254 - regression_loss: 9.3907 - val_classification_loss: 0.6907 - val_loss: 0.8147 - val_regression_loss: 1.2846 - learning_rate: 3.1250e-05\n",
            "Epoch 295/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - classification_loss: 0.6892 - loss: 1.7022 - regression_loss: 10.1321 - val_classification_loss: 0.6907 - val_loss: 0.8147 - val_regression_loss: 1.2843 - learning_rate: 3.1250e-05\n",
            "Epoch 296/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - classification_loss: 0.6857 - loss: 1.6826 - regression_loss: 9.9733 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2837 - learning_rate: 3.1250e-05\n",
            "Epoch 297/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - classification_loss: 0.6854 - loss: 1.6681 - regression_loss: 9.8246 - val_classification_loss: 0.6907 - val_loss: 0.8145 - val_regression_loss: 1.2824 - learning_rate: 3.1250e-05\n",
            "Epoch 298/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 58ms/step - classification_loss: 0.6876 - loss: 1.6418 - regression_loss: 9.5384 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2834 - learning_rate: 3.1250e-05\n",
            "Epoch 299/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 54ms/step - classification_loss: 0.6870 - loss: 1.6324 - regression_loss: 9.4604 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2832 - learning_rate: 3.1250e-05\n",
            "Epoch 300/300\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - classification_loss: 0.6883 - loss: 1.6704 - regression_loss: 9.8285 - val_classification_loss: 0.6907 - val_loss: 0.8146 - val_regression_loss: 1.2837 - learning_rate: 3.1250e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}