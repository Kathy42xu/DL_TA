{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqM8VQAFpTst"
      },
      "source": [
        "# Case 1 (Part I): House price prediction\n",
        "\n",
        "In this case (Part I), you will build a multilayer perceptron network to predict the selling price of properties. The dataset consists of all single family houses and condos that were sold in Denver in a given year.\n",
        "\n",
        "You need to submit the following files on canvas site:\n",
        "\n",
        "- A report in the pdf format containing the plots of the training errors for the multi-layer perception model and the linear regression model, and the answers to the two questions below. You should also provide interpretations and implications of each plot/table in your report. It is not enough to simply put a chart or a table of numbers in the report and expect the audience to understand what the chart means and what it implies. The point is to provide some insights for an audience like senior manager at Zillow.\n",
        "\n",
        "- The complete Jupyter notebook containing all your Pytorch code with explanations, along with a Markdown text explaining different parts if needed.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68m8rPx7vka3"
      },
      "source": [
        "---\n",
        "## Kaggle community competition: Prof. X's Prize\n",
        "\n",
        "\n",
        "You need to set up a Kaggle account and joined the Kaggle competition by following the [link](https://www.kaggle.com/t/414a77c12150407d97e39fae245e34ef).\n",
        "\n",
        "- Name your team as Section_X_Team_Y, where X is either A or B or C or D, and Y is your team number.\n",
        "- One of the team members can serve as team leader and invite other members of your team to join the team.\n",
        "\n",
        "- Each team can submit at most 20 predictions daily\n",
        "\n",
        "To get the test error for your model, you need to submit your predicted prices for test data on Kaggle. See Kaggle competition website for more detailed instructions. Note that in Part I of the case, you do not need to worry about optimizing your model to get the lowest error possible. The Part I will be graded based on your implemention of the base models as specified below.  We will come back to optimize the model and compete for Prof. X's Prize in Part II of the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4QjqzSng9P7"
      },
      "source": [
        "---\n",
        "## Data Loading and Visualize Data\n",
        "\n",
        "\n",
        "The train data and test data are available on the Kaggle competition website.\n",
        "You need to first download them, then upload them to the google colab, and then read the data using pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-14T16:46:12.975303400Z",
          "start_time": "2023-08-14T16:46:12.877462900Z"
        },
        "id": "K4gWVq036dy1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # Importing pandas, which is a library for data manipulation and analysis\n",
        "#Read the datasets\n",
        "train_df =pd.read_csv(\"train.csv\")\n",
        "test_df =pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mo-iBoh0MJK"
      },
      "source": [
        "### Visualization of SALE PRICES in train data\n",
        "\n",
        "Let's take a closer look at the sale prices in the train data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiGSFJFy0Rl-"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt  # Importing matplotlib's pyplot for making plots and charts\n",
        "\n",
        "# Set the style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['SALE_PRICE'], bins=50, color='blue')\n",
        "plt.title('Histogram of Sale Prices (Train Data)')\n",
        "plt.xlabel('Sale Price')\n",
        "plt.ylabel('Number of Properties')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHY3Ye4wCmWM"
      },
      "source": [
        "---\n",
        "## Data Preparation\n",
        "\n",
        "The first step when building a neural network model is getting your data into the proper form to feed into the network.\n",
        "\n",
        "- **Train labels**: We need to extract the sale prices from the train data as train labels. Since the house prices can take very large values, to make training fast it is helpful to define the train labels as the sale prices divided by a normalization factor.\n",
        "\n",
        "- **Handing non-numeric features**: Some of the house features are non-numeric. We will learn about how to process categorical data in the upcoming lectures. For now, you can  remove those non-numeric features and only train over the numeric features.\n",
        "\n",
        "- **Feature standardization**: When predicting house prices, you started from features that took a variety of ranges—some features had small floating-point values, and others had fairly large integer values. The model might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice for dealing with such data is to do feature-wise normalization: for each feature in the input data (a column in the input dataframe), we subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has\n",
        "a unit standard deviation. **Note**: We need to ensure that the train and test data go through the same normalization.\n",
        "\n",
        "- **Handling missing values**: There may exist some entries with missing values. After the feature standardization, we can impute the missing values with zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCOi8_E20q0A"
      },
      "source": [
        "We see that the sale_price in train data has a wide range from 50K to 2 million, with the median price 431K. We can divide the sale_price by 100K, so the normalized sale_price is between 0.5 and 20 in training data. Remember, when we output the predicted price for the test data, we need to multiply back the normalization factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp02cVWcBOJb"
      },
      "outputs": [],
      "source": [
        "#TODO: define labels for train data as the sale prices divided by $100,000\n",
        "normalization_factor=100000\n",
        "train_labels ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjV866fJBuNt"
      },
      "outputs": [],
      "source": [
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROUIrsA0Bn1F"
      },
      "outputs": [],
      "source": [
        "#TODO: Write code to construct feature vectors for train and test data after data preparation.\n",
        "train_features =\n",
        "test_features ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJA07PMIBjPj"
      },
      "outputs": [],
      "source": [
        "train_features.shape, test_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nclPlDEBtMD"
      },
      "source": [
        "Finally, we convert features and labels to PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcjojyA1r6g1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Convert training features and labels to PyTorch tensors\n",
        "train_features = torch.tensor(train_features.values.astype(np.float32), dtype=torch.float32)\n",
        "test_features = torch.tensor(test_features.values.astype(np.float32), dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels.values.reshape(-1, 1).astype(np.float32), dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpfNC1455HYf"
      },
      "source": [
        "---\n",
        "## DataLoaders and Batching\n",
        "\n",
        "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
        "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
        "2. Create DataLoaders and batch our training, validation, and test Tensor datasets. Note that we will shuffle the train data, so the model will not learn a particular order. For test data, we do not shuffle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFnex-AG5hu6"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "#  Create DataLoaders and batch our train data\n",
        "train_data = TensorDataset(train_features, train_labels)\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL-N-Sr5AGey"
      },
      "outputs": [],
      "source": [
        "#TODO: Create DataLoaders and batch for test data\n",
        "test_loader ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdAZUijCBb2U"
      },
      "source": [
        "Let's take a batch to have a sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEyW4FHfC-gc"
      },
      "outputs": [],
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "features, labels = next(dataiter)\n",
        "\n",
        "print('Sample input size: ', features.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', features)\n",
        "print()\n",
        "print('Sample label size: ', labels.size()) # batch_size\n",
        "print('Sample label: \\n', labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ7a1MTxbK_T"
      },
      "source": [
        "---\n",
        "## Linear regression as benchmark\n",
        "\n",
        "Let us first build a linear regression model as a benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-OjHSVBEP90"
      },
      "outputs": [],
      "source": [
        "#TODO: Build a linear regression model network\n",
        "lin_net ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkJ_Dr1Y3Vm9"
      },
      "source": [
        "Let's take a batch and see the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDRr20263VEG"
      },
      "outputs": [],
      "source": [
        "features, labels = next(dataiter)\n",
        "output=lin_net(features)\n",
        "output.shape,labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gP6KXY_TAR3"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EBhmGZh4Gt9"
      },
      "source": [
        "First, we will use GPU training if it is availabe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFheqQ4Q4L49"
      },
      "outputs": [],
      "source": [
        "#TODO: use GPU for training if it is availabe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKEzpB5F5FCa"
      },
      "source": [
        "Second, let us specify the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rEH9aVe4_7E"
      },
      "outputs": [],
      "source": [
        "#TODO: specify the loss function for training\n",
        "criterion ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFXI-u1qxePM"
      },
      "source": [
        "We are now ready to train the network.\n",
        "\n",
        "\n",
        "Note that with house prices, as with stock prices, we care about relative quantities more than absolute quantities. Thus we tend to care more about the relative error than about the absolute error. For instance, if our prediction is off by \\\\$100,000 when estimating the sale price of a house which is \\\\$125,000, then we are probably doing a horrible job. On the other hand, if we err by this amount for a house with sale price \\\\$2 million, this might represent a pretty  accurate prediction.\n",
        "\n",
        "To this end, we will use the median error rate (MER) used by [Zestimate](https://www.zillow.com/z/zestimate/) to measure the predictive performance. The error rate is defined as\n",
        "$$\n",
        "\\text{Error Rate} = \\left| \\frac{\\text{Predicted Price}-\\text{Actual Price}}{\\text{Actual Price}} \\right|\n",
        "$$\n",
        "The median error rate is defined as the median of error rates for all properties."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUGatRu_DxzT"
      },
      "outputs": [],
      "source": [
        "#TODO: Write code to train the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpNjElY1D3xa"
      },
      "source": [
        "Plot the training error (MER) over epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9U1YyMmEV9s"
      },
      "outputs": [],
      "source": [
        "#TODO: Write code to plot the training error (MER) over epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2OHyGi23fKD"
      },
      "source": [
        "---\n",
        "## Build the Multi-layer Perceptron Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucgArogM4SJF"
      },
      "source": [
        "In the following, we build a multi-layer perception model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtyVuY9H3i-w"
      },
      "outputs": [],
      "source": [
        "#TODO: Build a multi-layer perception neural network with 2 hidden layers of sizes 256 and 128, respectively and ReLu activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXSQ6uOx3pLo"
      },
      "outputs": [],
      "source": [
        "#TODO: write code to train the MLP network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBr4bAXe3ngp"
      },
      "outputs": [],
      "source": [
        "#TODO: Write code to plot the training error (MER) over epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9e9Yeqr8PZZ"
      },
      "source": [
        "**Question 1**: What are your final training errors of the multilayer perception model and the linear regression model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08gC_-Q4pDon"
      },
      "source": [
        "---\n",
        "## Inference on test data\n",
        "\n",
        "After the MLP model is trained, we can use it for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jP5GJ4tCpHG8"
      },
      "outputs": [],
      "source": [
        "#TODO: write the code to generate predicted sale prices for test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmN6bTDupJyh"
      },
      "outputs": [],
      "source": [
        "#TODO: save the predicted sale prices into submission_csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNje9TjRphTx"
      },
      "source": [
        "Now, we can submit our predictions on Kaggle and see how they compare with the actual house prices (labels) on the test set.\n",
        "\n",
        "- Log in to the Kaggle website and visit the house price prediction competition page.\n",
        "\n",
        "- Click the “Submit Predictions”.\n",
        "\n",
        "- Click the “Browse Files” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n",
        "\n",
        "- Click the “Submit” button at the bottom of the page to view your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28-wqckhrA0z"
      },
      "source": [
        "**Question 2**: What is the test error shown on Kaggle? How does it compare with the train error?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "s4QjqzSng9P7",
        "-Mo-iBoh0MJK",
        "PHY3Ye4wCmWM",
        "GpfNC1455HYf",
        "BQ7a1MTxbK_T",
        "4gP6KXY_TAR3",
        "c2OHyGi23fKD",
        "08gC_-Q4pDon"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
