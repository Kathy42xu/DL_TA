{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# House price prediction for Denver Dataset\n",
        "\n",
        "In this case (Part II), there are two main tasks:\n",
        "\n",
        "- First, you will need to split the train data into train and validate data and tune the model hyperparameters to choose the best model. Submit the predicted prices for test data to Kaggle to compete for Prof. X's Prize! You need to select up to 2 submissions to be evaluated for your final leaderboard score. The evaluated submission with the best Private Score is used for your final score.\n",
        "\n",
        "- Then you will examine the profit of the iBuyer business model based on the predicted price on the valid data.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eqM8VQAFpTst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to submit a report in pdf format containing the following material on canvas site:\n",
        "\n",
        "1.   A plot of the training errors and validation errors over epochs for a base multilayer perceptron model with 2 hidden layers of sizes 256 and 128.\n",
        "\n",
        "2.   A plot of the training errors and validation errors over epochs for a  multilayer perceptron model with 4 hidden layers of sizes 512, 256, 128, 64.\n",
        "\n",
        "3.  A plot of the training errors and validation errors over epochs for a  multilayer perceptron model with 4 hidden layers of sizes 512, 256, 128, 64\n",
        "and norm regularization.\n",
        "\n",
        "4. A plot of the training errors and validation errors over epochs for a  multilayer perceptron model with 4 hidden layers of sizes 512, 256, 128, 64\n",
        "and norm regularization and dropout layers.\n",
        "\n",
        "5. A table listing all the model hyperparameters that you have tried with the corresponding validation errors that you found.\n",
        "\n",
        "6. Your profit analysis of the iBuyer business model based on the predicted price on the valid data and answers to the four questions therein.\n",
        "\n",
        "You should also provide interpretations and implications of each plot/table in your report. It is not enough to simply put a chart or a table of numbers in the report and expect the audience to understand what the chart means and what it implies. The point is to provide some insights for an audience like senior management at Zillow.\n",
        "\n",
        "You also need to submit on canvas site:\n",
        "\n",
        "- The complete Juyputer notebook containing all your Pytorch code with explanations, along with a Markdown text explaining different parts if needed.\n",
        "-  A checkpoint.pth file containing all the necessary information to retrieve your best model and predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "I6LbqGHGF66J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4QjqzSng9P7"
      },
      "source": [
        "---\n",
        "## Data Loading and Visualize Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train data and test data are available on Kaggle website.\n",
        "You can first download them, then upload them to the google colab, and then read the data using pandas."
      ],
      "metadata": {
        "id": "qJzPZD90wSNP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4gWVq036dy1",
        "scrolled": true,
        "ExecuteTime": {
          "end_time": "2023-08-14T16:46:12.975303400Z",
          "start_time": "2023-08-14T16:46:12.877462900Z"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # Importing pandas, which is a library for data manipulation and analysis\n",
        "#TODO: Read the datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the train dataframe\n",
        "print(train_df.shape)\n",
        "print(train_df.columns)"
      ],
      "metadata": {
        "id": "XaQo7FuB0doZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the test dataframe\n",
        "print(test_df.shape)\n",
        "print(test_df.columns)"
      ],
      "metadata": {
        "id": "r4JCo589iNmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we have 11581 training samples and 4964 test samples, each with 16 features. The training samples contain the sale_prices, which are the labels. The test samples do not contain the sale_prices, which we will predict by building a MLP model.\n"
      ],
      "metadata": {
        "id": "szikaZ_Stjbw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of SALE PRICES\n",
        "\n",
        "Let's take a closer look at the sale prices in the train data."
      ],
      "metadata": {
        "id": "sJfzgCkCqUio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt  # Importing matplotlib's pyplot, it provides a MATLAB-like interface for making plots and charts\n",
        "\n",
        "# Set the style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['SALE_PRICE'], bins=50, color='blue')\n",
        "plt.title('Histogram of Sale Prices (Train Data)')\n",
        "plt.xlabel('Sale Price')\n",
        "plt.ylabel('Number of Properties')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1AaQ9nLKJej5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the minimum and maximum sale_price in train data."
      ],
      "metadata": {
        "id": "Etksi7L3jPOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['SALE_PRICE'].min())\n",
        "print(train_df['SALE_PRICE'].max())\n",
        "print(train_df['SALE_PRICE'].median())"
      ],
      "metadata": {
        "id": "bHZOKq7yPx9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the sale_price has a wide range from 50K to 2 million, with the median price 431K."
      ],
      "metadata": {
        "id": "CD8wV4aFqG1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of Correlation\n",
        "\n",
        "We can also compute and visualize the correlation matrix."
      ],
      "metadata": {
        "id": "5AgRR1yHFlWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the correlation matrix:\n",
        "correlation_matrix = train_df.corr()\n",
        "\n",
        "# 1. Increase the figure size for clarity\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# 2. Use a heatmap with annotations, a color map, and specific formatting for the annotations\n",
        "ax = sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", annot_kws={'size': 8})\n",
        "\n",
        "# 3. Rotate the x-axis labels for better visibility\n",
        "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "# Rotate the y-axis labels\n",
        "plt.setp(ax.get_yticklabels(), rotation=0)\n",
        "\n",
        "# 4. Title and display\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.tight_layout()  # This can help if any labels are still being cut off\n",
        "plt.show()\n",
        "\n",
        "# 5. Optionally save the figure with high resolution\n",
        "# plt.savefig(\"heatmap.png\", dpi=300)"
      ],
      "metadata": {
        "id": "lclsy9WmFmAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the Sale_Price has high correlation with Living_SQFT and number of Full Bathrooms."
      ],
      "metadata": {
        "id": "mkxgvlgoF8PP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribution of houses over different NBHD"
      ],
      "metadata": {
        "id": "UmJzfROtGEms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the number of houses per neighborhood\n",
        "House_by_NBHD = train_df['NBHD'].value_counts()\n",
        "print(House_by_NBHD )"
      ],
      "metadata": {
        "id": "tnx5QYiCGFes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering ZIP codes that appear more than once\n",
        "filtered_House_by_NBHD = House_by_NBHD[House_by_NBHD > 200]\n",
        "filtered_House_by_NBHD.plot(kind='bar', figsize=(10,6))\n",
        "plt.title('Distribution of Houses by NBHD')\n",
        "plt.ylabel('Number of Houses')\n",
        "plt.xlabel('NBHD')\n",
        "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability, if necessary\n",
        "plt.tight_layout()  # Ensure everything fits without overlapping\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "iMNHS8RzGyog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Data Preparation\n",
        "\n",
        "The first step when building a neural network model is getting your data into the proper form to feed into the network.\n",
        "\n",
        "- Train labels: We need to extract the sale prices from the train data as train labels. Since the house prices can take very large values, to make training fast it is helpful to define the train labels as the sale prices divided by a normalization factor.\n",
        "\n",
        "- **Handing non-numeric features**: Some of the house features are non-numeric. We will learn about how to process categorical data in the upcoming lectures. For now, you can  remove those non-numeric features and only train over the numeric features.\n",
        "\n",
        "- **Feature standardization**: When predicting house prices, you started from features that took a variety of ranges—some features had small floating-point values, and others had fairly large integer values. The model might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice for dealing with such data is to do feature-wise normalization: for each feature in the input data (a column in the input dataframe), we subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has\n",
        "a unit standard deviation. Note that here we combine the feature vectors in the train and test data. In this way, the train and test data go through the same normalization.\n",
        "\n",
        "- **Handling missing values**: There may exist some entries with missing values. After the feature standardization, we can impute the missing values with zeros."
      ],
      "metadata": {
        "id": "PHY3Ye4wCmWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.info()"
      ],
      "metadata": {
        "id": "t3DkZgCn0mM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the sale_price in train data has a wide range from 50K to 2 million, with the median price 431K. We can divide the sale_price by 100K, so the normalized sale_price is between 0.5 and 20 in training data. Remember, when we output the predicted price for the test data, we need to multiply back the normalization factor."
      ],
      "metadata": {
        "id": "Vc9Mbiy0d-39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: define labels for train data\n",
        "normalization_factor=100000\n",
        "train_labels = train_df['SALE_PRICE']/normalization_factor\n",
        "train_df.drop('SALE_PRICE', axis=1, inplace=True) # drop the sale_prices in features."
      ],
      "metadata": {
        "id": "6tLe4pPFeBCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inplace parameter, when set to True , allows you to drop the rows or columns without returning a new DataFrame. The issue arises when the drop function reorders the DataFrame, which can be problematic when the order of your data matters"
      ],
      "metadata": {
        "id": "Yu_DUz_aenSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that both the training samples and test samples contain an ID column, which is not informative for predicting the house price. Thus we will drop the ID column."
      ],
      "metadata": {
        "id": "zghhA9Q1esTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ID=train_df['ID']\n",
        "test_ID=test_df['ID']\n",
        "train_df.drop('ID', axis=1, inplace=True)\n",
        "test_df.drop('ID', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "7qZ0EjfxetnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we combine the feature vectors in the train data and test data\n",
        "features=pd.concat(objs=[train_df,test_df],axis=0)"
      ],
      "metadata": {
        "id": "gG71VfKuyd9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features.info()"
      ],
      "metadata": {
        "id": "44PZBN4py82z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are three non-numeric features, namely `NBHD`, `PROP_CLASS`, and `STYLE_CN`. We will apply one-hot encoding to those non-numeric features in our model; you could also simply drop these non-numeric features."
      ],
      "metadata": {
        "id": "f1L4I7q0ygyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = features.dtypes[features.dtypes != 'object'].index\n",
        "non_numeric_features = features.dtypes[features.dtypes == 'object'].index\n",
        "numeric_features, non_numeric_features"
      ],
      "metadata": {
        "id": "N7RHSwODoVGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to drop the non-numeric features, you just set drop_non_numeric_features= True.\n",
        "drop_non_numeric_features= False\n",
        "\n",
        "if drop_non_numeric_features:\n",
        "    features= features.drop(non_numeric_features, axis=1)\n",
        "else:\n",
        "    # One-hot encode categorical features\n",
        "    features = pd.get_dummies(features, columns=non_numeric_features, dummy_na=True)\n",
        "\n",
        "# Check for non-numeric columns\n",
        "non_numeric_cols = features.select_dtypes(include=['object']).columns\n",
        "if not non_numeric_cols.empty:\n",
        "    raise ValueError(f\"DataFrame contains non-numeric columns: {non_numeric_cols.tolist()}\")"
      ],
      "metadata": {
        "id": "c9X9cUC6nvxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize numeric features\n",
        "features[numeric_features] = features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))"
      ],
      "metadata": {
        "id": "UdpzCjfyyXRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recheck the mean and std after standardization\n",
        "features[numeric_features].mean(), features[numeric_features].std()"
      ],
      "metadata": {
        "id": "3nDD_WHCTPJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that after standardization, the features for the train data have mean 0 and standard deviation 1."
      ],
      "metadata": {
        "id": "SI4eO3UKZMmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After the feature standardization, we can impute the missing values with zeros.\n",
        "features[numeric_features] = features[numeric_features].fillna(0)"
      ],
      "metadata": {
        "id": "b0M6y8TPp2lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double check the features after data processing."
      ],
      "metadata": {
        "id": "Aw-sbAfpr-LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features.info()\n",
        "print(features.columns)"
      ],
      "metadata": {
        "id": "cNhSp4LVI4F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that after the one-hot encoding of the non-numeric features, now we have 105-dimensional feature."
      ],
      "metadata": {
        "id": "2ZMyEDlqTxz2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# check whether there is any missing entry\n",
        "print(features.isnull().sum())"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-14T16:46:13.162308500Z",
          "start_time": "2023-08-14T16:46:13.134614Z"
        },
        "id": "173vEPejAFLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we extract out the train and test features\n",
        "train_features = features.iloc[:len(train_labels)]\n",
        "test_features = features.iloc[len(train_labels):]\n",
        "train_features.shape, test_features.shape"
      ],
      "metadata": {
        "id": "wc4ZJf6l0azu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Convert training features and labels to PyTorch tensors\n",
        "train_features = torch.tensor(train_features.values.astype(np.float32), dtype=torch.float32)\n",
        "test_features = torch.tensor(test_features.values.astype(np.float32), dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels.values.reshape(-1, 1).astype(np.float32), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "XcjojyA1r6g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-08-14T16:46:13.162308500Z",
          "start_time": "2023-08-14T16:46:13.134614Z"
        },
        "id": "67Wx2_SOAFLh"
      },
      "outputs": [],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "id": "-bc0cuKEjC5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Training and Validation\n",
        "\n",
        "To prevent overfitting, we'll split it our training data into training and validation. We will use validation set to select the appropriate model.\n",
        "One way is to use the [`train_test_split` function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). You're more than welcome to use your own way."
      ],
      "metadata": {
        "id": "gBJ3mU_qLVRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: filling in the missing code to split train data into train and validation\n",
        "from sklearn.model_selection import train_test_split  # Importing train_test_split function from sklearn for splitting data into training set and validation set\n",
        "# Splitting the training data: 20% is validation data\n",
        "train_indices, valid_indices, train_features, valid_features, train_labels, valid_labels ="
      ],
      "metadata": {
        "id": "kua2akLcLgLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_features.shape)\n",
        "print(valid_features.shape)"
      ],
      "metadata": {
        "id": "W97WNbfULuGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## DataLoaders and Batching\n",
        "\n",
        "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
        "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
        "2. Create DataLoaders and batch our training, validation, and test Tensor datasets. Note that we will shuffle the train data, so the model will not learn a particular order. For valid and test data, we do not shuffle."
      ],
      "metadata": {
        "id": "GpfNC1455HYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "train_data = TensorDataset(train_features, train_labels)\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
        "#TODO: create dataloader for validation data\n",
        "valid_loader ="
      ],
      "metadata": {
        "id": "iFnex-AG5hu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = TensorDataset(test_features)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "k8LmCiW3S8cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a batch to have a sanity check"
      ],
      "metadata": {
        "id": "IdAZUijCBb2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "features, labels = next(dataiter)\n",
        "\n",
        "print('Sample input size: ', features.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', features)\n",
        "print()\n",
        "print('Sample label size: ', labels.size()) # batch_size\n",
        "print('Sample label: \\n', labels)"
      ],
      "metadata": {
        "id": "TO2H6Wo1BbAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Linear Regression as Benchmark\n",
        "\n",
        "Let us build a linear regression model as a benchmark. Note that the linear regression model can be viewed as a special instance of multi-layer perception with no hidden layer and a single output neuron."
      ],
      "metadata": {
        "id": "BQ7a1MTxbK_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a linear regression model network\n",
        "import torch.nn as nn\n",
        "lin_net = nn.Linear(train_features.shape[1], 1)"
      ],
      "metadata": {
        "id": "ez55bAsjbOhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print out the model achitecture."
      ],
      "metadata": {
        "id": "O1_ZsGNRfOVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin_net"
      ],
      "metadata": {
        "id": "wYyVOBJIfPmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a batch and see the output"
      ],
      "metadata": {
        "id": "sjsqBCoGfTgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = next(dataiter)\n",
        "output=lin_net(features)\n",
        "output.shape,labels.shape"
      ],
      "metadata": {
        "id": "Jck1SXWWfWdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Train the model"
      ],
      "metadata": {
        "id": "4gP6KXY_TAR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will use GPU training if it is availabe."
      ],
      "metadata": {
        "id": "2EBhmGZh4Gt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "lin_net = lin_net.to(device)"
      ],
      "metadata": {
        "id": "CFheqQ4Q4L49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, let us specify the loss function."
      ],
      "metadata": {
        "id": "SKEzpB5F5FCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Since both the output and the label are real valued, we will use the standard mean-squared loss.\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "_rEH9aVe4_7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Third, while we are using the mean-squared loss for training loss, we will use\n",
        "a different metric to measure the predictive performance.\n",
        "\n",
        "Note that with house prices, as with stock prices, we care about relative quantities more than absolute quantities. Thus we tend to care more about the relative error than about the absolute error. For instance, if our prediction is off by \\\\$100,000 when estimating the sale price of a house which is \\\\$125,000, then we are probably doing a horrible job. On the other hand, if we err by this amount for a house with sale price \\\\$2 million, this might represent a pretty  accurate prediction.\n",
        "\n",
        "To address this issue, we will use the median error rate (MER) used by [Zestimate](https://www.zillow.com/z/zestimate/) to measure the predictive performance. The error rate is defined as\n",
        "$$\n",
        "\\text{Error Rate} = \\left| \\frac{\\text{Predicted Price}-\\text{Actual Price}}{\\text{Actual Price}} \\right|\n",
        "$$\n",
        "The median error rate is defined as the median of error rates for all properties."
      ],
      "metadata": {
        "id": "YosuRJqr7VDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to train the network. Let’s try training the model a bit longer: 200 epochs. To keep a record of how well the model does at each epoch, we will save the per-epoch training error and validation error in the training loop."
      ],
      "metadata": {
        "id": "TweT3hAxadKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write code to train the network and save training and validation error."
      ],
      "metadata": {
        "id": "n83k2GL-3Mnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function to plot errors."
      ],
      "metadata": {
        "id": "DXRA3VrD5--n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write code to plot the training and vadliation errors (MER) over epochs"
      ],
      "metadata": {
        "id": "rYbMwpcn5-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Build the Multi-layer Perceptron Base Model"
      ],
      "metadata": {
        "id": "vsxR1nrM1yFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, we build a multi-layer perception model with 2 hidden layers of sizes 256 and 128, respectively and ReLu activations."
      ],
      "metadata": {
        "id": "2hZc2xpgZxmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a feed-forward network\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(train_features.shape[1], 256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128,1))"
      ],
      "metadata": {
        "id": "8tYjrMNm2FXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print out the model achitecture."
      ],
      "metadata": {
        "id": "XHmRp7esaFWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "c_QdNFsAZ_L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "HiraJa-zf5LZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: write code to train the MLP network and save training and validation error."
      ],
      "metadata": {
        "id": "Ivqg6xw_f9H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write code to plot the training and validation error (MER) over epochs"
      ],
      "metadata": {
        "id": "VsnEBtxKfit7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Change network architecture\n"
      ],
      "metadata": {
        "id": "96o6pXJZtLIF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, build a MLP with 4 hidden layer of sizes 512, 256, 128, 64, respectively.  "
      ],
      "metadata": {
        "id": "E6_u31AKvGAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: building a MLP with 4 hidden layer of sizes 512, 256, 128, 64,"
      ],
      "metadata": {
        "id": "T4uh-Uyaur3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: plot the training and validation error (MER) over epochs"
      ],
      "metadata": {
        "id": "Cfrs6pdpvqyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Add norm regularization\n",
        "\n",
        "In the following, use the norm regularization to retrain the above MLP."
      ],
      "metadata": {
        "id": "7318EVBqr6lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: plot the training and validation error (MER) over epochs after using norm regularization"
      ],
      "metadata": {
        "id": "pccxcR7CwFjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Add dropout layer\n",
        "\n",
        "In the following, add dropout layer to the above MLP.\n"
      ],
      "metadata": {
        "id": "awKh4JrcwKQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: plot the training and validation error (MER) over epochs after using norm regularization"
      ],
      "metadata": {
        "id": "8gI38LVoBtSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some other model variations\n",
        "\n",
        "**You're more than welcome to try some other model varations (e.g. different number of hidden layers, hidden neurons, learning rate, etc) to achieve lower valid error. Include a table listing all the model hyperparameters that you have tried with the corresponding validation errors that you found.**"
      ],
      "metadata": {
        "id": "A2aLRME5nOMA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qSNpDljQnN1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Save Model\n",
        "\n",
        "Save a `checkpoint.pth` file containing all the necessary information to retrieve your best model and predictions. Remember submitting this file to Canvas siste.\n",
        "\n",
        "Hint: Check out the `Lecture 4 - Saving and Loading Models.ipynb` on Canvas if you do not know how to save model."
      ],
      "metadata": {
        "id": "TKAgZM-PzOcM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ztaxE5pVKKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Inference on test data\n",
        "\n",
        "After the model is trained, we can use it for inference."
      ],
      "metadata": {
        "id": "Gmrzpydv34Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: write the code to generate predicted sale prices for test data"
      ],
      "metadata": {
        "id": "S6zaW1GwVSia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: save the predicted sale prices into submission_csv"
      ],
      "metadata": {
        "id": "KjiNm30lVUBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can submit our predictions on Kaggle and see how they compare with the actual house prices (labels) on the test set.\n",
        "\n",
        "- Log in to the Kaggle website and visit the house price prediction competition page.\n",
        "\n",
        "- Click the “Submit Predictions”.\n",
        "\n",
        "- Click the “Browse Files” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n",
        "\n",
        "- Click the “Submit” button at the bottom of the page to view your results."
      ],
      "metadata": {
        "id": "VfIYGk-BVZSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Include your best test error shown on Kaggle in your case report!**"
      ],
      "metadata": {
        "id": "VCzV9HsWVd3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Evaluate the profit of iBuyer business model\n",
        "\n",
        "\n",
        "In class, we have dicussed the iBuyer business model and its opportunities and risks. In the following analysis, imagine you work in a consulting firm and would like to investigate the profitability of the iBuyer business model.\n",
        "\n",
        "You have taken the Mordern Analytics course and remembered that Prof. X advocated the data-driven approach in business decision making. Thus, you would like to perform analysis based on model and data.\n",
        "\n",
        "Note that since we do not know the true sale prices in the future (like test data), we need to conduct the analysis based on the historical data (train or validation data). Previously, you have already trained a multilayer perceptron model using the train data. Now, let's evaluate the profit of iBuyer business model based on the predicted prices on the **validation data**."
      ],
      "metadata": {
        "id": "UBvBbWT_BjGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first compute the predicted prices on the valid data."
      ],
      "metadata": {
        "id": "s-SDd9Zzv2bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: Load your best model from your saved checkpoint.pth file."
      ],
      "metadata": {
        "id": "XeZXNhtdV3zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: compute the predicted prices on valid data using your best model"
      ],
      "metadata": {
        "id": "HardNM2Zv9IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute the signed error rates (without taking the absolute value sign), that is\n",
        "\n",
        "$$\n",
        "\\text{Signed Error Rate} = \\frac{\\text{Predicted Price}-\\text{Actual Price}}{\\text{Actual Price}}\n",
        "$$\n",
        "We will call signed error rate as prediction error henceafter."
      ],
      "metadata": {
        "id": "Gi5KDRpfmA0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: compute the signed error rates (henceafter called prediction errors)"
      ],
      "metadata": {
        "id": "EDhCZd-3mb2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis and visualization of valid errors"
      ],
      "metadata": {
        "id": "LqhwqLxIx-ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the histogram of prediction errors.\n",
        "\n",
        "**Question 1**: what is the bias of the prediction errors? Include the histogram of prediction errors and the bias in your report."
      ],
      "metadata": {
        "id": "QLsAypFuAumy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Profit Analysis\n",
        "\n",
        "In the following profit analysis, we assume the iBuyer will make an offer to every property in the valid data based on their predicted price $PP$. We assume the iBuyer decides the offer price $OP$ according to\n",
        "$$\n",
        "OP = \\frac{PP}{1+\\alpha},\n",
        "$$\n",
        "where $\\alpha$ is the (targted) profit margin of the iBuyer.\n",
        "Here we assume the profit margin has already taken into consideration the commission fee charged by the iBuyer\n",
        "and various costs associated such as transaction cost, administration cost, and holding cost. Note that the commission fee charged by Zillow is often around $7.5\\%$ and Zillow may charge additional repair costs after home inspection. Thus we take $\\alpha=12\\%$ in this case study.\n",
        "\n",
        "We further assume that the iBuyer can resell the property at the same\n",
        "price as the broker in the future once the property is bought. In other words, the resell price is equal to the sale price in the valid data. This assumption may not be exactly true in practice and the iBuyer may sell the house at either a higher or lower price depending on the market trend. But our conclusion will not change too much.\n",
        "\n",
        "Based on the above two assumptions, we can now determine the percentage profit\n",
        "for a property bought by the iBuyer as\n",
        "$$\n",
        "\\frac{SP- OP}{OP}.\n",
        "$$\n",
        "We use the percentage profit instead of the absolute profit because the iBuyer cannot hope to purchase all houses in the market. Therefore, the percentage profit is a better measure of the profitability of the iBuyer business model.\n",
        " The aim of the iBuyer in this simplified setting is to purchase properties for less money than they are sold for, to generate a profit."
      ],
      "metadata": {
        "id": "tpp6qgMuwlgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "profit_margin = 0.12"
      ],
      "metadata": {
        "id": "etKvSNDOzgSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: Consider the hypothetical scenario where the offers are all accepted regardless of their values,\n",
        "what is the average percentage profit? Do you see a big difference compared to the profit margin $\\alpha$? Include your answers in the report."
      ],
      "metadata": {
        "id": "KYDo-OoV8lUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Offer Acceptance Rule**\n",
        "\n",
        "\n",
        "However, not every offer will be accepted by the home owner. Given an offer price, whether the homeowner accepts it\n",
        "depends on the homeowner's perceived valuation. For the current dataset, we lack enough data to determine the homeowner's perceived valuation of the property. However, the actual sale price in the valid data serves as a reasonable proxy of the homeowner's perceived valuation. Therefore, we assume that the home owner will accept the offer, if\n",
        "$$\n",
        "OP> (1-\\beta) SP,\n",
        "$$\n",
        "where $\\beta$ is a discounting factor. Here the discounting factor captures the commission fee charged\n",
        "by the conventional realtors which is around 6%, as well as the convenience factor that models\n",
        "how much the homeowner values the quick transaction services of the iBuyer over the conventional\n",
        "relator. We assume $\\beta = 10\\%$ in this case."
      ],
      "metadata": {
        "id": "qq8r4Yn2_Jui"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**: Based on the sale price in the valid data and the acceptance rule, what is the mean percentage profit among all accepted offers? Do you see a big difference compared to the targeted profit margin $\\alpha$?  Include your answers in your report.  "
      ],
      "metadata": {
        "id": "yvUa2TdyzT5G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the histogram of the prediction errors for those properties whose home owners accepted the offer.\n",
        "\n",
        "**Question 4**: What is the bias of the prediction errors when restricting to those properties whose owners accepted the offer? Based on the histogram and bias, can you explain your answers to Question 3?"
      ],
      "metadata": {
        "id": "Z2_qBob7Bo_f"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "s4QjqzSng9P7",
        "PHY3Ye4wCmWM",
        "gBJ3mU_qLVRv",
        "GpfNC1455HYf",
        "BQ7a1MTxbK_T",
        "4gP6KXY_TAR3",
        "vsxR1nrM1yFH",
        "96o6pXJZtLIF",
        "7318EVBqr6lX",
        "awKh4JrcwKQC",
        "A2aLRME5nOMA",
        "TKAgZM-PzOcM",
        "Gmrzpydv34Y5",
        "UBvBbWT_BjGr"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}